{
  "total_queries": 12,
  "total_papers": 190,
  "total_open_access": 143,
  "queries": [
    {
      "original_query": "Deep learning medical imaging",
      "final_query": "Deep learning medical imaging",
      "attempts": 1,
      "paper_count": 20,
      "open_access_count": 17,
      "papers": [
        {
          "title": "A Survey on Explainable Artificial Intelligence (XAI) Techniques for Visualizing Deep Learning Models in Medical Imaging",
          "authors": [
            "Deepshikha Bhati",
            "Fnu Neha",
            "Md Amiruzzaman"
          ],
          "year": 2024,
          "citations": 31,
          "abstract": "The combination of medical imaging and deep learning has significantly improved diagnostic and prognostic capabilities in the healthcare domain. Nevertheless, the inherent complexity of deep learning models poses challenges in understanding their decision-making processes. Interpretability and visualization techniques have emerged as crucial tools to unravel the black-box nature of these models, providing insights into their inner workings and enhancing trust in their predictions. This survey paper comprehensively examines various interpretation and visualization techniques applied to deep learning models in medical imaging. The paper reviews methodologies, discusses their applications, and evaluates their effectiveness in enhancing the interpretability, reliability, and clinical relevance of deep learning models in medical image analysis.",
          "pdf_url": "https://doi.org/10.3390/jimaging10100239",
          "doi": "10.3390/jimaging10100239",
          "url": "https://www.semanticscholar.org/paper/1c9f96e44e7138049b53ff9cfe593b7f95f44f53",
          "source": "Semantic Scholar"
        },
        {
          "title": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
          "authors": [
            "Fabian Isensee",
            "P. Jaeger",
            "Simon A. A. Kohl",
            "Jens Petersen",
            "Klaus Hermann Maier-Hein"
          ],
          "year": 2020,
          "citations": 6660,
          "abstract": null,
          "pdf_url": "https://arxiv.org/pdf/1904.08128",
          "doi": "10.1038/s41592-020-01008-z",
          "url": "https://www.semanticscholar.org/paper/f28e387d4229c5f690ce4570a391c0f47e7155c7",
          "source": "Semantic Scholar"
        },
        {
          "title": "Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review",
          "authors": [
            "Aghiles Kebaili",
            "J. Lapuyade-Lahorgue",
            "S. Ruan"
          ],
          "year": 2023,
          "citations": 190,
          "abstract": "Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.",
          "pdf_url": "https://www.mdpi.com/2313-433X/9/4/81/pdf?version=1681461366",
          "doi": "10.3390/jimaging9040081",
          "url": "https://www.semanticscholar.org/paper/13afd3c131dbb836bf9c6c65466ca8f5234bca11",
          "source": "Semantic Scholar"
        },
        {
          "title": "Machine learning and deep learning techniques for breast cancer diagnosis and classification: a comprehensive review of medical imaging studies",
          "authors": [
            "Mehran Radak",
            "H. Y. Lafta",
            "H. Fallahi"
          ],
          "year": 2023,
          "citations": 61,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1007/s00432-023-04956-z",
          "url": "https://www.semanticscholar.org/paper/3308b1ac35bb7ad5eef94d3f588b6a70a9a5ad55",
          "source": "Semantic Scholar"
        },
        {
          "title": "Applying Deep Learning to Medical Imaging: A Review",
          "authors": [
            "H. Zhang",
            "Yufei Qie"
          ],
          "year": 2023,
          "citations": 72,
          "abstract": "Deep learning (DL) has made significant strides in medical imaging. This review article presents an in-depth analysis of DL applications in medical imaging, focusing on the challenges, methods, and future perspectives. We discuss the impact of DL on the diagnosis and treatment of diseases and how it has revolutionized the medical imaging field. Furthermore, we examine the most recent DL techniques, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs), and their applications in medical imaging. Lastly, we provide insights into the future of DL in medical imaging, highlighting its potential advancements and challenges.",
          "pdf_url": "https://www.mdpi.com/2076-3417/13/18/10521/pdf?version=1695283635",
          "doi": "10.3390/app131810521",
          "url": "https://www.semanticscholar.org/paper/d069034d2995cb4cc53bdb56ee53c716f020fca2",
          "source": "Semantic Scholar"
        },
        {
          "title": "Large-Scale Deep Learning Medical Image Methodology and Applications Using Multiple GPUs",
          "authors": [
            "Ezhilmathi Krishnasamy",
            "Selma Boudisa",
            "G. Kanli",
            "Francesc Xavier Marzal-Abarca",
            "Oliver Keunen",
            "Pascal Bouvry"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Digitalization of medical images has become firmly established in research labs and clinical settings worldwide, and its numerous benefits have helped drive its rapid expansion, which is predicted to continue momentum over the coming years. Its advantages of cutting costs, improving analysis, productivity and patient outcomes, reducing errors, and enhancing imaging and innovation will likely propel its widespread adoption. In recent years digitalization of medical images especially relied on high performance computing. This work illustrates how a large volume of digital radiology data can be analysed in a parallel heterogeneous architecture. We have used TensorFlow (including Keras) and the Horovod API to speed up the learning of Deep Learning models, particularly using GPUs in single and multiple compute nodes. We conclude that using Horovod API with data parallelism shows tremendous speed gains compared to traditional CPU when training the 2D UNet model.",
          "pdf_url": "",
          "doi": "10.1109/ICABME59496.2023.10293105",
          "url": "https://www.semanticscholar.org/paper/d3c751af2df1ae84b2041c3c950b6ec01e0430b0",
          "source": "Semantic Scholar"
        },
        {
          "title": "A survey on automatic generation of medical imaging reports based on deep learning",
          "authors": [
            "Ting Pang",
            "Peigao Li",
            "Lijie Zhao"
          ],
          "year": 2023,
          "citations": 51,
          "abstract": "Recent advances in deep learning have shown great potential for the automatic generation of medical imaging reports. Deep learning techniques, inspired by image captioning, have made significant progress in the field of diagnostic report generation. This paper provides a comprehensive overview of recent research efforts in deep learning-based medical imaging report generation and proposes future directions in this field. First, we summarize and analyze the data set, architecture, application, and evaluation of deep learning-based medical imaging report generation. Specially, we survey the deep learning architectures used in diagnostic report generation, including hierarchical RNN-based frameworks, attention-based frameworks, and reinforcement learning-based frameworks. In addition, we identify potential challenges and suggest future research directions to support clinical applications and decision-making using medical imaging report generation systems.",
          "pdf_url": "https://biomedical-engineering-online.biomedcentral.com/counter/pdf/10.1186/s12938-023-01113-y",
          "doi": "10.1186/s12938-023-01113-y",
          "url": "https://www.semanticscholar.org/paper/8d7093e0524e33dffff56ea1ecae2a83f82ab603",
          "source": "Semantic Scholar"
        },
        {
          "title": "Enhancing radiomics and Deep Learning systems through the standardization of medical imaging workflows",
          "authors": [
            "Miriam Cobo",
            "P. Menéndez Fernández-Miranda",
            "G. Bastarrika",
            "L. Lloret Iglesias"
          ],
          "year": 2023,
          "citations": 44,
          "abstract": "Recent advances in computer-aided diagnosis, treatment response and prognosis in radiomics and deep learning challenge radiology with requirements for world-wide methodological standards for labeling, preprocessing and image acquisition protocols. The adoption of these standards in the clinical workflows is a necessary step towards generalization and interoperability of radiomics and artificial intelligence algorithms in medical imaging.",
          "pdf_url": "https://www.nature.com/articles/s41597-023-02641-x.pdf",
          "doi": "10.1038/s41597-023-02641-x",
          "url": "https://www.semanticscholar.org/paper/090d12df4cb4431d1f4be02995c2f970aec981f4",
          "source": "Semantic Scholar"
        },
        {
          "title": "Recent Advances in Deep Learning and Medical Imaging for Head and Neck Cancer Treatment: MRI, CT, and PET Scans",
          "authors": [
            "Mathew Illimoottil",
            "D. Ginat"
          ],
          "year": 2023,
          "citations": 46,
          "abstract": "Simple Summary Deep learning techniques have significant potential in head and neck cancer imaging, particularly in tumor detection, segmentation, and outcome prediction using magnetic resonance imaging (MRI), computed tomography (CT), and positron emission tomography (PET) scans. Advanced deep learning methods, such as convolutional autoencoders, generative adversarial networks (GANs), and transformer models, have further enhanced imaging capabilities. Comparing deep learning and traditional techniques and the advantages and limits of each reveals their complementary roles in cancer management. Integrating radiogenomics with deep learning models promises further advancements in personalized care. However, challenges such as standardization, data quality, model overfitting, and computational requirements persist. Addressing these issues, integrating multimodal and temporal information, enhancing explainability, and conducting clinical validation are crucial for implementing deep learning models in head and neck cancer diagnosis and treatment. Overcoming these obstacles will pave the way for improved patient outcomes and personalized treatment strategies in head and neck cancer management. Abstract Deep learning techniques have been developed for analyzing head and neck cancer imaging. This review covers deep learning applications in cancer imaging, emphasizing tumor detection, segmentation, classification, and response prediction. In particular, advanced deep learning techniques, such as convolutional autoencoders, generative adversarial networks (GANs), and transformer models, as well as the limitations of traditional imaging and the complementary roles of deep learning and traditional techniques in cancer management are discussed. Integration of radiomics, radiogenomics, and deep learning enables predictive models that aid in clinical decision-making. Challenges include standardization, algorithm interpretability, and clinical validation. Key gaps and controversies involve model generalizability across different imaging modalities and tumor types and the role of human expertise in the AI era. This review seeks to encourage advancements in deep learning applications for head and neck cancer management, ultimately enhancing patient care and outcomes.",
          "pdf_url": "https://www.mdpi.com/2072-6694/15/13/3267/pdf?version=1687940793",
          "doi": "10.3390/cancers15133267",
          "url": "https://www.semanticscholar.org/paper/1d833c1290430ec16880980b2efcff53ea930983",
          "source": "Semantic Scholar"
        },
        {
          "title": "Hybrid Deep learning based Semi-supervised Model for Medical Imaging",
          "authors": [
            "Hemlata Sahu",
            "R. Kashyap",
            "B. Dewangan"
          ],
          "year": 2023,
          "citations": 60,
          "abstract": "One of the most promising fields in medicine is the application of artificial intelligence methods to medical imaging. Though annotating medical images is an expensive operation, most of the recent success in this field has relied heavily on vast amounts of meticulously annotated data. The best that we can tell, the method we present in this research, hybrid approach, is the first to use recent developments in semi-supervised learning (SSL) for medical imaging recognition. Due to the growing complexity of healthcare data, machine learning techniques like Deep Neural Network (DNN) models have become more and more popular. Machine learning (ML) techniques can uncover hidden patterns and other important facts from the massive amount of health data that traditional analytics can’t find in a reasonable length of time. These advancements will have a significant impact on medical imaging technology, medical healthcare data processing, medical illness diagnostics, and general healthcare. We have two goals: First to conduct a survey on DL techniques to medical pictures; and second proposed hybrid DL-based approaches for image classification. This paper mainly proposed hybrid methodology for medical imaging which can improve semi supervised model.",
          "pdf_url": "",
          "doi": "10.1109/OTCON56053.2023.10113904",
          "url": "https://www.semanticscholar.org/paper/e4cb318d9f7e5c004b04f9a07b2a2da7da8ce9a2",
          "source": "Semantic Scholar"
        },
        {
          "title": "Invariant Scattering Transform for Medical Imaging",
          "authors": [
            "Nafisa Labiba Ishrat Huda",
            "Angona Biswas",
            "MD Abdullah Al Nasim",
            "Md. Fahim Rahman",
            "Shoaib Ahmed"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Invariant scattering transform introduces new area of research that merges\nthe signal processing with deep learning for computer vision. Nowadays, Deep\nLearning algorithms are able to solve a variety of problems in medical sector.\nMedical images are used to detect diseases brain cancer or tumor, Alzheimer's\ndisease, breast cancer, Parkinson's disease and many others. During pandemic\nback in 2020, machine learning and deep learning has played a critical role to\ndetect COVID-19 which included mutation analysis, prediction, diagnosis and\ndecision making. Medical images like X-ray, MRI known as magnetic resonance\nimaging, CT scans are used for detecting diseases. There is another method in\ndeep learning for medical imaging which is scattering transform. It builds\nuseful signal representation for image classification. It is a wavelet\ntechnique; which is impactful for medical image classification problems. This\nresearch article discusses scattering transform as the efficient system for\nmedical image analysis where it's figured by scattering the signal information\nimplemented in a deep convolutional network. A step by step case study is\nmanifested at this research work.",
          "pdf_url": "http://arxiv.org/pdf/2307.04771v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2307.04771v1",
          "source": "arXiv"
        },
        {
          "title": "Trends in deep learning for medical hyperspectral image analysis",
          "authors": [
            "Uzair Khan",
            "Paheding Sidike",
            "Colin Elkin",
            "Vijay Devabhaktuni"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Deep learning algorithms have seen acute growth of interest in their\napplications throughout several fields of interest in the last decade, with\nmedical hyperspectral imaging being a particularly promising domain. So far, to\nthe best of our knowledge, there is no review paper that discusses the\nimplementation of deep learning for medical hyperspectral imaging, which is\nwhat this review paper aims to accomplish by examining publications that\ncurrently utilize deep learning to perform effective analysis of medical\nhyperspectral imagery. This paper discusses deep learning concepts that are\nrelevant and applicable to medical hyperspectral imaging analysis, several of\nwhich have been implemented since the boom in deep learning. This will comprise\nof reviewing the use of deep learning for classification, segmentation, and\ndetection in order to investigate the analysis of medical hyperspectral\nimaging. Lastly, we discuss the current and future challenges pertaining to\nthis discipline and the possible efforts to overcome such trials.",
          "pdf_url": "http://arxiv.org/pdf/2011.13974v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2011.13974v1",
          "source": "arXiv"
        },
        {
          "title": "Evaluation of Inference Attack Models for Deep Learning on Medical Data",
          "authors": [
            "Maoqiang Wu",
            "Xinyue Zhang",
            "Jiahao Ding",
            "Hien Nguyen",
            "Rong Yu",
            "Miao Pan",
            "Stephen T. Wong"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Deep learning has attracted broad interest in healthcare and medical\ncommunities. However, there has been little research into the privacy issues\ncreated by deep networks trained for medical applications. Recently developed\ninference attack algorithms indicate that images and text records can be\nreconstructed by malicious parties that have the ability to query deep\nnetworks. This gives rise to the concern that medical images and electronic\nhealth records containing sensitive patient information are vulnerable to these\nattacks. This paper aims to attract interest from researchers in the medical\ndeep learning community to this important problem. We evaluate two prominent\ninference attack models, namely, attribute inference attack and model inversion\nattack. We show that they can reconstruct real-world medical images and\nclinical reports with high fidelity. We then investigate how to protect\npatients' privacy using defense mechanisms, such as label perturbation and\nmodel perturbation. We provide a comparison of attack results between the\noriginal and the medical deep learning models with defenses. The experimental\nevaluations show that our proposed defense approaches can effectively reduce\nthe potential privacy leakage of medical deep learning from the inference\nattacks.",
          "pdf_url": "http://arxiv.org/pdf/2011.00177v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2011.00177v1",
          "source": "arXiv"
        },
        {
          "title": "Generative Adversarial Network for Medical Images (MI-GAN)",
          "authors": [
            "Talha Iqbal",
            "Hazrat Ali"
          ],
          "year": 2018,
          "citations": 0,
          "abstract": "Deep learning algorithms produces state-of-the-art results for different\nmachine learning and computer vision tasks. To perform well on a given task,\nthese algorithms require large dataset for training. However, deep learning\nalgorithms lack generalization and suffer from over-fitting whenever trained on\nsmall dataset, especially when one is dealing with medical images. For\nsupervised image analysis in medical imaging, having image data along with\ntheir corresponding annotated ground-truths is costly as well as time consuming\nsince annotations of the data is done by medical experts manually. In this\npaper, we propose a new Generative Adversarial Network for Medical Imaging\n(MI-GAN). The MI-GAN generates synthetic medical images and their segmented\nmasks, which can then be used for the application of supervised analysis of\nmedical images. Particularly, we present MI-GAN for synthesis of retinal\nimages. The proposed method generates precise segmented images better than the\nexisting techniques. The proposed model achieves a dice coefficient of 0.837 on\nSTARE dataset and 0.832 on DRIVE dataset which is state-of-the-art performance\non both the datasets.",
          "pdf_url": "http://arxiv.org/pdf/1810.00551v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1810.00551v1",
          "source": "arXiv"
        },
        {
          "title": "Data efficient deep learning for medical image analysis: A survey",
          "authors": [
            "Suruchi Kumari",
            "Pravendra Singh"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "The rapid evolution of deep learning has significantly advanced the field of\nmedical image analysis. However, despite these achievements, the further\nenhancement of deep learning models for medical image analysis faces a\nsignificant challenge due to the scarcity of large, well-annotated datasets. To\naddress this issue, recent years have witnessed a growing emphasis on the\ndevelopment of data-efficient deep learning methods. This paper conducts a\nthorough review of data-efficient deep learning methods for medical image\nanalysis. To this end, we categorize these methods based on the level of\nsupervision they rely on, encompassing categories such as no supervision,\ninexact supervision, incomplete supervision, inaccurate supervision, and only\nlimited supervision. We further divide these categories into finer\nsubcategories. For example, we categorize inexact supervision into multiple\ninstance learning and learning with weak annotations. Similarly, we categorize\nincomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize\ncommonly used datasets for data efficient deep learning in medical image\nanalysis and investigate future research directions to conclude this survey.",
          "pdf_url": "http://arxiv.org/pdf/2310.06557v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2310.06557v1",
          "source": "arXiv"
        },
        {
          "title": "Deep AUC Maximization for Medical Image Classification: Challenges and\n  Opportunities",
          "authors": [
            "Tianbao Yang"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "In this extended abstract, we will present and discuss opportunities and\nchallenges brought about by a new deep learning method by AUC maximization (aka\n\\underline{\\bf D}eep \\underline{\\bf A}UC \\underline{\\bf M}aximization or {\\bf\nDAM}) for medical image classification. Since AUC (aka area under ROC curve) is\na standard performance measure for medical image classification, hence directly\noptimizing AUC could achieve a better performance for learning a deep neural\nnetwork than minimizing a traditional loss function (e.g., cross-entropy loss).\nRecently, there emerges a trend of using deep AUC maximization for large-scale\nmedical image classification. In this paper, we will discuss these recent\nresults by highlighting (i) the advancements brought by stochastic non-convex\noptimization algorithms for DAM; (ii) the promising results on various medical\nimage classification problems. Then, we will discuss challenges and\nopportunities of DAM for medical image classification from three perspectives,\nfeature learning, large-scale optimization, and learning trustworthy AI models.",
          "pdf_url": "http://arxiv.org/pdf/2111.02400v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2111.02400v1",
          "source": "arXiv"
        },
        {
          "title": "Medical Image Generation using Generative Adversarial Networks",
          "authors": [
            "Nripendra Kumar Singh",
            "Khalid Raza"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Generative adversarial networks (GANs) are unsupervised Deep Learning\napproach in the computer vision community which has gained significant\nattention from the last few years in identifying the internal structure of\nmultimodal medical imaging data. The adversarial network simultaneously\ngenerates realistic medical images and corresponding annotations, which proven\nto be useful in many cases such as image augmentation, image registration,\nmedical image generation, image reconstruction, and image-to-image translation.\nThese properties bring the attention of the researcher in the field of medical\nimage analysis and we are witness of rapid adaption in many novel and\ntraditional applications. This chapter provides state-of-the-art progress in\nGANs-based clinical application in medical image generation, and cross-modality\nsynthesis. The various framework of GANs which gained popularity in the\ninterpretation of medical images, such as Deep Convolutional GAN (DCGAN),\nLaplacian GAN (LAPGAN), pix2pix, CycleGAN, and unsupervised image-to-image\ntranslation model (UNIT), continue to improve their performance by\nincorporating additional hybrid architecture, has been discussed. Further, some\nof the recent applications of these frameworks for image reconstruction, and\nsynthesis, and future research directions in the area have been covered.",
          "pdf_url": "http://arxiv.org/pdf/2005.10687v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2005.10687v1",
          "source": "arXiv"
        },
        {
          "title": "Explainable artificial intelligence (XAI) in deep learning-based medical\n  image analysis",
          "authors": [
            "Bas H. M. van der Velden",
            "Hugo J. Kuijf",
            "Kenneth G. A. Gilhuijs",
            "Max A. Viergever"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "With an increase in deep learning-based methods, the call for explainability\nof such methods grows, especially in high-stakes decision making areas such as\nmedical image analysis. This survey presents an overview of eXplainable\nArtificial Intelligence (XAI) used in deep learning-based medical image\nanalysis. A framework of XAI criteria is introduced to classify deep\nlearning-based medical image analysis methods. Papers on XAI techniques in\nmedical image analysis are then surveyed and categorized according to the\nframework and according to anatomical location. The paper concludes with an\noutlook of future opportunities for XAI in medical image analysis.",
          "pdf_url": "http://arxiv.org/pdf/2107.10912v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2107.10912v1",
          "source": "arXiv"
        },
        {
          "title": "Reference Based Color Transfer for Medical Volume Rendering",
          "authors": [
            "Sudarshan Devkota",
            "Summanta Pattanaik"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "The benefits of medical imaging are enormous. Medical images provide\nconsiderable amounts of anatomical information and this facilitates medical\npractitioners in performing effective disease diagnosis and deciding upon the\nbest course of medical treatment. A transition from traditional monochromatic\nmedical images like CT scans, X-Rays or MRI images to a colored 3D\nrepresentation of the anatomical structure further enhances the capabilities of\nmedical professionals in extracting valuable medical information. The proposed\nframework in our research starts with performing color transfer by finding deep\nsemantic correspondence between two medical images: a colored reference image,\nand a monochromatic CT scan or an MRI image. We extend this idea of\nreference-based colorization technique to perform colored volume rendering from\na stack of grayscale medical images. Furthermore, we also propose to use an\neffective reference image recommendation system to aid in the selection of good\nreference images. With our approach, we successfully perform colored medical\nvolume visualization and essentially eliminate the painstaking process of user\ninteraction with a transfer function to obtain color and opacity parameters for\nvolume rendering.",
          "pdf_url": "http://arxiv.org/pdf/2210.08083v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2210.08083v1",
          "source": "arXiv"
        },
        {
          "title": "A Survey on Incorporating Domain Knowledge into Deep Learning for\n  Medical Image Analysis",
          "authors": [
            "Xiaozheng Xie",
            "Jianwei Niu",
            "Xuefeng Liu",
            "Zhengsu Chen",
            "Shaojie Tang",
            "Shui Yu"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Although deep learning models like CNNs have achieved great success in\nmedical image analysis, the small size of medical datasets remains a major\nbottleneck in this area. To address this problem, researchers have started\nlooking for external information beyond current available medical datasets.\nTraditional approaches generally leverage the information from natural images\nvia transfer learning. More recent works utilize the domain knowledge from\nmedical doctors, to create networks that resemble how medical doctors are\ntrained, mimic their diagnostic patterns, or focus on the features or areas\nthey pay particular attention to. In this survey, we summarize the current\nprogress on integrating medical domain knowledge into deep learning models for\nvarious tasks, such as disease diagnosis, lesion, organ and abnormality\ndetection, lesion and organ segmentation. For each task, we systematically\ncategorize different kinds of medical domain knowledge that have been utilized\nand their corresponding integrating methods. We also provide current challenges\nand directions for future research.",
          "pdf_url": "http://arxiv.org/pdf/2004.12150v4.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2004.12150v4",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "AI shoulder implant identification",
      "final_query": "AI shoulder implant identification",
      "attempts": 1,
      "paper_count": 20,
      "open_access_count": 14,
      "papers": [
        {
          "title": "SSP: self-supervised pertaining technique for classification of shoulder implants in x-ray medical images: a broad experimental study",
          "authors": [
            "Laith Alzubaidi",
            "M. Fadhel",
            "Freek Hollman",
            "Asma Salhi",
            "José I. Santamaría",
            "Ye Duan",
            "Ashish Gupta",
            "K. Cutbush",
            "Amin Abbosh",
            "Yuantong Gu"
          ],
          "year": 2024,
          "citations": 12,
          "abstract": "Multiple pathologic conditions can lead to a diseased and symptomatic glenohumeral joint for which total shoulder arthroplasty (TSA) replacement may be indicated. The long-term survival of implants is limited. With the increasing incidence of joint replacement surgery, it can be anticipated that joint replacement revision surgery will become more common. It can be challenging at times to retrieve the manufacturer of the in situ implant. Therefore, certain systems facilitated by AI techniques such as deep learning (DL) can help correctly identify the implanted prosthesis. Correct identification of implants in revision surgery can help reduce perioperative complications and complications. DL was used in this study to categorise different implants based on X-ray images into four classes (as a first case study of the small dataset): Cofield, Depuy, Tornier, and Zimmer. Imbalanced and small public datasets for shoulder implants can lead to poor performance of DL model training. Most of the methods in the literature have adopted the idea of transfer learning (TL) from ImageNet models. This type of TL has been proven ineffective due to some concerns regarding the contrast between features learnt from natural images (ImageNet: colour images) and shoulder implants in X-ray images (greyscale images). To address that, a new TL approach (self-supervised pertaining (SSP)) is proposed to resolve the issue of small datasets. The SSP approach is based on training the DL models (ImageNet models) on a large number of unlabelled greyscale medical images in the domain to update the features. The models are then trained on a small labelled data set of X-ray images of shoulder implants. The SSP shows excellent results in five ImageNet models, including MobilNetV2, DarkNet19, Xception, InceptionResNetV2, and EfficientNet with precision of 96.69%, 95.45%, 98.76%, 98.35%, and 96.6%, respectively. Furthermore, it has been shown that different domains of TL (such as ImageNet) do not significantly affect the performance of shoulder implants in X-ray images. A lightweight model trained from scratch achieves 96.6% accuracy, which is similar to using standard ImageNet models. The features extracted by the DL models are used to train several ML classifiers that show outstanding performance by obtaining an accuracy of 99.20% with Xception+SVM. Finally, extended experimentation has been carried out to elucidate our approach’s real effectiveness in dealing with different medical imaging scenarios. Specifically, five different datasets are trained and tested with and without the proposed SSP, including the shoulder X-ray with an accuracy of 99.47% and CT brain stroke with an accuracy of 98.60%.",
          "pdf_url": "https://doi.org/10.1007/s10462-024-10878-0",
          "doi": "10.1007/s10462-024-10878-0",
          "url": "https://www.semanticscholar.org/paper/8893cd60d17ea999c4e80033f47ba8572fa3da17",
          "source": "Semantic Scholar"
        },
        {
          "title": "Debridement Technique for Single-Stage Revision Shoulder Arthroplasty.",
          "authors": [
            "Logan C. Kolakowski",
            "Monica J. Stadecker",
            "Justin Givens",
            "Christian M. Schmidt",
            "M. Mighell",
            "Kaitlyn N. Christmas",
            "M. Frankle"
          ],
          "year": 2024,
          "citations": 1,
          "abstract": "Background\nThe incidence of revision shoulder arthroplasty continues to rise, and infection is a common indication for revision surgery. Treatment of periprosthetic joint infection (PJI) in the shoulder remains a controversial topic, with the literature reporting varying methodologies, including the use of debridement and implant retention, single-stage and 2-stage surgeries, antibiotic spacers, and resection arthroplasty20. Single-stage revision has been shown to have a low rate of recurrent infection, making it more favorable because it precludes the morbidity of a 2-stage operation. The present video article describes a meticulous debridement technique as it applies to revision shoulder arthroplasty.\n\n\nDescription\nThe previous deltopectoral incision should be utilized, with extension 1 to 1.5 cm proximally and distally, removing any draining sinuses. First, develop subcutaneous flaps above the muscle layer to better establish normal tissue planes. A large medial subcutaneous flap will allow for identification of the superior border of the pectoralis major. The pectoralis can be traced laterally to its humeral insertion, which is often in confluence with the deltoid insertion. Hohmann retractors can be placed sequentially, working distal to proximal, under the deltoid in order to recreate the subdeltoid space. Next, reestablish the subpectoral space by releasing any scar tissue tethering the pectoralis muscle and conjoined tendon. Dislocate the prosthesis and remove modular components. Restore the subcoracoid space by dissecting between the subscapularis and the conjoined tendon, allowing for axillary nerve identification. Complete a full capsular excision circumferentially around the glenoid, taking care to protect the axillary nerve as it passes from the subcoracoid space under the inferior glenoid to the deltoid muscle. The decision to remove well-fixed components should be made by the surgeon. Any exposed osseous surfaces should undergo debridement to reduce bacterial burden. Reimplantation should focus on obtaining stable bone-implant interfaces to minimize any micromotion that may increase risk of reinfection. Our preference is to irrigate with 9 L of normal saline solution, Irrisept (Irrimax), and Bactisure Wound Lavage (Zimmer Biomet). Multiple cultures should be taken and followed carefully postoperatively to allow tailoring of the antibiotic regimen with infectious disease specialists.\n\n\nAlternatives\nTwo-stage revision is the most common alternative treatment for shoulder PJI and consists of removal of components, debridement, and delayed component reimplantation; however, it requires at least 1 return to the operating room for definitive treatment.\n\n\nRationale\nSerum laboratory studies and joint aspiration are not reliable predictors of shoulder PJI because of the high rate of Cutibacterium acnes infections21,22. The incidence of unexpected positive cultures in seemingly aseptic revisions ranges from 11% to 52.2%6-8,23,24. It is prudent for all revision shoulder arthroplasties to be treated as involving a presumed infection, with thorough debridement, because of the high rate of unexpected positive cultures and the greater prevalence of low-virulence organisms in shoulder arthroplasty for PJI.\n\n\nExpected Outcomes\nThe International Consensus Meeting guidelines for PJI were developed in 2018, and patients with higher Infection Probability Scores are theorized to have higher rates of recurrence19,21. With meticulous debridement, the rate of recurrent infections requiring reoperation is just 5% following 1-stage revision shoulder arthroplasty, averaged across all Infection Probability Scores19.\n\n\nImportant Tips\nEnsure that an adequate incision is made in order to allow for identification of the deltoid origin on the clavicle and insertion on the humerus.The superior border of the pectoralis major can be traced laterally to the humerus to correctly identify the deltopectoral interval.Subdeltoid dissection is complete when you are able to identify deep deltoid fibers superficially, rotator cuff tendon posteriorly, and humeral bone. Exposure can be improved by abducting and internally rotating the humerus.Capsule excision around the glenoid is complete when the subscapularis can be visualized anteriorly, the fatty tissue of the inferior glenoid space inferiorly, and the rotator cuff tendon (or subdeltoid space if the cuff is absent) posteriorly and superiorly.\n\n\nAcronyms and Abbreviations\nPJI = periprosthetic joint infectionC. acnes = Cutibacterium acnesUPC = unexpected positive cultureIS score = Infection Probability ScoreDAIR = debridement, antibiotics, and implant retentionCT = computed tomographyWBC = white blood cellCRP = C-reactive proteinESR = erythrocyte sedimentation rateCHG = chlorhexidine gluconateAC = acromioclavicularGT = greater tuberositySGHL = superior glenohumeral ligament.",
          "pdf_url": "",
          "doi": "10.2106/jbjs.st.23.00093",
          "url": "https://www.semanticscholar.org/paper/482815d2cfaa676af6aba96d7c8b68c313594fe6",
          "source": "Semantic Scholar"
        },
        {
          "title": "Implementação de rotação de trabalho em uma metalúrgica de produtos eletrônicos.",
          "authors": [
            "A. R. Ribeiro",
            "Carla Reny Tagamori",
            "André Ricardo Césares Arruda",
            "D. Bertoncello"
          ],
          "year": 2007,
          "citations": 0,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.15675/GEPROS.V0I3.169",
          "url": "https://www.semanticscholar.org/paper/947dda4221c812e56881db39475f3bb86ae5a8ce",
          "source": "Semantic Scholar"
        },
        {
          "title": "Identification of dental implant systems from low-quality and distorted dental radiographs using AI trained on a large multi-center dataset",
          "authors": [
            "Jae-Hong Lee",
            "Young-Taek Kim",
            "Jong-Bin Lee"
          ],
          "year": 2024,
          "citations": 13,
          "abstract": "Most artificial intelligence (AI) studies have attempted to identify dental implant systems (DISs) while excluding low-quality and distorted dental radiographs, limiting their actual clinical use. This study aimed to evaluate the effectiveness of an AI model, trained on a large and multi-center dataset, in identifying different types of DIS in low-quality and distorted dental radiographs. Based on the fine-tuned pre-trained ResNet-50 algorithm, 156,965 panoramic and periapical radiological images were used as training and validation datasets, and 530 low-quality and distorted images of four types (including those not perpendicular to the axis of the fixture, radiation overexposure, cut off the apex of the fixture, and containing foreign bodies) were used as test datasets. Moreover, the accuracy performance of low-quality and distorted DIS classification was compared using AI and five periodontists. Based on a test dataset, the performance evaluation of the AI model achieved accuracy, precision, recall, and F1 score metrics of 95.05%, 95.91%, 92.49%, and 94.17%, respectively. However, five periodontists performed the classification of nine types of DISs based on four different types of low-quality and distorted radiographs, achieving a mean overall accuracy of 37.2 ± 29.0%. Within the limitations of this study, AI demonstrated superior accuracy in identifying DIS from low-quality or distorted radiographs, outperforming dental professionals in classification tasks. However, for actual clinical application of AI, extensive standardization research on low-quality and distorted radiographic images is essential.",
          "pdf_url": "https://www.nature.com/articles/s41598-024-63422-z.pdf",
          "doi": "10.1038/s41598-024-63422-z",
          "url": "https://www.semanticscholar.org/paper/62e984d2bd50fdc6a27cf607fc910aead8e1daf2",
          "source": "Semantic Scholar"
        },
        {
          "title": "Machine Learning Model for Classification of Shoulder Implant Manufacturer Using X-Ray Images",
          "authors": [
            "Idris Djibo",
            "I. Z. Yakubu",
            "Muhammad Raiyan",
            "G. Manikandan",
            "Fatima Shittu",
            "Z. Musa"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Recently, synthetic prosthesis built from metals and plastic components are often used to mitigate pain and restore functions of injured human shoulders. The procedure involves the replacement of the affected shoulder ball and socket joint with the synthetically generated prosthesis. Long after the replacement of the affected part, the synthetic prosthesis maybe damaged or worn out, thus the reoperation process maybe repeated or revised. To guarantee a robust and seamless reoperation, detail of the model and manufacturer of the synthetic prosthesis is a paramount. There are circumstances where information regarding the prosthesis is not available. To determine the model and manufacturer, a thorough inspection and physical analogy of various manufacturers’ prosthesis are therefore required. The manual method consumes longer time and is liable to errors. With the evolution and continuous growth in the field of Machine Learning, prediction models can be used to learn the prosthesis and predict the model and manufacturer, thus reducing the time and error in the manual approach. In this paper, a model based on ensemble learning for identification of the manufacturer of synthetic prosthesis is proposed. Deep Convolution Neural Network (DCNN), High Resolution Network (HRNet), and Support Vector Machine (SVM) were combined to form the ensemble model. To ensure accurate prediction of the manufacturer, the components of the ensemble model are separately trained and then integrated using a novel weighted average ensemble technique. This strategy aids in identifying the prosthesis' manufacturer by assigning greater weight to the model that performs the best. The proposed model performance is compared with the performance of its individual components, where it outperforms the individual models in terms of accuracy, recall, precision and f measures.",
          "pdf_url": "",
          "doi": "10.1109/ICAST61769.2024.10856466",
          "url": "https://www.semanticscholar.org/paper/0548261ee0921a5cb04e43cc17ccc45ce765326e",
          "source": "Semantic Scholar"
        },
        {
          "title": "AI-Driven Orthopedic Implant Identification in Indian Clinical Practice: A Dynamic Cross-Attention Swin Transformer Approach",
          "authors": [
            "G. Malathi",
            "B. Latha"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1007/s43465-025-01432-3",
          "url": "https://www.semanticscholar.org/paper/d58afa8d823d62530ba8eabca34aee12f595baeb",
          "source": "Semantic Scholar"
        },
        {
          "title": "Accuracy of Artificial Intelligence Models in Dental Implant Fixture Identification and Classification from Radiographs: A Systematic Review",
          "authors": [
            "Wael I. Ibraheem"
          ],
          "year": 2024,
          "citations": 10,
          "abstract": "Background and Objectives: The availability of multiple dental implant systems makes it difficult for the treating dentist to identify and classify the implant in case of inaccessibility or loss of previous records. Artificial intelligence (AI) is reported to have a high success rate in medical image classification and is effectively used in this area. Studies have reported improved implant classification and identification accuracy when AI is used with trained dental professionals. This systematic review aims to analyze various studies discussing the accuracy of AI tools in implant identification and classification. Methods: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed, and the study was registered with the International Prospective Register of Systematic Reviews (PROSPERO). The focused PICO question for the current study was “What is the accuracy (outcome) of artificial intelligence tools (Intervention) in detecting and/or classifying the type of dental implant (Participant/population) using X-ray images?” Web of Science, Scopus, MEDLINE-PubMed, and Cochrane were searched systematically to collect the relevant published literature. The search strings were based on the formulated PICO question. The article search was conducted in January 2024 using the Boolean operators and truncation. The search was limited to articles published in English in the last 15 years (January 2008 to December 2023). The quality of all the selected articles was critically analyzed using the Quality Assessment and Diagnostic Accuracy Tool (QUADAS-2). Results: Twenty-one articles were selected for qualitative analysis based on predetermined selection criteria. Study characteristics were tabulated in a self-designed table. Out of the 21 studies evaluated, 14 were found to be at risk of bias, with high or unclear risk in one or more domains. The remaining seven studies, however, had a low risk of bias. The overall accuracy of AI models in implant detection and identification ranged from a low of 67% to as high as 98.5%. Most included studies reported mean accuracy levels above 90%. Conclusions: The articles in the present review provide considerable evidence to validate that AI tools have high accuracy in identifying and classifying dental implant systems using 2-dimensional X-ray images. These outcomes are vital for clinical diagnosis and treatment planning by trained dental professionals to enhance patient treatment outcomes.",
          "pdf_url": "https://www.mdpi.com/2075-4418/14/8/806/pdf?version=1712837129",
          "doi": "10.3390/diagnostics14080806",
          "url": "https://www.semanticscholar.org/paper/66e4e1b508cdcfad12704d72d182d81769bb8762",
          "source": "Semantic Scholar"
        },
        {
          "title": "EFFICACY OF ARTIFICIAL INTELLIGENCE-BASED MODELS FOR SHOULDER ARTHROPLASTY IMPLANT DETECTION AND CLASSIFICATION USING UPPER LIMB RADIOGRAPHS: A SYSTEMATIC REVIEW AND META-ANALYSIS",
          "authors": [
            "A.M. Asgari",
            "F. Shaker",
            "M. Fallahy",
            "M. Soleimani",
            "S. Shafiei",
            "Y. Fallah"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Shoulder arthroplasty (SA) has been performed with different types of implants, each requiring different replacement systems. However, data on previously utilized implant types are not always available before revision surgery, which is paramount to determining the appropriate equipment and procedure. Therefore, this meta-analysis aimed to evaluate the accuracy of the AI models in classifying SA implant types.This systematic review was conducted in Pubmed, Embase, SCOPUS, and Web of Science from inception to December 2023, according to PRISMA guidelines. Peer-reviewed research evaluating the accuracy of AI-based tools on upper-limb X-rays for recognizing and categorizing SA implants was included. In addition to the overall meta-analysis, subgroup analysis was performed according to the type of AI model applied (CNN (Convolutional neural network), non-CNN, or Combination of both) and the similarity of utilized datasets between studies.13 articles were eligible for inclusion in this meta-analysis (including 138 different tests assessing models’ efficacy). Our meta-analysis demonstrated an overall sensitivity and specificity of 0.891 (95% CI:0.866-0.912) and 0.549 (95% CI:0.532,0.566) for classifying implants in SA, respectively. The results of our subgroup analyses were as follows: CNN-subgroup: a sensitivity of 0.898 (95% CI:0.873-0.919) and a specificity of 0.554 (95% CI:0.537,0.570), Non-CNN subgroup: a sensitivity of 0.809 (95% CI:0.665-0.900) and specificity of 0.522 (95% CI:0.440,0.603), combined subgroup: a sensitivity of 0.891 (95% CI:0.752-0.957) and a specificity of 0.547 (95% CI:0.463,0.629).Studies using the same dataset demonstrated an overall sensitivity and specificity of 0.881 (95% CI:0.856-0.903) and 0.542 (95% CI:0.53,0.554), respectively. Studies that used other datasets showed an overall sensitivity and specificity of 0.995 (95% CI:969,0.999) and 0.678 (95% CI:0.234, 0.936), respectively.AI-based classification of shoulder implant types can be considered a sensitive method. Our study showed the potential role of using CNN-based models and different datasets to enhance accuracy, which could be investigated in future studies.",
          "pdf_url": "",
          "doi": "10.1302/1358-992x.2024.18.060",
          "url": "https://www.semanticscholar.org/paper/49cbb15e4f895dee2c29e1f245ce37b650d141d4",
          "source": "Semantic Scholar"
        },
        {
          "title": "Artificial intelligence can be used in the identification and classification of shoulder osteoarthritis and avascular necrosis on plain radiographs: a training study of 7,139 radiograph sets",
          "authors": [
            "Martin Magnéli",
            "Michael Axenhus",
            "Johan Fagrell",
            "Petter Ling",
            "Jacob Gislén",
            "Yilmaz Demir",
            "Erica Domeij-Arverud",
            "Kristofer Hallberg",
            "Björn Salomonsson",
            "Max Gordon"
          ],
          "year": 2024,
          "citations": 4,
          "abstract": "Background and purpose Knowledge concerning the use AI models for the classification of glenohumeral osteoarthritis (GHOA) and avascular necrosis (AVN) of the humeral head is lacking. We aimed to analyze how a deep learning (DL) model trained to identify and grade GHOA on plain radiographs performs. Our secondary aim was to train a DL model to identify and grade AVN on plain radiographs. Patients and methods A modified ResNet-type network was trained on a dataset of radiographic shoulder examinations from a large tertiary hospital. A total of 7,139 radiographs were included. The dataset included various projections of the shoulder, and the network was trained using stochastic gradient descent. Performance evaluation metrics, area under the receiver operating characteristic curve (AUC), sensitivity, and specificity were used to assess the network’s performance for each outcome. Results The network demonstrated AUC values ranging from 0.73 to 0.93 for GHOA classification and > 0.90 for all AVN classification classes. The network exhibited lower AUC for mild cases compared with definitive cases of GHOA. When none and mild grades were combined, the AUC increased, suggesting difficulties in distinguishing between these 2 grades. Conclusion We found that a DL model can be trained to identify and grade GHOA on plain radiographs. Furthermore, we show that a DL model can identify and grade AVN on plain radiographs. The network performed well, particularly for definitive cases of GHOA and any level of AVN. However, challenges remain in distinguishing between none and mild GHOA grades.",
          "pdf_url": "https://actaorthop.org/actao/article/download/40905/46352",
          "doi": "10.2340/17453674.2024.40905",
          "url": "https://www.semanticscholar.org/paper/0a3f6cc1898656279a8fc6767cc0aca86940aba2",
          "source": "Semantic Scholar"
        },
        {
          "title": "Dental Implant Identification Methods",
          "authors": [
            "Veena B Benakatti"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Dental implants are a well-accepted prosthetic alternative for missing teeth. After implant restoration, they will need maintenance in due course of time due to biological and mechanical complications, during which information about the implant system is required. Until today there is no well-established method for implant identification and conventional tools such as interpretation from radiographs need time and effort. Researchers have proposed several methods for implant identification and the review focuses on a comprehensive discussion of the proposed methods. For this review, comprehensive data from databases, including PubMed, Scopus, Web of Science, Cochrane, and Google Scholar, was thoroughly examined ensuring the most up-to-date and relevant information regarding implant identification. The proposed methods include an interpretation from radiographs based on the implant design specifications listed, implant records, implant recognition software, retrieving implant information through a wireless reader from a radiofrequency chip fitted into an implant screw hole, QR-encoded implant identification wallet, bar code encryption by implant manufacturers, incorporating laser-etched batch and serial numbers in implant collars, Sharma Jhingta system of implant identification and artificial intelligence methods. Amongst existing methods, AI research shows potential in offering a quick and accurate method of implant identification however developing a robust AI model with a comprehensive database is a complex task and requires considerable effort and time.",
          "pdf_url": "",
          "doi": "10.55995/j-cpi.2024008",
          "url": "https://www.semanticscholar.org/paper/73b63e31e4d1a5b83f0618c5c314c16968d7e97b",
          "source": "Semantic Scholar"
        },
        {
          "title": "Shoulder Implant X-Ray Manufacturer Classification: Exploring with\n  Vision Transformer",
          "authors": [
            "Meng Zhou",
            "Shanglin Mo"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Shoulder replacement surgery, also called total shoulder replacement, is a\ncommon and complex surgery in Orthopedics discipline. It involves replacing a\ndead shoulder joint with an artificial implant. In the market, there are many\nartificial implant manufacturers and each of them may produce different\nimplants with different structures compares to other providers. The problem\narises in the following situation: a patient has some problems with the\nshoulder implant accessory and the manufacturer of that implant maybe unknown\nto either the patient or the doctor, therefore, correctly identification of the\nmanufacturer is the key prior to the treatment. In this paper, we will\ndemonstrate different methods for classifying the manufacturer of a shoulder\nimplant. We will use Vision Transformer approach to this task for the first\ntime ever",
          "pdf_url": "http://arxiv.org/pdf/2104.07667v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2104.07667v2",
          "source": "arXiv"
        },
        {
          "title": "Computational simulation of bone remodelling post reverse total shoulder\n  arthroplasty",
          "authors": [
            "H. Liedtke",
            "A. T. McBride",
            "S. Sivarasu",
            "S. Roche"
          ],
          "year": 2017,
          "citations": 0,
          "abstract": "Bone is a living material. It adapts, in an optimal sense, to loading by\nchanging its density and trabeculae architecture - a process termed\nremodelling. Implanted orthopaedic devices can significantly alter the loading\non the surrounding bone, which can have a detrimental impact on bone ingrowth\nthat is critical to ensure secure implant fixation. In this contribution, a\ncomputational model that accounts for bone remodelling is developed to\nelucidate the response of bone following a reverse shoulder procedure for\nrotator cuff deficient patients. The physical process of remodelling is\nmodelled using continuum scale, open system thermodynamics whereby the density\nof bone evolves isotropically in response to the loading it experiences. The\nfully-nonlinear continuum theory is solved approximately using the finite\nelement method. The code developed to model the reverse shoulder procedure is\nvalidated using a series of benchmark problems.",
          "pdf_url": "http://arxiv.org/pdf/1705.08324v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1705.08324v1",
          "source": "arXiv"
        },
        {
          "title": "Influence of Rotator Cuff Integrity on Loading and Kinematics Before and\n  After Reverse Shoulder Arthroplasty",
          "authors": [
            "Fabien Péan",
            "Philippe Favre",
            "Orcun Goksel"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Reverse Shoulder Arthroplasty (RSA) has become a very common procedure for\nshoulder joint replacement, even for scenarios where an anatomical\nreconstruction would traditionally be used. In this study, we investigate joint\nreaction forces and scapular kinematics for rotator cuff tears of different\ntendons with and without a reverse prosthesis. Available motion capture data\nduring anterior flexion was input to a finite-element musculoskeletal shoulder\nmodel, and muscle activations were computed using inverse dynamics. The model\nwas validated with respect to in-vivo glenohumeral joint reaction force (JRF)\nmeasurements, and also compared to existing clinical and biomechanical data.\nSimulations were carried out for the intact joint as well as for various\ntendons involved in a rotator cuff tear: superior (supraspinatus),\nsuperior-anterior (supraspinatus and subscapularis), and superior-posterior\n(supraspinatus, infraspinatus and teres minor). Each rotator cuff tear\ncondition was repeated after shifting the humerus and the glenohumeral joint\ncenter of rotation to simulate the effect of a reverse prosthesis. Changes in\ncompressive, shear, and total JRF were analysed, along with scapular upward\nrotation. The model compared favourably to in-vivo measurements, and existing\nclinical and biomechanical knowledge. Simulated JRF lie in the ranges of\nin-vivo JRF measurements and shows a linear increase past 90 degree flexion.\nImplanting a reverse prosthesis with a functional rotator cuff or with an\nisolated supraspinatus tear led to over 2 times higher compressive force\ncomponent than with massive rotator cuff tears (superior-anterior or\nsuperior-posterior). Such higher compression forces might increase the risk of\nwear and implant fracture.",
          "pdf_url": "http://arxiv.org/pdf/2012.09763v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2012.09763v1",
          "source": "arXiv"
        },
        {
          "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture\n  Detection in Clinical Radiographs",
          "authors": [
            "Hemanth Kumar M",
            "Karthika M",
            "Saianiruth M",
            "Vasanthakumar Venugopal",
            "Anandakumar D",
            "Revathi Ezhumalai",
            "Charulatha K",
            "Kishore Kumar J",
            "Dayana G",
            "Kalyan Sivasailam",
            "Bargava Subramanian"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.",
          "pdf_url": "http://arxiv.org/pdf/2507.13408v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2507.13408v1",
          "source": "arXiv"
        },
        {
          "title": "Design and Implementation of a Hybrid Wireless Power and Communication\n  System for Medical Implants",
          "authors": [
            "A. Khaleghi",
            "A. Hasanvand",
            "I. Balasingham"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Data collection and analysis from multiple implant nodes in humans can\nprovide targeted medicine and treatment strategies that can prevent many\nchronic diseases. This data can be collected for a long time and processed\nusing artificial intelligence (AI) techniques in a medical network for early\ndetection and prevention of diseases. Additionally, machine learning (ML)\nalgorithms can be applied for the analysis of big data for health monitoring of\nthe population. Wireless powering, sensing, and communication are essential\nparts of future wireless implants that aim to achieve the aforementioned goals.\nIn this paper, we present the technical development of a wireless implant that\nis powered by radio frequency (RF) at 401 MHz, with the sensor data being\ncommunicated to an on-body reader. The implant communication is based on two\nsimultaneous wireless links: RF backscatter for implant-to-on-body\ncommunication and a galvanic link for intra-body implant-to-implant\nconnectivity. It is demonstrated that RF powering, using the proposed compact\nantennas, can provide an efficient and integrable system for powering up to an\n8 cm depth inside body tissues. Furthermore, the same antennas are utilized for\nbackscatter and galvanic communication.",
          "pdf_url": "http://arxiv.org/pdf/2311.08933v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2311.08933v1",
          "source": "arXiv"
        },
        {
          "title": "Detecting total hip replacement prosthesis design on preoperative\n  radiographs using deep convolutional neural network",
          "authors": [
            "Alireza Borjali",
            "Antonia F. Chen",
            "Orhun K. Muratoglu",
            "Mohammad A. Morid",
            "Kartik M. Varadarajan"
          ],
          "year": 2019,
          "citations": 0,
          "abstract": "Identifying the design of a failed implant is a key step in preoperative\nplanning of revision total joint arthroplasty. Manual identification of the\nimplant design from radiographic images is time consuming and prone to error.\nFailure to identify the implant design preoperatively can lead to increased\noperating room time, more complex surgery, increased blood loss, increased bone\nloss, increased recovery time, and overall increased healthcare costs. In this\nstudy, we present a novel, fully automatic and interpretable approach to\nidentify the design of total hip replacement (THR) implants from plain\nradiographs using deep convolutional neural network (CNN). CNN achieved 100%\naccuracy in identification of three commonly used THR implant designs. Such CNN\ncan be used to automatically identify the design of a failed THR implant\npreoperatively in just a few seconds, saving time and improving the\nidentification accuracy. This can potentially improve patient outcomes, free\npractitioners time, and reduce healthcare costs.",
          "pdf_url": "http://arxiv.org/pdf/1911.12387v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1911.12387v1",
          "source": "arXiv"
        },
        {
          "title": "Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of\n  Person Re-Identification",
          "authors": [
            "Boqiang Xu",
            "Lingxiao He",
            "Xingyu Liao",
            "Wu Liu",
            "Zhenan Sun",
            "Tao Mei"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Person re-identification (Re-ID) aims at retrieving an input person image\nfrom a set of images captured by multiple cameras. Although recent Re-ID\nmethods have made great success, most of them extract features in terms of the\nattributes of clothing (e.g., color, texture). However, it is common for people\nto wear black clothes or be captured by surveillance systems in low light\nillumination, in which cases the attributes of the clothing are severely\nmissing. We call this problem the Black Re-ID problem. To solve this problem,\nrather than relying on the clothing information, we propose to exploit\nhead-shoulder features to assist person Re-ID. The head-shoulder adaptive\nattention network (HAA) is proposed to learn the head-shoulder feature and an\ninnovative ensemble method is designed to enhance the generalization of our\nmodel. Given the input person image, the ensemble method would focus on the\nhead-shoulder feature by assigning a larger weight if the individual insides\nthe image is in black clothing. Due to the lack of a suitable benchmark dataset\nfor studying the Black Re-ID problem, we also contribute the first Black-reID\ndataset, which contains 1274 identities in training set. Extensive evaluations\non the Black-reID, Market1501 and DukeMTMC-reID datasets show that our model\nachieves the best result compared with the state-of-the-art Re-ID methods on\nboth Black and conventional Re-ID problems. Furthermore, our method is also\nproved to be effective in dealing with person Re-ID in similar clothing. Our\ncode and dataset are avaliable on https://github.com/xbq1994/.",
          "pdf_url": "http://arxiv.org/pdf/2008.08528v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2008.08528v1",
          "source": "arXiv"
        },
        {
          "title": "Impact of surface and laser-induced noise on the spectral stability of\n  implanted nitrogen-vacancy centers in diamond",
          "authors": [
            "Srivatsa Chakravarthi",
            "Christian Pederson",
            "Zeeshawn Kazi",
            "Andrew Ivanov",
            "Kai-Mei C. Fu"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Scalable realizations of quantum network technologies utilizing the nitrogen\nvacancy center in diamond require creation of optically coherent NV centers in\nclose proximity to a surface for coupling to optical structures. We create\nsingle NV centers by $^{15}$N ion implantation and high-temperature vacuum\nannealing. Origin of the NV centers is established by optically detected\nmagnetic resonance spectroscopy for nitrogen isotope identification. Near\nlifetime-limited optical linewidths ($<$ 60 MHz) are observed for the majority\nof the normal-implant (7$^\\circ$, $\\approx$ 100 nm deep) $^{15}$NV centers.\nLong-term stability of the NV$^-$ charge state and emission frequency is\ndemonstrated. The effect of NV-surface interaction is investigated by varying\nthe implantation angle for a fixed ion-energy, and thus lattice damage profile.\nIn contrast to the normal implant condition, NVs from an oblique-implant\n(85$^\\circ$, $\\approx$ 20 nm deep) exhibit substantially reduced optical\ncoherence. Our results imply that the surface is a larger source of\nperturbation than implantation damage for shallow implanted NVs. This work\nsupports the viability of ion implantation for formation of optically stable NV\ncenters. However, careful surface preparation will be necessary for scalable\ndefect engineering.",
          "pdf_url": "http://arxiv.org/pdf/2105.09483v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2105.09483v2",
          "source": "arXiv"
        },
        {
          "title": "Zydeco-Style Spike Sorting Low Power VLSI Architecture for IoT BCI\n  Implants",
          "authors": [
            "Zag ElSayed",
            "Murat Ozer",
            "Nelly Elsayed",
            "Magdy Bayoumi"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "Brain Computer Interface (BCI) has great potential for solving many brain\nsignal analysis limitations, mental disorder resolutions, and restoring missing\nlimb functionality via neural-controlled implants. However, there is no single\navailable, and safe implant for daily life usage exists yet. Most of the\nproposed implants have several implementation issues, such as infection hazards\nand heat dissipation, which limits their usability and makes it more\nchallenging to pass regulations and quality control production. The wireless\nimplant does not require a chronic wound in the skull. However, the current\ncomplex clustering neuron identification algorithms inside the implant chip\nconsume a lot of power and bandwidth, causing higher heat dissipation issues\nand draining the implant's battery. The spike sorting is the core unit of an\ninvasive BCI chip, which plays a significant role in power consumption,\naccuracy, and area. Therefore, in this study, we propose a low-power adaptive\nsimplified VLSI architecture, \"Zydeco-Style,\" for BCI spike sorting that is\ncomputationally less complex with higher accuracy that performs up to 93.5% in\nthe worst-case scenario. The architecture uses a low-power Bluetooth Wireless\ncommunication module with external IoT medical ICU devices. The proposed\narchitecture was implemented and simulated in Verilog. In addition, we are\nproposing an implant conceptual design.",
          "pdf_url": "http://arxiv.org/pdf/2209.04427v3.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2209.04427v3",
          "source": "arXiv"
        },
        {
          "title": "An intracardiac electrogram model to bridge virtual hearts and\n  implantable cardiac devices",
          "authors": [
            "Weiwei Ai",
            "Nitish Patel",
            "Partha Roop",
            "Avinash Malik",
            "Nathan Allen",
            "Mark L. Trew"
          ],
          "year": 2017,
          "citations": 0,
          "abstract": "Virtual heart models have been proposed to enhance the safety of implantable\ncardiac devices through closed loop validation. To communicate with a virtual\nheart, devices have been driven by cardiac signals at specific sites. As a\nresult, only the action potentials of these sites are sensed. However, the real\ndevice implanted in the heart will sense a complex combination of near and\nfar-field extracellular potential signals. Therefore many device functions,\nsuch as blanking periods and refractory periods, are designed to handle these\nunexpected signals. To represent these signals, we develop an intracardiac\nelectrogram (IEGM) model as an interface between the virtual heart and the\ndevice. The model can capture not only the local excitation but also far-field\nsignals and pacing afterpotentials. Moreover, the sensing controller can\nspecify unipolar or bipolar electrogram (EGM) sensing configurations and\nintroduce various oversensing and undersensing modes. The simulation results\nshow that the model is able to reproduce clinically observed sensing problems,\nwhich significantly extends the capabilities of the virtual heart model in the\ncontext of device validation.",
          "pdf_url": "http://arxiv.org/pdf/1703.01107v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1703.01107v1",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Automated implant manufacturer classification",
      "final_query": "Automated implant manufacturer classification",
      "attempts": 1,
      "paper_count": 20,
      "open_access_count": 14,
      "papers": [
        {
          "title": "Orthopedic Implant Fixation Type Detection in Hip Arthroplasty Using Two-Staged Low-Shot Network",
          "authors": [
            "Aparna Kanakatte",
            "Divya Bhatia",
            "Rupsha Mukherjee",
            "Murali Poduval",
            "Aniruddha Sinha",
            "A. Ghose"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Hip Arthroplasty is a surgical procedure in which the hip joint is replaced by a prosthetic implant. These implants fail over time and need revision surgery. The burden of revision of a failed arthroplasty is gradually increasing as the number of primary arthroplasty have undergone an exponential increase. It is very important to identify the type of manufacturer and fixation from x-ray images to plan the surgeries better as well in getting the right set of instruments for the surgery. There are 3 types of fixation in hip arthroplasty namely cemented, cementless, and hybrid. In spite of its significance, to the best of our knowledge, we could not find any automated work on fixation type detection. In this work, we have annotated the open source dataset [2] with fixation type and proposed a two-stage low-shot learning approach for detecting the different types of fixation. The key challenge was inter-class confusion and class imbalance. Our proposed work provides 93% classification accuracy and 86% accuracy when blind-tested from another source of clinical data.",
          "pdf_url": "",
          "doi": "10.1109/EMBC53108.2024.10782068",
          "url": "https://www.semanticscholar.org/paper/fd97c3ae534394227a7bfc348e33c80ef0c1c9f5",
          "source": "Semantic Scholar"
        },
        {
          "title": "Enabling Personalized Medicine in Orthopaedic Surgery Through Artificial Intelligence: A Critical Analysis Review.",
          "authors": [
            "Nickelas Huffman",
            "Ignacio Pasqualini",
            "Shujaa T. Khan",
            "A. Klika",
            "Matthew E. Deren",
            "Yuxuan Jin",
            "K. Kunze",
            "N. Piuzzi"
          ],
          "year": 2024,
          "citations": 10,
          "abstract": "» The application of artificial intelligence (AI) in the field of orthopaedic surgery holds potential for revolutionizing health care delivery across 3 crucial domains: (I) personalized prediction of clinical outcomes and adverse events, which may optimize patient selection, surgical planning, and enhance patient safety and outcomes; (II) diagnostic automated and semiautomated imaging analyses, which may reduce time burden and facilitate precise and timely diagnoses; and (III) forecasting of resource utilization, which may reduce health care costs and increase value for patients and institutions.» Computer vision is one of the most highly studied areas of AI within orthopaedics, with applications pertaining to fracture classification, identification of the manufacturer and model of prosthetic implants, and surveillance of prosthesis loosening and failure.» Prognostic applications of AI within orthopaedics include identifying patients who will likely benefit from a specified treatment, predicting prosthetic implant size, postoperative length of stay, discharge disposition, and surgical complications. Not only may these applications be beneficial to patients but also to institutions and payors because they may inform potential cost expenditure, improve overall hospital efficiency, and help anticipate resource utilization.» AI infrastructure development requires institutional financial commitment and a team of clinicians and data scientists with expertise in AI that can complement skill sets and knowledge. Once a team is established and a goal is determined, teams (1) obtain, curate, and label data; (2) establish a reference standard; (3) develop an AI model; (4) evaluate the performance of the AI model; (5) externally validate the model, and (6) reinforce, improve, and evaluate the model's performance until clinical implementation is possible.» Understanding the implications of AI in orthopaedics may eventually lead to wide-ranging improvements in patient care. However, AI, while holding tremendous promise, is not without methodological and ethical limitations that are essential to address. First, it is important to ensure external validity of programs before their use in a clinical setting. Investigators should maintain high quality data records and registry surveillance, exercise caution when evaluating others' reported AI applications, and increase transparency of the methodological conduct of current models to improve external validity and avoid propagating bias. By addressing these challenges and responsibly embracing the potential of AI, the medical field may eventually be able to harness its power to improve patient care and outcomes.",
          "pdf_url": "",
          "doi": "10.2106/jbjs.rvw.23.00232",
          "url": "https://www.semanticscholar.org/paper/54f7b602d476d7abb508d44763cc72ab53f34ed4",
          "source": "Semantic Scholar"
        },
        {
          "title": "Automated Shoulder Implant Manufacturer Detection using Encoder Decoder based Classifier from X-ray Images",
          "authors": [
            "Aparna Kanakatte",
            "Divya Bhatia",
            "Avik Ghose"
          ],
          "year": 2023,
          "citations": 2,
          "abstract": "Total shoulder arthroplasty is the process of replacing the damaged ball and socket joint in the shoulder with a prosthesis made with polyethylene and metal components. The prosthesis helps to restore the normal range of motion and reduce pain, enabling the patient to return to their daily activities. These implants may need to be replaced over the years due to damage or wear and tear. It is a tedious and time-consuming process to identify the type of implant if medical records are not properly maintained. Artificial intelligence systems can speed up the treatment process by classifying the manufacturer and model of the prosthesis. We have proposed an encoder-decoder based classifier along with the supervised contrastive loss function that can identify the implant manufacturer effectively with increased accuracy of 92% from X-ray images overcoming the class imbalance problem.",
          "pdf_url": "",
          "doi": "10.1109/EMBC40787.2023.10340429",
          "url": "https://www.semanticscholar.org/paper/39dae7d467adc924ae52c27cc52f9d1a1b2fa003",
          "source": "Semantic Scholar"
        },
        {
          "title": "A Deep Learning Framework for Detection and Classification of Implant Manufacturer using X-Ray Radiographs",
          "authors": [
            "Attar Mahay Sheetal",
            "K. Sreekumar"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "—Now-a-days, artificial prosthesis is widely used to mitigate pain in damaged shoulders and restore their movement ability. The process involves a complex surgery that attempts to fix an artificial prosthesis into a dead shoulder as a replacement for the ball and socket joints of the shoulder. Long after the surgical process, the need for revision or reoperation may arise due to some problems with the prosthesis. Identification of prosthesis manufacturer is a paramount step in the reoperation exercise. Traditional approach compares the prosthesis under consideration with prosthesis from a vast number of manufacturers. This approach is cost-efficient and requires no extra training for the physician to identify the prosthesis manufacturer. However, the method is time inefficient and is prone to mistakes. Systems based on machine learning have the potential to reduce human errors and expedite the revision process. This paper proposes a shallow 2D convolution neural network (CNN) for the classification of shoulder prosthesis To speed-up the learning process and improve the performance of the deep learning model for implant classification, this paper employed three different techniques. Firstly, a generative adversarial network (GAN) is applied to the dataset to augment the classes with fewer samples to ensure the data imbalance problem is eliminated. Secondly, the highly discriminating features are extracted using principal component analysis (PCA) and used to train the proposed model. Lastly, the model hyper-parameters are optimised to ensure optimal model performance. The model trained with extracted features with a variance of 0.99 achieved the best accuracy of 99.8%.",
          "pdf_url": "http://thesai.org/Downloads/Volume15No3/Paper_77-A_Deep_Learning_Framework_for_Detection_and_Classification.pdf",
          "doi": "10.14569/ijacsa.2024.0150377",
          "url": "https://www.semanticscholar.org/paper/a450e2b58918e08d7831510381709c66ec815b45",
          "source": "Semantic Scholar"
        },
        {
          "title": "Machine Learning Model for Classification of Shoulder Implant Manufacturer Using X-Ray Images",
          "authors": [
            "Idris Djibo",
            "I. Z. Yakubu",
            "Muhammad Raiyan",
            "G. Manikandan",
            "Fatima Shittu",
            "Z. Musa"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Recently, synthetic prosthesis built from metals and plastic components are often used to mitigate pain and restore functions of injured human shoulders. The procedure involves the replacement of the affected shoulder ball and socket joint with the synthetically generated prosthesis. Long after the replacement of the affected part, the synthetic prosthesis maybe damaged or worn out, thus the reoperation process maybe repeated or revised. To guarantee a robust and seamless reoperation, detail of the model and manufacturer of the synthetic prosthesis is a paramount. There are circumstances where information regarding the prosthesis is not available. To determine the model and manufacturer, a thorough inspection and physical analogy of various manufacturers’ prosthesis are therefore required. The manual method consumes longer time and is liable to errors. With the evolution and continuous growth in the field of Machine Learning, prediction models can be used to learn the prosthesis and predict the model and manufacturer, thus reducing the time and error in the manual approach. In this paper, a model based on ensemble learning for identification of the manufacturer of synthetic prosthesis is proposed. Deep Convolution Neural Network (DCNN), High Resolution Network (HRNet), and Support Vector Machine (SVM) were combined to form the ensemble model. To ensure accurate prediction of the manufacturer, the components of the ensemble model are separately trained and then integrated using a novel weighted average ensemble technique. This strategy aids in identifying the prosthesis' manufacturer by assigning greater weight to the model that performs the best. The proposed model performance is compared with the performance of its individual components, where it outperforms the individual models in terms of accuracy, recall, precision and f measures.",
          "pdf_url": "",
          "doi": "10.1109/ICAST61769.2024.10856466",
          "url": "https://www.semanticscholar.org/paper/0548261ee0921a5cb04e43cc17ccc45ce765326e",
          "source": "Semantic Scholar"
        },
        {
          "title": "Classification of Shoulder Implant Manufacturer Using Pre‐Trained DenseNet201 Combined With Capsule Network",
          "authors": [
            "Xianzhong Jian",
            "Zhenling Zhou",
            "Wuwen Zhang"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "This study aims to accelerate revision surgery and treatment using X‐ray imaging and deep learning to identify shoulder implant manufacturers in advance.",
          "pdf_url": "",
          "doi": "10.1002/rcs.2672",
          "url": "https://www.semanticscholar.org/paper/c83781aeda2d1312a758004897de894e9ece54e0",
          "source": "Semantic Scholar"
        },
        {
          "title": "Automated deep learning for classification of dental implant radiographs using a large multi-center dataset",
          "authors": [
            "Won-Se Park",
            "Jong-Ki Huh",
            "Jae-Hong Lee"
          ],
          "year": 2023,
          "citations": 40,
          "abstract": "This study aimed to evaluate the accuracy of automated deep learning (DL) algorithm for identifying and classifying various types of dental implant systems (DIS) using a large-scale multicenter dataset. Dental implant radiographs of pos-implant surgery were collected from five college dental hospitals and 10 private dental clinics, and validated by the National Information Society Agency and the Korean Academy of Oral and Maxillofacial Implantology. The dataset contained a total of 156,965 panoramic and periapical radiographic images and comprised 10 manufacturers and 27 different types of DIS. The accuracy, precision, recall, F1 score, and confusion matrix were calculated to evaluate the classification performance of the automated DL algorithm. The performance metrics of the automated DL based on accuracy, precision, recall, and F1 score for 116,756 panoramic and 40,209 periapical radiographic images were 88.53%, 85.70%, 82.30%, and 84.00%, respectively. Using only panoramic images, the DL algorithm achieved 87.89% accuracy, 85.20% precision, 81.10% recall, and 83.10% F1 score, whereas the corresponding values using only periapical images achieved 86.87% accuracy, 84.40% precision, 81.70% recall, and 83.00% F1 score, respectively. Within the study limitations, automated DL shows a reliable classification accuracy based on large-scale and comprehensive datasets. Moreover, we observed no statistically significant difference in accuracy performance between the panoramic and periapical images. The clinical feasibility of the automated DL algorithm requires further confirmation using additional clinical datasets.",
          "pdf_url": "https://www.nature.com/articles/s41598-023-32118-1.pdf",
          "doi": "10.1038/s41598-023-32118-1",
          "url": "https://www.semanticscholar.org/paper/f2cebd991135947725c5b17f2bb280ea111236ab",
          "source": "Semantic Scholar"
        },
        {
          "title": "Shoulder Implant X-Ray Manufacturer Classification: Exploring with Vision Transformer",
          "authors": [
            "Meng Zhou",
            "Shanglin Mo"
          ],
          "year": 2021,
          "citations": 13,
          "abstract": "Shoulder replacement surgery, also called total shoulder replacement, is a common and complex surgery in Orthopedics discipline. It involves replacing a dead shoulder joint with an artificial implant. In the market, there are many artificial implant manufacturers and each of them may produce different implants with different structures compares to other providers. The problem arises in the following situation: a patient has some problems with the shoulder implant accessory and the manufacturer of that implant maybe unknown to either the patient or the doctor, therefore, correctly identification of the manufacturer is the key prior to the treatment. In this paper, we will demonstrate different methods for classifying the manufacturer of a shoulder implant.",
          "pdf_url": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bbb15fed76a76680d8d34f67995c9e01f5ce4231",
          "source": "Semantic Scholar"
        },
        {
          "title": "Author Correction: Automated deep learning for classification of dental implant radiographs using a large multi-center dataset",
          "authors": [
            "W. Park",
            "Jong-Ki Huh",
            "Jae-Hong Lee"
          ],
          "year": 2023,
          "citations": 3,
          "abstract": null,
          "pdf_url": "https://www.nature.com/articles/s41598-023-33768-x.pdf",
          "doi": "10.1038/s41598-023-33768-x",
          "url": "https://www.semanticscholar.org/paper/e9fec22eb0ecc4bc1400b34bbb435d4481954496",
          "source": "Semantic Scholar"
        },
        {
          "title": "A Performance Comparison between Automated Deep Learning and Dental Professionals in Classification of Dental Implant Systems from Dental Imaging: A Multi-Center Study",
          "authors": [
            "Jae-Hong Lee",
            "Young-Taek Kim",
            "Jong-Bin Lee",
            "S. Jeong"
          ],
          "year": 2020,
          "citations": 79,
          "abstract": "In this study, the efficacy of the automated deep convolutional neural network (DCNN) was evaluated for the classification of dental implant systems (DISs) and the accuracy of the performance was compared against that of dental professionals using dental radiographic images collected from three dental hospitals. A total of 11,980 panoramic and periapical radiographic images with six different types of DISs were divided into training (n = 9584) and testing (n = 2396) datasets. To compare the accuracy of the trained automated DCNN with dental professionals (including six board-certified periodontists, eight periodontology residents, and 11 residents not specialized in periodontology), 180 images were randomly selected from the test dataset. The accuracy of the automated DCNN based on the AUC, Youden index, sensitivity, and specificity, were 0.954, 0.808, 0.955, and 0.853, respectively. The automated DCNN outperformed most of the participating dental professionals, including board-certified periodontists, periodontal residents, and residents not specialized in periodontology. The automated DCNN was highly effective in classifying similar shapes of different types of DISs based on dental radiographic images. Further studies are necessary to determine the efficacy and feasibility of applying an automated DCNN in clinical practice.",
          "pdf_url": "https://www.mdpi.com/2075-4418/10/11/910/pdf?version=1604735703",
          "doi": "10.3390/diagnostics10110910",
          "url": "https://www.semanticscholar.org/paper/b7d17bb4a8a64fca1982b022877e89ed85139b5e",
          "source": "Semantic Scholar"
        },
        {
          "title": "Shoulder Implant X-Ray Manufacturer Classification: Exploring with\n  Vision Transformer",
          "authors": [
            "Meng Zhou",
            "Shanglin Mo"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Shoulder replacement surgery, also called total shoulder replacement, is a\ncommon and complex surgery in Orthopedics discipline. It involves replacing a\ndead shoulder joint with an artificial implant. In the market, there are many\nartificial implant manufacturers and each of them may produce different\nimplants with different structures compares to other providers. The problem\narises in the following situation: a patient has some problems with the\nshoulder implant accessory and the manufacturer of that implant maybe unknown\nto either the patient or the doctor, therefore, correctly identification of the\nmanufacturer is the key prior to the treatment. In this paper, we will\ndemonstrate different methods for classifying the manufacturer of a shoulder\nimplant. We will use Vision Transformer approach to this task for the first\ntime ever",
          "pdf_url": "http://arxiv.org/pdf/2104.07667v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2104.07667v2",
          "source": "arXiv"
        },
        {
          "title": "Fully automated workflow for designing patient-specific orthopaedic\n  implants: application to total knee arthroplasty",
          "authors": [
            "Aziliz Guezou-Philippe",
            "Arnaud Clavé",
            "Ehouarn Maguet",
            "Ludivine Maintier",
            "Charles Garraud",
            "Jean-Rassaire Fouefack",
            "Valérie Burdin",
            "Eric Stindel",
            "Guillaume Dardenne"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Background. Osteoarthritis affects about 528 million people worldwide,\ncausing pain and stiffness in the joints. Arthroplasty is commonly performed to\ntreat joint osteoarthritis, reducing pain and improving mobility. Nevertheless,\na significant share of patients remain unsatisfied with their surgery.\nPersonalised arthroplasty was introduced to improve surgical outcomes however\ncurrent solutions require delays, making it difficult to integrate in clinical\nroutine. We propose a fully automated workflow to design patient-specific\nimplants for total knee arthroplasty.\n  Methods. The proposed pipeline first uses artificial neural networks to\nsegment the femur and tibia proximal and distal extremities. Then the full\nbones are reconstructed using augmented statistical shape models, combining\nshape and landmarks information. Finally, 77 morphological parameters are\ncomputed to design patient-specific implants. The developed workflow has been\ntrained on 91 CT scans and evaluated on 41 CT scans, in terms of accuracy and\nexecution time.\n  Results. The workflow accuracy was $0.4\\pm0.2mm$ for segmentation,\n$1.0\\pm0.3mm$ for full bone reconstruction, and $2.2\\pm1.5mm$ for anatomical\nlandmarks determination. The custom implants fitted the patients' anatomy with\n$0.9\\pm0.5mm$ accuracy. The whole process from segmentation to implants' design\nlasted about 15 minutes.\n  Conclusion. The proposed workflow performs a fast and reliable\npersonalisation of knee implants, directly from a CT image without requiring\nany manual intervention. It allows the establishment of a patient-specific\npre-operative planning in a very short time, making it easily available for all\npatients. Combined with efficient implant manufacturing techniques, this\nsolution could help answer the growing number of arthroplasties while reducing\ncomplications and improving patients' satisfaction.",
          "pdf_url": "http://arxiv.org/pdf/2403.15353v3.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2403.15353v3",
          "source": "arXiv"
        },
        {
          "title": "In Vivo Study of Bone Growth Around Additively Manufactured Implants\n  with Ti-6Al-4V and Bioactive Glass Powder Composites",
          "authors": [
            "Chih-Yu Lee",
            "Pei-Ching Kung",
            "Chih-Chieh Huang",
            "Shao-Ju Shih",
            "E-Wen Huang",
            "San-Yuan Chen",
            "Meng-Huang Wu",
            "Nien-Ti Tsou"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Osseointegration is crucial to the success of biomedical implants. Additive\nmanufacturing of implants offers a high degree of design freedom, enabling\nprecise control over implant geometry and material composition. Bioactive glass\n(BG) can substantially enhance bone binding and bioactivity; however, limited\nresearch has been conducted on its incorporation into additively manufactured\nimplants. The performance of BG varies depending on the incorporation method,\nand the spatial and temporal evolution of its integration remains unclear. In\nthis study, we synthesized Ti-6Al-4V/58S BG composites by using the selective\nlaser melting method and systematically compared the effects of BG coating and\ndoping in additively manufactured implants. In vivo histological results from\nanimal tests were statistically analyzed and discussed in terms of\nosseointegration over 4- and 12-week periods. Bone-to-implant contact (BIC) and\nbone density (BD) were used as quantitative metrics to evaluate interactions\nbetween the implants and surrounding bone. Our findings indicate that both\nBG-doped and BG-coated implants accelerated bone ingrowth during the early\nstages of healing. BG-coated implants demonstrated a greater improvement than\ndid pure 3D-printed Ti-6Al-4V implants. However, the effects of BG became\nnonsignificant during the later healing stage (12 weeks). This study provides a\nfoundation for systematically investigating BG incorporation methods in\n3D-printed biomedical implants and their effect on osseointegration.",
          "pdf_url": "http://arxiv.org/pdf/2501.11098v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2501.11098v1",
          "source": "arXiv"
        },
        {
          "title": "A Novel Visual Fault Detection and Classification System for\n  Semiconductor Manufacturing Using Stacked Hybrid Convolutional Neural\n  Networks",
          "authors": [
            "Tobias Schlosser",
            "Frederik Beuth",
            "Michael Friedrich",
            "Danny Kowerko"
          ],
          "year": 2019,
          "citations": 0,
          "abstract": "Automated visual inspection in the semiconductor industry aims to detect and\nclassify manufacturing defects utilizing modern image processing techniques.\nWhile an earliest possible detection of defect patterns allows quality control\nand automation of manufacturing chains, manufacturers benefit from an increased\nyield and reduced manufacturing costs. Since classical image processing systems\nare limited in their ability to detect novel defect patterns, and machine\nlearning approaches often involve a tremendous amount of computational effort,\nthis contribution introduces a novel deep neural network based hybrid approach.\nUnlike classical deep neural networks, a multi-stage system allows the\ndetection and classification of the finest structures in pixel size within\nhigh-resolution imagery. Consisting of stacked hybrid convolutional neural\nnetworks (SH-CNN) and inspired by current approaches of visual attention, the\nrealized system draws the focus over the level of detail from its structures to\nmore task-relevant areas of interest. The results of our test environment show\nthat the SH-CNN outperforms current approaches of learning-based automated\nvisual inspection, whereas a distinction depending on the level of detail\nenables the elimination of defect patterns in earlier stages of the\nmanufacturing process.",
          "pdf_url": "http://arxiv.org/pdf/1911.11250v6.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1911.11250v6",
          "source": "arXiv"
        },
        {
          "title": "An Online Platform for Automatic Skull Defect Restoration and Cranial\n  Implant Design",
          "authors": [
            "Jianning Li",
            "Antonio Pepe",
            "Christina Gsaxner",
            "Jan Egger"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "We introduce a fully automatic system for cranial implant design, a common\ntask in cranioplasty operations. The system is currently integrated in\nStudierfenster (http://studierfenster.tugraz.at/), an online, cloud-based\nmedical image processing platform for medical imaging applications. Enhanced by\ndeep learning algorithms, the system automatically restores the missing part of\na skull (i.e., skull shape completion) and generates the desired implant by\nsubtracting the defective skull from the completed skull. The generated implant\ncan be downloaded in the STereoLithography (.stl) format directly via the\nbrowser interface of the system. The implant model can then be sent to a 3D\nprinter for in loco implant manufacturing. Furthermore, thanks to the standard\nformat, the user can thereafter load the model into another application for\npost-processing whenever necessary. Such an automatic cranial implant design\nsystem can be integrated into the clinical practice to improve the current\nroutine for surgeries related to skull defect repair (e.g., cranioplasty). Our\nsystem, although currently intended for educational and research use only, can\nbe seen as an application of additive manufacturing for fast, patient-specific\nimplant design.",
          "pdf_url": "http://arxiv.org/pdf/2006.00980v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2006.00980v1",
          "source": "arXiv"
        },
        {
          "title": "Improving Deep Learning-based Automatic Cranial Defect Reconstruction by\n  Heavy Data Augmentation: From Image Registration to Latent Diffusion Models",
          "authors": [
            "Marek Wodzinski",
            "Kamil Kwarciak",
            "Mateusz Daniol",
            "Daria Hemmerling"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Modeling and manufacturing of personalized cranial implants are important\nresearch areas that may decrease the waiting time for patients suffering from\ncranial damage. The modeling of personalized implants may be partially\nautomated by the use of deep learning-based methods. However, this task suffers\nfrom difficulties with generalizability into data from previously unseen\ndistributions that make it difficult to use the research outcomes in real\nclinical settings. Due to difficulties with acquiring ground-truth annotations,\ndifferent techniques to improve the heterogeneity of datasets used for training\nthe deep networks have to be considered and introduced. In this work, we\npresent a large-scale study of several augmentation techniques, varying from\nclassical geometric transformations, image registration, variational\nautoencoders, and generative adversarial networks, to the most recent advances\nin latent diffusion models. We show that the use of heavy data augmentation\nsignificantly increases both the quantitative and qualitative outcomes,\nresulting in an average Dice Score above 0.94 for the SkullBreak and above 0.96\nfor the SkullFix datasets. Moreover, we show that the synthetically augmented\nnetwork successfully reconstructs real clinical defects. The work is a\nconsiderable contribution to the field of artificial intelligence in the\nautomatic modeling of personalized cranial implants.",
          "pdf_url": "http://arxiv.org/pdf/2406.06372v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2406.06372v1",
          "source": "arXiv"
        },
        {
          "title": "An Ontology for Defect Detection in Metal Additive Manufacturing",
          "authors": [
            "Massimo Carraturo",
            "Andrea Mazzullo"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "A key challenge for Industry 4.0 applications is to develop control systems\nfor automated manufacturing services that are capable of addressing both data\nintegration and semantic interoperability issues, as well as monitoring and\ndecision making tasks. To address such an issue in advanced manufacturing\nsystems, principled knowledge representation approaches based on formal\nontologies have been proposed as a foundation to information management and\nmaintenance in presence of heterogeneous data sources. In addition, ontologies\nprovide reasoning and querying capabilities to aid domain experts and end users\nin the context of constraint validation and decision making. Finally,\nontology-based approaches to advanced manufacturing services can support the\nexplainability and interpretability of the behaviour of monitoring, control,\nand simulation systems that are based on black-box machine learning algorithms.\nIn this work, we provide a novel ontology for the classification of\nprocess-induced defects known from the metal additive manufacturing literature.\nTogether with a formal representation of the characterising features and\nsources of defects, we integrate our knowledge base with state-of-the-art\nontologies in the field. Our knowledge base aims at enhancing the modelling\ncapabilities of additive manufacturing ontologies by adding further defect\nanalysis terminology and diagnostic inference features.",
          "pdf_url": "http://arxiv.org/pdf/2210.04772v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2210.04772v1",
          "source": "arXiv"
        },
        {
          "title": "A materials perspective on the design of damage-resilient artificial\n  bones and bone implants through additive/advanced manufacturing",
          "authors": [
            "Hortense Le Ferrand",
            "Christos E Athanasiou"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "After more than five decades of research, the failure of bone implants is\nstill an issue that becomes increasingly urgent to solve in our ageing\npopulation. Among the reasons for failure, catastrophic brittle fracture is one\nevent that is directly related to the implant s material and fabrication and\nthat deserves more attention. Indeed, clinically available implants pale at\nreproducing the hierarchical and heterogeneous microstructural organization of\nour natural bones, ultimately failing at reproducing their mechanical strength\nand toughness. Nevertheless, the recent advances in additive and advanced\nmanufacturing open new horizons for the fabrication of biomimetic bone\nimplants, challenging at the same time their characterization, testing and\nmodelling. This critical review covers selected recent achievements in bone\nimplant research from a materials standpoint and aims at deciphering some of\nthe most urgent issues in this multidisciplinary field.",
          "pdf_url": "http://arxiv.org/pdf/2103.01448v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2103.01448v1",
          "source": "arXiv"
        },
        {
          "title": "Point Cloud Diffusion Models for Automatic Implant Generation",
          "authors": [
            "Paul Friedrich",
            "Julia Wolleb",
            "Florentin Bieder",
            "Florian M. Thieringer",
            "Philippe C. Cattin"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Advances in 3D printing of biocompatible materials make patient-specific\nimplants increasingly popular. The design of these implants is, however, still\na tedious and largely manual process. Existing approaches to automate implant\ngeneration are mainly based on 3D U-Net architectures on downsampled or\npatch-wise data, which can result in a loss of detail or contextual\ninformation. Following the recent success of Diffusion Probabilistic Models, we\npropose a novel approach for implant generation based on a combination of 3D\npoint cloud diffusion models and voxelization networks. Due to the stochastic\nsampling process in our diffusion model, we can propose an ensemble of\ndifferent implants per defect, from which the physicians can choose the most\nsuitable one. We evaluate our method on the SkullBreak and SkullFix datasets,\ngenerating high-quality implants and achieving competitive evaluation scores.",
          "pdf_url": "http://arxiv.org/pdf/2303.08061v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2303.08061v2",
          "source": "arXiv"
        },
        {
          "title": "Enhanced Knee Kinematics: Leveraging Deep Learning and Morphing\n  Algorithms for 3D Implant Modeling",
          "authors": [
            "Viet-Dung Nguyen",
            "Michael T. LaCour",
            "Richard D. Komistek"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Accurate reconstruction of implanted knee models is crucial in orthopedic\nsurgery and biomedical engineering, enhancing preoperative planning, optimizing\nimplant design, and improving surgical outcomes. Traditional methods rely on\nlabor-intensive and error-prone manual segmentation. This study proposes a\nnovel approach using machine learning (ML) algorithms and morphing techniques\nfor precise 3D reconstruction of implanted knee models.\n  The methodology begins with acquiring preoperative imaging data, such as\nfluoroscopy or X-ray images of the patient's knee joint. A convolutional neural\nnetwork (CNN) is then trained to automatically segment the femur contour of the\nimplanted components, significantly reducing manual effort and ensuring high\naccuracy.\n  Following segmentation, a morphing algorithm generates a personalized 3D\nmodel of the implanted knee joint, using the segmented data and biomechanical\nprinciples. This algorithm considers implant position, size, and orientation to\nsimulate the knee joint's shape. By integrating morphological data with\nimplant-specific parameters, the reconstructed models accurately reflect the\npatient's implant anatomy and configuration.\n  The approach's effectiveness is demonstrated through quantitative\nevaluations, including comparisons with ground truth data and existing\ntechniques. In 19 test cases involving various implant types, the ML-based\nsegmentation method showed superior accuracy and consistency compared to manual\nsegmentation, with an average RMS error of 0.58 +/- 0.14 mm.\n  This research advances orthopedic surgery by providing a robust framework for\nthe automated reconstruction of implanted knee models. Leveraging ML and\nmorphing algorithms, clinicians and researchers gain valuable insights into\npatient-specific knee anatomy, implant biomechanics, and surgical planning,\nleading to improved patient outcomes and enhanced quality of care.",
          "pdf_url": "http://arxiv.org/pdf/2408.01557v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2408.01557v1",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Shoulder arthroplasty revision surgery",
      "final_query": "Shoulder implant classification deep learning",
      "attempts": 2,
      "paper_count": 20,
      "open_access_count": 13,
      "papers": [
        {
          "title": "Harnessing the Potential of Deep Learning for Total Shoulder Implant Classification: A Comparative Study",
          "authors": [
            "Aakriti Mishra",
            "A. Ramanathan",
            "V. Batta",
            "C. Malathy",
            "Soumya Snigdha Kundu",
            "M. Gayathri",
            "D. Vathana",
            "S. Kamineni"
          ],
          "year": 2023,
          "citations": 1,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1007/978-3-031-48593-0_9",
          "url": "https://www.semanticscholar.org/paper/b39eeb1ef6c0b61923596e93d94196c17a04d83f",
          "source": "Semantic Scholar"
        },
        {
          "title": "An Ensemble Voting Approach for Shoulder Implant Classification from X-Ray Images",
          "authors": [
            "Elif Baykal Kablan"
          ],
          "year": 2024,
          "citations": 1,
          "abstract": "Total Shoulder Arthroplasty (TSA) is an effective method that involves replacing damaged joint surfaces with appropriate prosthetic components to manage pain and improve joint mobility. Over time, the prosthetic replacement or improvement process requires both the surgeon and the patient to know the prosthesis manufacturer for successful outcomes. However, inadequate medical records often necessitate additional X-rays for the patient. In this paper, a deep learning-based method is proposed to analyze prosthesis details from X-ray images automatically. The method combines predictions made by pre-trained ViT, DeiT, and Swin model versions on a dataset consisting of 597 shoulder implant X-ray images to achieve more accurate classification. The performances of hard and soft voting ensembles were analyzed respectively. During the experiments, the soft voting approach yielded the highest performance, achieving 96.15 % accuracy, 92.12 % precision, and 88.56 % recall, marking the highest classification performance observed thus far. The results demonstrate that our voting ensemble method can be utilized as a reliable and effective tool for automatically analyzing prosthesis details.",
          "pdf_url": "",
          "doi": "10.1109/TSP63128.2024.10605940",
          "url": "https://www.semanticscholar.org/paper/d6a1b7151fd7ad070a4c6396b97fffb2d60f04bd",
          "source": "Semantic Scholar"
        },
        {
          "title": "A Deep Learning Framework for Detection and Classification of Implant Manufacturer using X-Ray Radiographs",
          "authors": [
            "Attar Mahay Sheetal",
            "K. Sreekumar"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "—Now-a-days, artificial prosthesis is widely used to mitigate pain in damaged shoulders and restore their movement ability. The process involves a complex surgery that attempts to fix an artificial prosthesis into a dead shoulder as a replacement for the ball and socket joints of the shoulder. Long after the surgical process, the need for revision or reoperation may arise due to some problems with the prosthesis. Identification of prosthesis manufacturer is a paramount step in the reoperation exercise. Traditional approach compares the prosthesis under consideration with prosthesis from a vast number of manufacturers. This approach is cost-efficient and requires no extra training for the physician to identify the prosthesis manufacturer. However, the method is time inefficient and is prone to mistakes. Systems based on machine learning have the potential to reduce human errors and expedite the revision process. This paper proposes a shallow 2D convolution neural network (CNN) for the classification of shoulder prosthesis To speed-up the learning process and improve the performance of the deep learning model for implant classification, this paper employed three different techniques. Firstly, a generative adversarial network (GAN) is applied to the dataset to augment the classes with fewer samples to ensure the data imbalance problem is eliminated. Secondly, the highly discriminating features are extracted using principal component analysis (PCA) and used to train the proposed model. Lastly, the model hyper-parameters are optimised to ensure optimal model performance. The model trained with extracted features with a variance of 0.99 achieved the best accuracy of 99.8%.",
          "pdf_url": "http://thesai.org/Downloads/Volume15No3/Paper_77-A_Deep_Learning_Framework_for_Detection_and_Classification.pdf",
          "doi": "10.14569/ijacsa.2024.0150377",
          "url": "https://www.semanticscholar.org/paper/a450e2b58918e08d7831510381709c66ec815b45",
          "source": "Semantic Scholar"
        },
        {
          "title": "Classification of Shoulder Implant Manufacturer Using Pre‐Trained DenseNet201 Combined With Capsule Network",
          "authors": [
            "Xianzhong Jian",
            "Zhenling Zhou",
            "Wuwen Zhang"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "This study aims to accelerate revision surgery and treatment using X‐ray imaging and deep learning to identify shoulder implant manufacturers in advance.",
          "pdf_url": "",
          "doi": "10.1002/rcs.2672",
          "url": "https://www.semanticscholar.org/paper/c83781aeda2d1312a758004897de894e9ece54e0",
          "source": "Semantic Scholar"
        },
        {
          "title": "A Combined Deep Learning Model with Attention Mechanism for Detection of Implant Manufacturer Using X-Ray Images",
          "authors": [
            "Attar Mahay Sheetal",
            "K. Sreekumar"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": ": Shoulder replacement surgery is one of the invasive techniques in orthopedic disciplines that replaces dead shoulder joints with prostheses made from polyethylene and metal components. To perform the surgery, there is a need to know the implant accessories and manufacturer of the implant. Some problems arise in a situation where the patient experiences pain and shoulder malfunctions that need replacement, and the accessories and implant manufacturer are mysterious to the doctor or the patient. In such a case, the solution to the problem depends on the accuracy of the identification of the manufacturer of the prosthesis. This research study proposes a novel detection and classification approach that integrates models based on deep learning with an attention mechanism to identify the implant manufacturers prior to surgery. The ensemble deep learning model utilizes the more sophisticated Long Short Term Memory architecture (LSTM) and the traditional multi layer Convolution Neural Networks for extraction of features and predicting the implant manufacturer. The model employs an attention mechanism to focus on the critical part of the prosthesis that is crucial in the detection of the prosthesis manufacturer. The features map from the attention layer is finally fed into the LSTM for prediction by the implant manufacturer. Collection implant images of 597 from different implant manufacturers, which include 294 images generated by the Depuy manufacturer, 83 images generated by the Cofield manufacturers, 149 generated by the Zimmer manufacturer, and 71 generated by the Tornier manufacturer, are utilized as a dataset not only for training but also for testing the model. The results show that the combined Deep Learning (DL) model with attention mechanism performs better than the Convolution Neural Network model, Convolution Neural Network + Attention, and Convolution Neural Network + LSTM models. Depending on the accomplishment of the model, it is concluded that this model could become an important tool for planning the preoperative procedure and this can be implemented for identifying and classifying the implants from different manufacturers.",
          "pdf_url": "",
          "doi": "10.3844/jcssp.2025.1322.1331",
          "url": "https://www.semanticscholar.org/paper/5c1b050c1ac50a2c21e30c9bde6f5a41b41fefea",
          "source": "Semantic Scholar"
        },
        {
          "title": "SSP: self-supervised pertaining technique for classification of shoulder implants in x-ray medical images: a broad experimental study",
          "authors": [
            "Laith Alzubaidi",
            "M. Fadhel",
            "Freek Hollman",
            "Asma Salhi",
            "José I. Santamaría",
            "Ye Duan",
            "Ashish Gupta",
            "K. Cutbush",
            "Amin Abbosh",
            "Yuantong Gu"
          ],
          "year": 2024,
          "citations": 12,
          "abstract": "Multiple pathologic conditions can lead to a diseased and symptomatic glenohumeral joint for which total shoulder arthroplasty (TSA) replacement may be indicated. The long-term survival of implants is limited. With the increasing incidence of joint replacement surgery, it can be anticipated that joint replacement revision surgery will become more common. It can be challenging at times to retrieve the manufacturer of the in situ implant. Therefore, certain systems facilitated by AI techniques such as deep learning (DL) can help correctly identify the implanted prosthesis. Correct identification of implants in revision surgery can help reduce perioperative complications and complications. DL was used in this study to categorise different implants based on X-ray images into four classes (as a first case study of the small dataset): Cofield, Depuy, Tornier, and Zimmer. Imbalanced and small public datasets for shoulder implants can lead to poor performance of DL model training. Most of the methods in the literature have adopted the idea of transfer learning (TL) from ImageNet models. This type of TL has been proven ineffective due to some concerns regarding the contrast between features learnt from natural images (ImageNet: colour images) and shoulder implants in X-ray images (greyscale images). To address that, a new TL approach (self-supervised pertaining (SSP)) is proposed to resolve the issue of small datasets. The SSP approach is based on training the DL models (ImageNet models) on a large number of unlabelled greyscale medical images in the domain to update the features. The models are then trained on a small labelled data set of X-ray images of shoulder implants. The SSP shows excellent results in five ImageNet models, including MobilNetV2, DarkNet19, Xception, InceptionResNetV2, and EfficientNet with precision of 96.69%, 95.45%, 98.76%, 98.35%, and 96.6%, respectively. Furthermore, it has been shown that different domains of TL (such as ImageNet) do not significantly affect the performance of shoulder implants in X-ray images. A lightweight model trained from scratch achieves 96.6% accuracy, which is similar to using standard ImageNet models. The features extracted by the DL models are used to train several ML classifiers that show outstanding performance by obtaining an accuracy of 99.20% with Xception+SVM. Finally, extended experimentation has been carried out to elucidate our approach’s real effectiveness in dealing with different medical imaging scenarios. Specifically, five different datasets are trained and tested with and without the proposed SSP, including the shoulder X-ray with an accuracy of 99.47% and CT brain stroke with an accuracy of 98.60%.",
          "pdf_url": "https://doi.org/10.1007/s10462-024-10878-0",
          "doi": "10.1007/s10462-024-10878-0",
          "url": "https://www.semanticscholar.org/paper/8893cd60d17ea999c4e80033f47ba8572fa3da17",
          "source": "Semantic Scholar"
        },
        {
          "title": "A robust framework for shoulder implant X-ray image classification",
          "authors": [
            "M. Vo",
            "Anh H. Vo",
            "Tuong Le"
          ],
          "year": 2021,
          "citations": 14,
          "abstract": "PurposeMedical images are increasingly popular; therefore, the analysis of these images based on deep learning helps diagnose diseases become more and more essential and necessary. Recently, the shoulder implant X-ray image classification (SIXIC) dataset that includes X-ray images of implanted shoulder prostheses produced by four manufacturers was released. The implant's model detection helps to select the correct equipment and procedures in the upcoming surgery.Design/methodology/approachThis study proposes a robust model named X-Net to improve the predictability for shoulder implants X-ray image classification in the SIXIC dataset. The X-Net model utilizes the Squeeze and Excitation (SE) block integrated into Residual Network (ResNet) module. The SE module aims to weigh each feature map extracted from ResNet, which aids in improving the performance. The feature extraction process of X-Net model is performed by both modules: ResNet and SE modules. The final feature is obtained by incorporating the extracted features from the above steps, which brings more important characteristics of X-ray images in the input dataset. Next, X-Net uses this fine-grained feature to classify the input images into four classes (Cofield, Depuy, Zimmer and Tornier) in the SIXIC dataset.FindingsExperiments are conducted to show the proposed approach's effectiveness compared with other state-of-the-art methods for SIXIC. The experimental results indicate that the approach outperforms the various experimental methods in terms of several performance metrics. In addition, the proposed approach provides the new state of the art results in all performance metrics, such as accuracy, precision, recall, F1-score and area under the curve (AUC), for the experimental dataset.Originality/valueThe proposed method with high predictive performance can be used to assist in the treatment of injured shoulder joints.",
          "pdf_url": "",
          "doi": "10.1108/dta-08-2021-0210",
          "url": "https://www.semanticscholar.org/paper/81995bd78a142f92ba0cc06bc9fa6fb721b76e54",
          "source": "Semantic Scholar"
        },
        {
          "title": "Artificial Intelligence-Based Solution in Personalized Computer-Aided Arthroscopy of Shoulder Prostheses",
          "authors": [
            "Haseeb Sultan",
            "Muhammad Owais",
            "Jiho Choi",
            "Tahir Mahmood",
            "Adnan Haider",
            "Nadeem Ullah",
            "K. Park"
          ],
          "year": 2022,
          "citations": 15,
          "abstract": "Background: Early recognition of prostheses before reoperation can reduce perioperative morbidity and mortality. Because of the intricacy of the shoulder biomechanics, accurate classification of implant models before surgery is fundamental for planning the correct medical procedure and setting apparatus for personalized medicine. Expert surgeons usually use X-ray images of prostheses to set the patient-specific apparatus. However, this subjective method is time-consuming and prone to errors. Method: As an alternative, artificial intelligence has played a vital role in orthopedic surgery and clinical decision-making for accurate prosthesis placement. In this study, three different deep learning-based frameworks are proposed to identify different types of shoulder implants in X-ray scans. We mainly propose an efficient ensemble network called the Inception Mobile Fully-Connected Convolutional Network (IMFC-Net), which is comprised of our two designed convolutional neural networks and a classifier. To evaluate the performance of the IMFC-Net and state-of-the-art models, experiments were performed with a public data set of 597 de-identified patients (597 shoulder implants). Moreover, to demonstrate the generalizability of IMFC-Net, experiments were performed with two augmentation techniques and without augmentation, in which our model ranked first, with a considerable difference from the comparison models. A gradient-weighted class activation map technique was also used to find distinct implant characteristics needed for IMFC-Net classification decisions. Results: The results confirmed that the proposed IMFC-Net model yielded an average accuracy of 89.09%, a precision rate of 89.54%, a recall rate of 86.57%, and an F1.score of 87.94%, which were higher than those of the comparison models. Conclusion: The proposed model is efficient and can minimize the revision complexities of implants.",
          "pdf_url": "https://www.mdpi.com/2075-4426/12/1/109/pdf?version=1642168876",
          "doi": "10.3390/jpm12010109",
          "url": "https://www.semanticscholar.org/paper/052b58a150342c6b10d5f8d6e67c9d2afa6150ee",
          "source": "Semantic Scholar"
        },
        {
          "title": "Transfer Learning-Based Class Imbalance-Aware Shoulder Implant Classification from X-Ray Images",
          "authors": [
            "Marut Jindal",
            "Birmohan Singh"
          ],
          "year": 2024,
          "citations": 4,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1007/s42235-023-00477-0",
          "url": "https://www.semanticscholar.org/paper/d7facd4b605b61f6972ab9e4b50664b20167ea06",
          "source": "Semantic Scholar"
        },
        {
          "title": "Machine Learning Model for Classification of Shoulder Implant Manufacturer Using X-Ray Images",
          "authors": [
            "Idris Djibo",
            "I. Z. Yakubu",
            "Muhammad Raiyan",
            "G. Manikandan",
            "Fatima Shittu",
            "Z. Musa"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Recently, synthetic prosthesis built from metals and plastic components are often used to mitigate pain and restore functions of injured human shoulders. The procedure involves the replacement of the affected shoulder ball and socket joint with the synthetically generated prosthesis. Long after the replacement of the affected part, the synthetic prosthesis maybe damaged or worn out, thus the reoperation process maybe repeated or revised. To guarantee a robust and seamless reoperation, detail of the model and manufacturer of the synthetic prosthesis is a paramount. There are circumstances where information regarding the prosthesis is not available. To determine the model and manufacturer, a thorough inspection and physical analogy of various manufacturers’ prosthesis are therefore required. The manual method consumes longer time and is liable to errors. With the evolution and continuous growth in the field of Machine Learning, prediction models can be used to learn the prosthesis and predict the model and manufacturer, thus reducing the time and error in the manual approach. In this paper, a model based on ensemble learning for identification of the manufacturer of synthetic prosthesis is proposed. Deep Convolution Neural Network (DCNN), High Resolution Network (HRNet), and Support Vector Machine (SVM) were combined to form the ensemble model. To ensure accurate prediction of the manufacturer, the components of the ensemble model are separately trained and then integrated using a novel weighted average ensemble technique. This strategy aids in identifying the prosthesis' manufacturer by assigning greater weight to the model that performs the best. The proposed model performance is compared with the performance of its individual components, where it outperforms the individual models in terms of accuracy, recall, precision and f measures.",
          "pdf_url": "",
          "doi": "10.1109/ICAST61769.2024.10856466",
          "url": "https://www.semanticscholar.org/paper/0548261ee0921a5cb04e43cc17ccc45ce765326e",
          "source": "Semantic Scholar"
        },
        {
          "title": "Shoulder Implant X-Ray Manufacturer Classification: Exploring with\n  Vision Transformer",
          "authors": [
            "Meng Zhou",
            "Shanglin Mo"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Shoulder replacement surgery, also called total shoulder replacement, is a\ncommon and complex surgery in Orthopedics discipline. It involves replacing a\ndead shoulder joint with an artificial implant. In the market, there are many\nartificial implant manufacturers and each of them may produce different\nimplants with different structures compares to other providers. The problem\narises in the following situation: a patient has some problems with the\nshoulder implant accessory and the manufacturer of that implant maybe unknown\nto either the patient or the doctor, therefore, correctly identification of the\nmanufacturer is the key prior to the treatment. In this paper, we will\ndemonstrate different methods for classifying the manufacturer of a shoulder\nimplant. We will use Vision Transformer approach to this task for the first\ntime ever",
          "pdf_url": "http://arxiv.org/pdf/2104.07667v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2104.07667v2",
          "source": "arXiv"
        },
        {
          "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture\n  Detection in Clinical Radiographs",
          "authors": [
            "Hemanth Kumar M",
            "Karthika M",
            "Saianiruth M",
            "Vasanthakumar Venugopal",
            "Anandakumar D",
            "Revathi Ezhumalai",
            "Charulatha K",
            "Kishore Kumar J",
            "Dayana G",
            "Kalyan Sivasailam",
            "Bargava Subramanian"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.",
          "pdf_url": "http://arxiv.org/pdf/2507.13408v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2507.13408v1",
          "source": "arXiv"
        },
        {
          "title": "Classification of Shoulder X-Ray Images with Deep Learning Ensemble\n  Models",
          "authors": [
            "Fatih Uysal",
            "Fırat Hardalaç",
            "Ozan Peker",
            "Tolga Tolunay",
            "Nil Tokgöz"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Fractures occur in the shoulder area, which has a wider range of motion than\nother joints in the body, for various reasons. To diagnose these fractures,\ndata gathered from Xradiation (X-ray), magnetic resonance imaging (MRI), or\ncomputed tomography (CT) are used. This study aims to help physicians by\nclassifying shoulder images taken from X-ray devices as fracture / non-fracture\nwith artificial intelligence. For this purpose, the performances of 26 deep\nlearning-based pretrained models in the detection of shoulder fractures were\nevaluated on the musculoskeletal radiographs (MURA) dataset, and two ensemble\nlearning models (EL1 and EL2) were developed. The pretrained models used are\nResNet, ResNeXt, DenseNet, VGG, Inception, MobileNet, and their spinal fully\nconnected (Spinal FC) versions. In the EL1 and EL2 models developed using\npretrained models with the best performance, test accuracy was 0.8455,0.8472,\nCohens kappa was 0.6907, 0.6942 and the area that was related with fracture\nclass under the receiver operating characteristic (ROC) curve (AUC) was\n0.8862,0.8695. As a result of 28 different classifications in total, the\nhighest test accuracy and Cohens kappa values were obtained in the EL2 model,\nand the highest AUC value was obtained in the EL1 model.",
          "pdf_url": "http://arxiv.org/pdf/2102.00515v3.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2102.00515v3",
          "source": "arXiv"
        },
        {
          "title": "Automatic Sleep Stage Classification with Cross-modal Self-supervised\n  Features from Deep Brain Signals",
          "authors": [
            "Chen Gong",
            "Yue Chen",
            "Yanan Sui",
            "Luming Li"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "The detection of human sleep stages is widely used in the diagnosis and\nintervention of neurological and psychiatric diseases. Some patients with deep\nbrain stimulator implanted could have their neural activities recorded from the\ndeep brain. Sleep stage classification based on deep brain recording has great\npotential to provide more precise treatment for patients. The accuracy and\ngeneralizability of existing sleep stage classifiers based on local field\npotentials are still limited. We proposed an applicable cross-modal transfer\nlearning method for sleep stage classification with implanted devices. This\nend-to-end deep learning model contained cross-modal self-supervised feature\nrepresentation, self-attention, and classification framework. We tested the\nmodel with deep brain recording data from 12 patients with Parkinson's disease.\nThe best total accuracy reached 83.2% for sleep stage classification. Results\nshowed speech self-supervised features catch the conversion pattern of sleep\nstages effectively. We provide a new method on transfer learning from acoustic\nsignals to local field potentials. This method supports an effective solution\nfor the insufficient scale of clinical data. This sleep stage classification\nmodel could be adapted to chronic and continuous monitor sleep for Parkinson's\npatients in daily life, and potentially utilized for more precise treatment in\ndeep brain-machine interfaces, such as closed-loop deep brain stimulation.",
          "pdf_url": "http://arxiv.org/pdf/2302.03227v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2302.03227v1",
          "source": "arXiv"
        },
        {
          "title": "Enhanced Knee Kinematics: Leveraging Deep Learning and Morphing\n  Algorithms for 3D Implant Modeling",
          "authors": [
            "Viet-Dung Nguyen",
            "Michael T. LaCour",
            "Richard D. Komistek"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Accurate reconstruction of implanted knee models is crucial in orthopedic\nsurgery and biomedical engineering, enhancing preoperative planning, optimizing\nimplant design, and improving surgical outcomes. Traditional methods rely on\nlabor-intensive and error-prone manual segmentation. This study proposes a\nnovel approach using machine learning (ML) algorithms and morphing techniques\nfor precise 3D reconstruction of implanted knee models.\n  The methodology begins with acquiring preoperative imaging data, such as\nfluoroscopy or X-ray images of the patient's knee joint. A convolutional neural\nnetwork (CNN) is then trained to automatically segment the femur contour of the\nimplanted components, significantly reducing manual effort and ensuring high\naccuracy.\n  Following segmentation, a morphing algorithm generates a personalized 3D\nmodel of the implanted knee joint, using the segmented data and biomechanical\nprinciples. This algorithm considers implant position, size, and orientation to\nsimulate the knee joint's shape. By integrating morphological data with\nimplant-specific parameters, the reconstructed models accurately reflect the\npatient's implant anatomy and configuration.\n  The approach's effectiveness is demonstrated through quantitative\nevaluations, including comparisons with ground truth data and existing\ntechniques. In 19 test cases involving various implant types, the ML-based\nsegmentation method showed superior accuracy and consistency compared to manual\nsegmentation, with an average RMS error of 0.58 +/- 0.14 mm.\n  This research advances orthopedic surgery by providing a robust framework for\nthe automated reconstruction of implanted knee models. Leveraging ML and\nmorphing algorithms, clinicians and researchers gain valuable insights into\npatient-specific knee anatomy, implant biomechanics, and surgical planning,\nleading to improved patient outcomes and enhanced quality of care.",
          "pdf_url": "http://arxiv.org/pdf/2408.01557v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2408.01557v1",
          "source": "arXiv"
        },
        {
          "title": "An Online Platform for Automatic Skull Defect Restoration and Cranial\n  Implant Design",
          "authors": [
            "Jianning Li",
            "Antonio Pepe",
            "Christina Gsaxner",
            "Jan Egger"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "We introduce a fully automatic system for cranial implant design, a common\ntask in cranioplasty operations. The system is currently integrated in\nStudierfenster (http://studierfenster.tugraz.at/), an online, cloud-based\nmedical image processing platform for medical imaging applications. Enhanced by\ndeep learning algorithms, the system automatically restores the missing part of\na skull (i.e., skull shape completion) and generates the desired implant by\nsubtracting the defective skull from the completed skull. The generated implant\ncan be downloaded in the STereoLithography (.stl) format directly via the\nbrowser interface of the system. The implant model can then be sent to a 3D\nprinter for in loco implant manufacturing. Furthermore, thanks to the standard\nformat, the user can thereafter load the model into another application for\npost-processing whenever necessary. Such an automatic cranial implant design\nsystem can be integrated into the clinical practice to improve the current\nroutine for surgeries related to skull defect repair (e.g., cranioplasty). Our\nsystem, although currently intended for educational and research use only, can\nbe seen as an application of additive manufacturing for fast, patient-specific\nimplant design.",
          "pdf_url": "http://arxiv.org/pdf/2006.00980v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2006.00980v1",
          "source": "arXiv"
        },
        {
          "title": "Dense Classification and Implanting for Few-Shot Learning",
          "authors": [
            "Yann Lifchitz",
            "Yannis Avrithis",
            "Sylvaine Picard",
            "Andrei Bursuc"
          ],
          "year": 2019,
          "citations": 0,
          "abstract": "Training deep neural networks from few examples is a highly challenging and\nkey problem for many computer vision tasks. In this context, we are targeting\nknowledge transfer from a set with abundant data to other sets with few\navailable examples. We propose two simple and effective solutions: (i) dense\nclassification over feature maps, which for the first time studies local\nactivations in the domain of few-shot learning, and (ii) implanting, that is,\nattaching new neurons to a previously trained network to learn new,\ntask-specific features. On miniImageNet, we improve the prior state-of-the-art\non few-shot classification, i.e., we achieve 62.5%, 79.8% and 83.8% on 5-way\n1-shot, 5-shot and 10-shot settings respectively.",
          "pdf_url": "http://arxiv.org/pdf/1903.05050v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1903.05050v1",
          "source": "arXiv"
        },
        {
          "title": "Edge Deep Learning for Neural Implants",
          "authors": [
            "Xilin Liu",
            "Andrew G. Richardson"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Implanted devices providing real-time neural activity classification and\ncontrol are increasingly used to treat neurological disorders, such as epilepsy\nand Parkinson's disease. Classification performance is critical to identifying\nbrain states appropriate for the therapeutic action. However, advanced\nalgorithms that have shown promise in offline studies, in particular deep\nlearning (DL) methods, have not been deployed on resource-restrained neural\nimplants. Here, we designed and optimized three embedded DL models of commonly\nadopted architectures and evaluated their inference performance in a case study\nof seizure detection. A deep neural network (DNN), a convolutional neural\nnetwork (CNN), and a long short-term memory (LSTM) network were designed to\nclassify ictal, preictal, and interictal phases from the CHB-MIT scalp EEG\ndatabase. After iterative model compression and quantization, the algorithms\nwere deployed on a general-purpose, off-the-shelf microcontroller. Inference\nsensitivity, false positive rate, execution time, memory size, and power\nconsumption were quantified. For seizure event detection, the sensitivity and\nFPR (h-1) for the DNN, CNN, and LSTM models were 87.36%/0.169, 96.70%/0.102,\nand 97.61%/0.071, respectively. Predicting seizures for early warnings was also\nfeasible. The implemented compression and quantization achieved a significant\nsaving of power and memory with an accuracy degradation of less than 0.5%. Edge\nDL models achieved performance comparable to many prior implementations that\nhad no time or computational resource limitations. Generic microcontrollers can\nprovide the required memory and computational resources, while model designs\ncan be migrated to ASICs for further optimization. The results suggest that\nedge DL inference is a feasible option for future neural implants to improve\nclassification performance and therapeutic outcomes.",
          "pdf_url": "http://arxiv.org/pdf/2012.00307v3.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2012.00307v3",
          "source": "arXiv"
        },
        {
          "title": "RibCageImp: A Deep Learning Framework for 3D Ribcage Implant Generation",
          "authors": [
            "Gyanendra Chaubey",
            "Aiman Farooq",
            "Azad Singh",
            "Deepak Mishra"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "The recovery of damaged or resected ribcage structures requires precise,\ncustom-designed implants to restore the integrity and functionality of the\nthoracic cavity. Traditional implant design methods rely mainly on manual\nprocesses, making them time-consuming and susceptible to variability. In this\nwork, we explore the feasibility of automated ribcage implant generation using\ndeep learning. We present a framework based on 3D U-Net architecture that\nprocesses CT scans to generate patient-specific implant designs. To the best of\nour knowledge, this is the first investigation into automated thoracic implant\ngeneration using deep learning approaches. Our preliminary results, while\nmoderate, highlight both the potential and the significant challenges in this\ncomplex domain. These findings establish a foundation for future research in\nautomated ribcage reconstruction and identify key technical challenges that\nneed to be addressed for practical implementation.",
          "pdf_url": "http://arxiv.org/pdf/2411.09204v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2411.09204v1",
          "source": "arXiv"
        },
        {
          "title": "Cascade learning in multi-task encoder-decoder networks for concurrent\n  bone segmentation and glenohumeral joint assessment in shoulder CT scans",
          "authors": [
            "Luca Marsilio",
            "Davide Marzorati",
            "Matteo Rossi",
            "Andrea Moglia",
            "Luca Mainardi",
            "Alfonso Manzotti",
            "Pietro Cerveri"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Osteoarthritis is a degenerative condition affecting bones and cartilage,\noften leading to osteophyte formation, bone density loss, and joint space\nnarrowing. Treatment options to restore normal joint function vary depending on\nthe severity of the condition. This work introduces an innovative deep-learning\nframework processing shoulder CT scans. It features the semantic segmentation\nof the proximal humerus and scapula, the 3D reconstruction of bone surfaces,\nthe identification of the glenohumeral (GH) joint region, and the staging of\nthree common osteoarthritic-related pathologies: osteophyte formation (OS), GH\nspace reduction (JS), and humeroscapular alignment (HSA). The pipeline\ncomprises two cascaded CNN architectures: 3D CEL-UNet for segmentation and 3D\nArthro-Net for threefold classification. A retrospective dataset of 571 CT\nscans featuring patients with various degrees of GH osteoarthritic-related\npathologies was used to train, validate, and test the pipeline. Root mean\nsquared error and Hausdorff distance median values for 3D reconstruction were\n0.22mm and 1.48mm for the humerus and 0.24mm and 1.48mm for the scapula,\noutperforming state-of-the-art architectures and making it potentially suitable\nfor a PSI-based shoulder arthroplasty preoperative plan context. The\nclassification accuracy for OS, JS, and HSA consistently reached around 90%\nacross all three categories. The computational time for the inference pipeline\nwas less than 15s, showcasing the framework's efficiency and compatibility with\northopedic radiology practice. The outcomes represent a promising advancement\ntoward the medical translation of artificial intelligence tools. This progress\naims to streamline the preoperative planning pipeline delivering high-quality\nbone surfaces and supporting surgeons in selecting the most suitable surgical\napproach according to the unique patient joint conditions.",
          "pdf_url": "http://arxiv.org/pdf/2410.12641v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2410.12641v1",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Dual-input neural network architecture",
      "final_query": "Attention mechanism classification neural network",
      "attempts": 2,
      "paper_count": 10,
      "open_access_count": 10,
      "papers": [
        {
          "title": "SECNN: Squeeze-and-Excitation Convolutional Neural Network for Sentence\n  Classification",
          "authors": [
            "Shandong Yuan"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Sentence classification is one of the basic tasks of natural language\nprocessing. Convolution neural network (CNN) has the ability to extract n-grams\nfeatures through convolutional filters and capture local correlations between\nconsecutive words in parallel, so CNN is a popular neural network architecture\nto dealing with the task. But restricted by the width of convolutional filters,\nit is difficult for CNN to capture long term contextual dependencies. Attention\nis a mechanism that considers global information and pays more attention to\nkeywords in sentences, thus attention mechanism is cooperated with CNN network\nto improve performance in sentence classification task. In our work, we don't\nfocus on keyword in a sentence, but on which CNN's output feature map is more\nimportant. We propose a Squeeze-and-Excitation Convolutional neural Network\n(SECNN) for sentence classification. SECNN takes the feature maps from multiple\nCNN as different channels of sentence representation, and then, we can utilize\nchannel attention mechanism, that is SE attention mechanism, to enable the\nmodel to learn the attention weights of different channel features. The results\nshow that our model achieves advanced performance in the sentence\nclassification task.",
          "pdf_url": "http://arxiv.org/pdf/2312.06088v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2312.06088v1",
          "source": "arXiv"
        },
        {
          "title": "Deep Features Analysis with Attention Networks",
          "authors": [
            "Shipeng Xie",
            "Da Chen",
            "Rong Zhang",
            "Hui Xue"
          ],
          "year": 2019,
          "citations": 0,
          "abstract": "Deep neural network models have recently draw lots of attention, as it\nconsistently produce impressive results in many computer vision tasks such as\nimage classification, object detection, etc. However, interpreting such model\nand show the reason why it performs quite well becomes a challenging question.\nIn this paper, we propose a novel method to interpret the neural network models\nwith attention mechanism. Inspired by the heatmap visualization, we analyze the\nrelation between classification accuracy with the attention based heatmap. An\nimproved attention based method is also included and illustrate that a better\nclassifier can be interpreted by the attention based heatmap.",
          "pdf_url": "http://arxiv.org/pdf/1901.10042v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1901.10042v1",
          "source": "arXiv"
        },
        {
          "title": "Neural Attention Models for Sequence Classification: Analysis and\n  Application to Key Term Extraction and Dialogue Act Detection",
          "authors": [
            "Sheng-syun Shen",
            "Hung-yi Lee"
          ],
          "year": 2016,
          "citations": 0,
          "abstract": "Recurrent neural network architectures combining with attention mechanism, or\nneural attention model, have shown promising performance recently for the tasks\nincluding speech recognition, image caption generation, visual question\nanswering and machine translation. In this paper, neural attention model is\napplied on two sequence classification tasks, dialogue act detection and key\nterm extraction. In the sequence labeling tasks, the model input is a sequence,\nand the output is the label of the input sequence. The major difficulty of\nsequence labeling is that when the input sequence is long, it can include many\nnoisy or irrelevant part. If the information in the whole sequence is treated\nequally, the noisy or irrelevant part may degrade the classification\nperformance. The attention mechanism is helpful for sequence classification\ntask because it is capable of highlighting important part among the entire\nsequence for the classification task. The experimental results show that with\nthe attention mechanism, discernible improvements were achieved in the sequence\nlabeling task considered here. The roles of the attention mechanism in the\ntasks are further analyzed and visualized in this paper.",
          "pdf_url": "http://arxiv.org/pdf/1604.00077v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1604.00077v1",
          "source": "arXiv"
        },
        {
          "title": "A Survey on Graph Classification and Link Prediction based on GNN",
          "authors": [
            "Xingyu Liu",
            "Juan Chen",
            "Quan Wen"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Traditional convolutional neural networks are limited to handling Euclidean\nspace data, overlooking the vast realm of real-life scenarios represented as\ngraph data, including transportation networks, social networks, and reference\nnetworks. The pivotal step in transferring convolutional neural networks to\ngraph data analysis and processing lies in the construction of graph\nconvolutional operators and graph pooling operators. This comprehensive review\narticle delves into the world of graph convolutional neural networks. Firstly,\nit elaborates on the fundamentals of graph convolutional neural networks.\nSubsequently, it elucidates the graph neural network models based on attention\nmechanisms and autoencoders, summarizing their application in node\nclassification, graph classification, and link prediction along with the\nassociated datasets.",
          "pdf_url": "http://arxiv.org/pdf/2307.00865v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2307.00865v1",
          "source": "arXiv"
        },
        {
          "title": "Text Classification with Lexicon from PreAttention Mechanism",
          "authors": [
            "QingBiao LI",
            "Chunhua Wu",
            "Kangfeng Zheng"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "A comprehensive and high-quality lexicon plays a crucial role in traditional\ntext classification approaches. And it improves the utilization of the\nlinguistic knowledge. Although it is helpful for the task, the lexicon has got\nlittle attention in recent neural network models. Firstly, getting a\nhigh-quality lexicon is not easy. We lack an effective automated lexicon\nextraction method, and most lexicons are hand crafted, which is very\ninefficient for big data. What's more, there is no an effective way to use a\nlexicon in a neural network. To address those limitations, we propose a\nPre-Attention mechanism for text classification in this paper, which can learn\nattention of different words according to their effects in the classification\ntasks. The words with different attention can form a domain lexicon.\nExperiments on three benchmark text classification tasks show that our models\nget competitive result comparing with the state-of-the-art methods. We get\n90.5% accuracy on Stanford Large Movie Review dataset, 82.3% on Subjectivity\ndataset, 93.7% on Movie Reviews. And compared with the text classification\nmodel without Pre-Attention mechanism, those with Pre-Attention mechanism\nimprove by 0.9%-2.4% accuracy, which proves the validity of the Pre-Attention\nmechanism. In addition, the Pre-Attention mechanism performs well followed by\ndifferent types of neural networks (e.g., convolutional neural networks and\nLong Short-Term Memory networks). For the same dataset, when we use\nPre-Attention mechanism to get attention value followed by different neural\nnetworks, those words with high attention values have a high degree of\ncoincidence, which proves the versatility and portability of the Pre-Attention\nmechanism. we can get stable lexicons by attention values, which is an\ninspiring method of information extraction.",
          "pdf_url": "http://arxiv.org/pdf/2002.07591v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2002.07591v1",
          "source": "arXiv"
        },
        {
          "title": "A Multi-Channel Temporal Attention Convolutional Neural Network Model\n  for Environmental Sound Classification",
          "authors": [
            "You Wang",
            "Chuyao Feng",
            "David V. Anderson"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Recently, many attention-based deep neural networks have emerged and achieved\nstate-of-the-art performance in environmental sound classification. The essence\nof attention mechanism is assigning contribution weights on different parts of\nfeatures, namely channels, spectral or spatial contents, and temporal frames.\nIn this paper, we propose an effective convolutional neural network structure\nwith a multi-channel temporal attention (MCTA) block, which applies a temporal\nattention mechanism within each channel of the embedded features to extract\nchannel-wise relevant temporal information. This multi-channel temporal\nattention structure will result in a distinct attention vector for each\nchannel, which enables the network to fully exploit the relevant temporal\ninformation in different channels. The datasets used to test our model include\nESC-50 and its subset ESC-10, along with development sets of DCASE 2018 and\n2019. In our experiments, MCTA performed better than the single-channel\ntemporal attention model and the non-attention model with the same number of\nparameters. Furthermore, we compared our model with some successful\nattention-based models and obtained competitive results with a relatively\nlighter network.",
          "pdf_url": "http://arxiv.org/pdf/2011.02561v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2011.02561v1",
          "source": "arXiv"
        },
        {
          "title": "Deep neural network-based classification model for Sentiment Analysis",
          "authors": [
            "Donghang Pan",
            "Jingling Yuan",
            "Lin Li",
            "Deming Sheng"
          ],
          "year": 2019,
          "citations": 0,
          "abstract": "The growing prosperity of social networks has brought great challenges to the\nsentimental tendency mining of users. As more and more researchers pay\nattention to the sentimental tendency of online users, rich research results\nhave been obtained based on the sentiment classification of explicit texts.\nHowever, research on the implicit sentiment of users is still in its infancy.\nAiming at the difficulty of implicit sentiment classification, a research on\nimplicit sentiment classification model based on deep neural network is carried\nout. Classification models based on DNN, LSTM, Bi-LSTM and CNN were established\nto judge the tendency of the user's implicit sentiment text. Based on the\nBi-LSTM model, the classification model of word-level attention mechanism is\nstudied. The experimental results on the public dataset show that the\nestablished LSTM series classification model and CNN classification model can\nachieve good sentiment classification effect, and the classification effect is\nsignificantly better than the DNN model. The Bi-LSTM based attention mechanism\nclassification model obtained the optimal R value in the positive category\nidentification.",
          "pdf_url": "http://arxiv.org/pdf/1907.02046v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1907.02046v1",
          "source": "arXiv"
        },
        {
          "title": "Attention mechanisms for physiological signal deep learning: which\n  attention should we take?",
          "authors": [
            "Seong-A Park",
            "Hyung-Chul Lee",
            "Chul-Woo Jung",
            "Hyun-Lim Yang"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "Attention mechanisms are widely used to dramatically improve deep learning\nmodel performance in various fields. However, their general ability to improve\nthe performance of physiological signal deep learning model is immature. In\nthis study, we experimentally analyze four attention mechanisms (e.g.,\nsqueeze-and-excitation, non-local, convolutional block attention module, and\nmulti-head self-attention) and three convolutional neural network (CNN)\narchitectures (e.g., VGG, ResNet, and Inception) for two representative\nphysiological signal prediction tasks: the classification for predicting\nhypotension and the regression for predicting cardiac output (CO). We evaluated\nmultiple combinations for performance and convergence of physiological signal\ndeep learning model. Accordingly, the CNN models with the spatial attention\nmechanism showed the best performance in the classification problem, whereas\nthe channel attention mechanism achieved the lowest error in the regression\nproblem. Moreover, the performance and convergence of the CNN models with\nattention mechanisms were better than stand-alone self-attention models in both\nproblems. Hence, we verified that convolutional operation and attention\nmechanisms are complementary and provide faster convergence time, despite the\nstand-alone self-attention models requiring fewer parameters.",
          "pdf_url": "http://arxiv.org/pdf/2207.06904v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2207.06904v1",
          "source": "arXiv"
        },
        {
          "title": "Comparative Analysis of Attention Mechanisms for Automatic Modulation\n  Classification in Radio Frequency Signals",
          "authors": [
            "Ferhat Ozgur Catak",
            "Murat Kuzlu",
            "Umit Cali"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Automatic Modulation Classification (AMC) is a critical component in\ncognitive radio systems and spectrum management applications. This study\npresents a comprehensive comparative analysis of three attention mechanisms\n(i.e., baseline multi-head attention, causal attention, and sparse attention)\nintegrated with Convolutional Neural Networks (CNNs) for radio frequency (RF)\nsignal classification. It proposes a novel CNN-Transformer hybrid architecture\nthat leverages different attention patterns to capture temporal dependencies in\nI/Q samples from the RML2016.10a dataset. The experimental results demonstrate\nthat while baseline attention achieves the highest accuracy of 85.05\\%, causal\nand sparse attention mechanisms offer significant computational advantages with\ninference times reduced by 83\\% and 75\\% respectively, while maintaining\ncompetitive classification performance above 84\\%. The analysis reveals\ndistinct attention pattern preferences across different modulation schemes,\nproviding insights for designing efficient attention mechanisms for real-time\nradio signal processing applications.",
          "pdf_url": "http://arxiv.org/pdf/2508.09996v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2508.09996v1",
          "source": "arXiv"
        },
        {
          "title": "Quantum Self-Attention Neural Networks for Text Classification",
          "authors": [
            "Guangxi Li",
            "Xuanqiang Zhao",
            "Xin Wang"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "An emerging direction of quantum computing is to establish meaningful quantum\napplications in various fields of artificial intelligence, including natural\nlanguage processing (NLP). Although some efforts based on syntactic analysis\nhave opened the door to research in Quantum NLP (QNLP), limitations such as\nheavy syntactic preprocessing and syntax-dependent network architecture make\nthem impracticable on larger and real-world data sets. In this paper, we\npropose a new simple network architecture, called the quantum self-attention\nneural network (QSANN), which can compensate for these limitations.\nSpecifically, we introduce the self-attention mechanism into quantum neural\nnetworks and then utilize a Gaussian projected quantum self-attention serving\nas a sensible quantum version of self-attention. As a result, QSANN is\neffective and scalable on larger data sets and has the desirable property of\nbeing implementable on near-term quantum devices. In particular, our QSANN\noutperforms the best existing QNLP model based on syntactic analysis as well as\na simple classical self-attention neural network in numerical experiments of\ntext classification tasks on public data sets. We further show that our method\nexhibits robustness to low-level quantum noises and showcases resilience to\nquantum neural network architectures.",
          "pdf_url": "http://arxiv.org/pdf/2205.05625v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2205.05625v2",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Multi-scale feature representation",
      "final_query": "Multi-scale feature representation",
      "attempts": 1,
      "paper_count": 10,
      "open_access_count": 10,
      "papers": [
        {
          "title": "An Enhanced Deep Feature Representation for Person Re-identification",
          "authors": [
            "Shangxuan Wu",
            "Ying-Cong Chen",
            "Xiang Li",
            "An-Cong Wu",
            "Jin-Jie You",
            "Wei-Shi Zheng"
          ],
          "year": 2016,
          "citations": 0,
          "abstract": "Feature representation and metric learning are two critical components in\nperson re-identification models. In this paper, we focus on the feature\nrepresentation and claim that hand-crafted histogram features can be\ncomplementary to Convolutional Neural Network (CNN) features. We propose a\nnovel feature extraction model called Feature Fusion Net (FFN) for pedestrian\nimage representation. In FFN, back propagation makes CNN features constrained\nby the handcrafted features. Utilizing color histogram features (RGB, HSV,\nYCbCr, Lab and YIQ) and texture features (multi-scale and multi-orientation\nGabor features), we get a new deep feature representation that is more\ndiscriminative and compact. Experiments on three challenging datasets (VIPeR,\nCUHK01, PRID450s) validates the effectiveness of our proposal.",
          "pdf_url": "http://arxiv.org/pdf/1604.07807v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1604.07807v2",
          "source": "arXiv"
        },
        {
          "title": "Cross-projective representations of pairs of anticommutative algebras,\n  alloys and finite-dimensional irreducible representations of some\n  infinite-dimensional Lie algebras",
          "authors": [
            "Denis V. Juriev"
          ],
          "year": 1998,
          "citations": 0,
          "abstract": "The article is devoted to some ``strange'' phenomena of representation theory\nand their interrelations. Cross-projective representations of pairs of\nanticommutative algebras, alloys, their universal envelopping Lie algebras and\ntheir representations, quaternary algebras and their alloyability are\ndiscussed. Considered examples allow to conclude that new representations have\nsome intriguing features (continuous moduli of finite-dimensional irreducible\nrepresentations, sophisticated Clebsch-Gordan coefficient calculus, etc.).",
          "pdf_url": "http://arxiv.org/pdf/math/9806005v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/math/9806005v1",
          "source": "arXiv"
        },
        {
          "title": "CL4CTR: A Contrastive Learning Framework for CTR Prediction",
          "authors": [
            "Fangye Wang",
            "Yingxu Wang",
            "Dongsheng Li",
            "Hansu Gu",
            "Tun Lu",
            "Peng Zhang",
            "Ning Gu"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "Many Click-Through Rate (CTR) prediction works focused on designing advanced\narchitectures to model complex feature interactions but neglected the\nimportance of feature representation learning, e.g., adopting a plain embedding\nlayer for each feature, which results in sub-optimal feature representations\nand thus inferior CTR prediction performance. For instance, low frequency\nfeatures, which account for the majority of features in many CTR tasks, are\nless considered in standard supervised learning settings, leading to\nsub-optimal feature representations. In this paper, we introduce\nself-supervised learning to produce high-quality feature representations\ndirectly and propose a model-agnostic Contrastive Learning for CTR (CL4CTR)\nframework consisting of three self-supervised learning signals to regularize\nthe feature representation learning: contrastive loss, feature alignment, and\nfield uniformity. The contrastive module first constructs positive feature\npairs by data augmentation and then minimizes the distance between the\nrepresentations of each positive feature pair by the contrastive loss. The\nfeature alignment constraint forces the representations of features from the\nsame field to be close, and the field uniformity constraint forces the\nrepresentations of features from different fields to be distant. Extensive\nexperiments verify that CL4CTR achieves the best performance on four datasets\nand has excellent effectiveness and compatibility with various representative\nbaselines.",
          "pdf_url": "http://arxiv.org/pdf/2212.00522v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2212.00522v1",
          "source": "arXiv"
        },
        {
          "title": "Self-optimizing Feature Generation via Categorical Hashing\n  Representation and Hierarchical Reinforcement Crossing",
          "authors": [
            "Wangyang Ying",
            "Dongjie Wang",
            "Kunpeng Liu",
            "Leilei Sun",
            "Yanjie Fu"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Feature generation aims to generate new and meaningful features to create a\ndiscriminative representation space.A generated feature is meaningful when the\ngenerated feature is from a feature pair with inherent feature interaction. In\nthe real world, experienced data scientists can identify potentially useful\nfeature-feature interactions, and generate meaningful dimensions from an\nexponentially large search space, in an optimal crossing form over an optimal\ngeneration path. But, machines have limited human-like abilities.We generalize\nsuch learning tasks as self-optimizing feature generation. Self-optimizing\nfeature generation imposes several under-addressed challenges on existing\nsystems: meaningful, robust, and efficient generation. To tackle these\nchallenges, we propose a principled and generic representation-crossing\nframework to solve self-optimizing feature generation.To achieve hashing\nrepresentation, we propose a three-step approach: feature discretization,\nfeature hashing, and descriptive summarization. To achieve reinforcement\ncrossing, we develop a hierarchical reinforcement feature crossing approach.We\npresent extensive experimental results to demonstrate the effectiveness and\nefficiency of the proposed method. The code is available at\nhttps://github.com/yingwangyang/HRC_feature_cross.git.",
          "pdf_url": "http://arxiv.org/pdf/2309.04612v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2309.04612v2",
          "source": "arXiv"
        },
        {
          "title": "Long Short View Feature Decomposition via Contrastive Video\n  Representation Learning",
          "authors": [
            "Nadine Behrmann",
            "Mohsen Fayyaz",
            "Juergen Gall",
            "Mehdi Noroozi"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Self-supervised video representation methods typically focus on the\nrepresentation of temporal attributes in videos. However, the role of\nstationary versus non-stationary attributes is less explored: Stationary\nfeatures, which remain similar throughout the video, enable the prediction of\nvideo-level action classes. Non-stationary features, which represent temporally\nvarying attributes, are more beneficial for downstream tasks involving more\nfine-grained temporal understanding, such as action segmentation. We argue that\na single representation to capture both types of features is sub-optimal, and\npropose to decompose the representation space into stationary and\nnon-stationary features via contrastive learning from long and short views,\ni.e. long video sequences and their shorter sub-sequences. Stationary features\nare shared between the short and long views, while non-stationary features\naggregate the short views to match the corresponding long view. To empirically\nverify our approach, we demonstrate that our stationary features work\nparticularly well on an action recognition downstream task, while our\nnon-stationary features perform better on action segmentation. Furthermore, we\nanalyse the learned representations and find that stationary features capture\nmore temporally stable, static attributes, while non-stationary features\nencompass more temporally varying ones.",
          "pdf_url": "http://arxiv.org/pdf/2109.11593v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2109.11593v1",
          "source": "arXiv"
        },
        {
          "title": "Semantically Consistent Multi-view Representation Learning",
          "authors": [
            "Yiyang Zhou",
            "Qinghai Zheng",
            "Shunshun Bai",
            "Jihua Zhu"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "In this work, we devote ourselves to the challenging task of Unsupervised\nMulti-view Representation Learning (UMRL), which requires learning a unified\nfeature representation from multiple views in an unsupervised manner. Existing\nUMRL methods mainly concentrate on the learning process in the feature space\nwhile ignoring the valuable semantic information hidden in different views. To\naddress this issue, we propose a novel Semantically Consistent Multi-view\nRepresentation Learning (SCMRL), which makes efforts to excavate underlying\nmulti-view semantic consensus information and utilize the information to guide\nthe unified feature representation learning. Specifically, SCMRL consists of a\nwithin-view reconstruction module and a unified feature representation learning\nmodule, which are elegantly integrated by the contrastive learning strategy to\nsimultaneously align semantic labels of both view-specific feature\nrepresentations and the learned unified feature representation. In this way,\nthe consensus information in the semantic space can be effectively exploited to\nconstrain the learning process of unified feature representation. Compared with\nseveral state-of-the-art algorithms, extensive experiments demonstrate its\nsuperiority.",
          "pdf_url": "http://arxiv.org/pdf/2303.04366v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2303.04366v1",
          "source": "arXiv"
        },
        {
          "title": "Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop\n  Feature Quality Estimation",
          "authors": [
            "Shiyuan Li",
            "Yixin Liu",
            "Qingfeng Chen",
            "Geoffrey I. Webb",
            "Shirui Pan"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Unsupervised graph representation learning (UGRL) based on graph neural\nnetworks (GNNs), has received increasing attention owing to its efficacy in\nhandling graph-structured data. However, existing UGRL methods ideally assume\nthat the node features are noise-free, which makes them fail to distinguish\nbetween useful information and noise when applied to real data with noisy\nfeatures, thus affecting the quality of learned representations. This urges us\nto take node noisy features into account in real-world UGRL. With empirical\nanalysis, we reveal that feature propagation, the essential operation in GNNs,\nacts as a \"double-edged sword\" in handling noisy features - it can both denoise\nand diffuse noise, leading to varying feature quality across nodes, even within\nthe same node at different hops. Building on this insight, we propose a novel\nUGRL method based on Multi-hop feature Quality Estimation (MQE for short).\nUnlike most UGRL models that directly utilize propagation-based GNNs to\ngenerate representations, our approach aims to learn representations through\nestimating the quality of propagated features at different hops. Specifically,\nwe introduce a Gaussian model that utilizes a learnable \"meta-representation\"\nas a condition to estimate the expectation and variance of multi-hop propagated\nfeatures via neural networks. In this way, the \"meta representation\" captures\nthe semantic and structural information underlying multiple propagated features\nbut is naturally less susceptible to interference by noise, thereby serving as\nhigh-quality node representations beneficial for downstream tasks. Extensive\nexperiments on multiple real-world datasets demonstrate that MQE in learning\nreliable node representations in scenarios with diverse types of feature noise.",
          "pdf_url": "http://arxiv.org/pdf/2407.19944v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2407.19944v1",
          "source": "arXiv"
        },
        {
          "title": "A Comparison of Representation Learning Methods for Dimensionality\n  Reduction of fMRI Scans for Classification of ADHD",
          "authors": [
            "Bhaskar Sen"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "This paper compares three feature representation techniques used to represent\nresting state functional magnetic resonance (fMRI) scans. The proposed models\nof feature representation consider the time averaged fMRI scans as raw\nrepresentation of image data. The effectiveness of the representation is\nevaluated by using these features for classification of Attention Deficit\nHyperactivity Disorder (ADHD) patients from healthy controls. The\ndimensionality reduction methods used for feature representation are\nmaximum-variance unfolding, locally linear embedding and auto-encoders. The\nclassifiers tested for classification purpose were neural net and support\nvector machine. Using auto-encoders with four hidden layers along with a\nsupport vector machine classifier yielded a classification accuracy of 61.25%\nalong with 65.69% sensitivity and 52.20% specificity.",
          "pdf_url": "http://arxiv.org/pdf/2202.01989v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2202.01989v1",
          "source": "arXiv"
        },
        {
          "title": "A simple and effective postprocessing method for image classification",
          "authors": [
            "Yan Liu",
            "Yun Li",
            "Yunhao Yuan",
            "jipeng qiang"
          ],
          "year": 2019,
          "citations": 0,
          "abstract": "Whether it is computer vision, natural language processing or speech\nrecognition, the essence of these applications is to obtain powerful feature\nrepresentations that make downstream applications completion more efficient.\nTaking image recognition as an example, whether it is hand-crafted low-level\nfeature representation or feature representation extracted by a convolutional\nneural networks(CNNs), the goal is to extract features that better represent\nimage features, thereby improving classification accuracy. However, we observed\nthat image feature representations share a large common vector and a few top\ndominating directions. To address this problems, we propose a simple but\neffective postprocessing method to render off-the-shelf feature representations\neven stronger by eliminating the common mean vector from off-the-shelf feature\nrepresentations. The postprocessing is empirically validated on a variety of\ndatasets and feature extraction methods.such as VGG, LBP, and HOG. Some\nexperiments show that the features that have been post-processed by\npostprocessing algorithm can get better results than original ones.",
          "pdf_url": "http://arxiv.org/pdf/1906.07934v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1906.07934v1",
          "source": "arXiv"
        },
        {
          "title": "Unsupervised patient representations from clinical notes with\n  interpretable classification decisions",
          "authors": [
            "Madhumita Sushil",
            "Simon Šuster",
            "Kim Luyckx",
            "Walter Daelemans"
          ],
          "year": 2017,
          "citations": 0,
          "abstract": "We have two main contributions in this work: 1. We explore the usage of a\nstacked denoising autoencoder, and a paragraph vector model to learn\ntask-independent dense patient representations directly from clinical notes. We\nevaluate these representations by using them as features in multiple supervised\nsetups, and compare their performance with those of sparse representations. 2.\nTo understand and interpret the representations, we explore the best encoded\nfeatures within the patient representations obtained from the autoencoder\nmodel. Further, we calculate the significance of the input features of the\ntrained classifiers when we use these pretrained representations as input.",
          "pdf_url": "http://arxiv.org/pdf/1711.05198v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1711.05198v1",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Medical image analysis TRIPOD CLAIM",
      "final_query": "Medical image analysis TRIPOD CLAIM",
      "attempts": 1,
      "paper_count": 10,
      "open_access_count": 10,
      "papers": [
        {
          "title": "False Promises in Medical Imaging AI? Assessing Validity of\n  Outperformance Claims",
          "authors": [
            "Evangelia Christodoulou",
            "Annika Reinke",
            "Pascaline Andrè",
            "Patrick Godau",
            "Piotr Kalinowski",
            "Rola Houhou",
            "Selen Erkan",
            "Carole H. Sudre",
            "Ninon Burgos",
            "Sofiène Boutaj",
            "Sophie Loizillon",
            "Maëlys Solal",
            "Veronika Cheplygina",
            "Charles Heitz",
            "Michal Kozubek",
            "Michela Antonelli",
            "Nicola Rieke",
            "Antoine Gilson",
            "Leon D. Mayer",
            "Minu D. Tizabi",
            "M. Jorge Cardoso",
            "Amber Simpson",
            "Annette Kopp-Schneider",
            "Gaël Varoquaux",
            "Olivier Colliot",
            "Lena Maier-Hein"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Performance comparisons are fundamental in medical imaging Artificial\nIntelligence (AI) research, often driving claims of superiority based on\nrelative improvements in common performance metrics. However, such claims\nfrequently rely solely on empirical mean performance. In this paper, we\ninvestigate whether newly proposed methods genuinely outperform the state of\nthe art by analyzing a representative cohort of medical imaging papers. We\nquantify the probability of false claims based on a Bayesian approach that\nleverages reported results alongside empirically estimated model congruence to\nestimate whether the relative ranking of methods is likely to have occurred by\nchance. According to our results, the majority (>80%) of papers claims\noutperformance when introducing a new method. Our analysis further revealed a\nhigh probability (>5%) of false outperformance claims in 86% of classification\npapers and 53% of segmentation papers. These findings highlight a critical flaw\nin current benchmarking practices: claims of outperformance in medical imaging\nAI are frequently unsubstantiated, posing a risk of misdirecting future\nresearch efforts.",
          "pdf_url": "http://arxiv.org/pdf/2505.04720v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2505.04720v2",
          "source": "arXiv"
        },
        {
          "title": "Towards Detecting Cascades of Biased Medical Claims on Twitter",
          "authors": [
            "Libby Tiderman",
            "Juan Sanchez Mercedes",
            "Fiona Romanoschi",
            "Fabricio Murai"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Social media may disseminate medical claims that highlight misleading\ncorrelations between social identifiers and diseases due to not accounting for\nstructural determinants of health. Our research aims to identify biased medical\nclaims on Twitter and measure their spread. We propose a machine learning\nframework that uses two models in tandem: RoBERTa to detect medical claims and\nDistilBERT to classify bias. After identifying original biased medical claims,\nwe conducted a retweet cascade analysis, computing their individual reach and\nrate of spread. Tweets containing biased claims were found to circulate faster\nand further than unbiased claims.",
          "pdf_url": "http://arxiv.org/pdf/2312.15040v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2312.15040v1",
          "source": "arXiv"
        },
        {
          "title": "Tree and Tripod Nim",
          "authors": [
            "Aidan Hennessey"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "This paper introduces a variant of the impartial combinatorial game nim,\ncalled tree nim, as well as a particular case of tree nim called tripod nim. A\ncertain existence-uniqueness result and a periodicity result are proven about\nthe distribution of $\\mathcal{P}$-positions and Grundy values in tree nim.\nTripod nim is associated to a family of arrays similar to those which arise in\nthe study of sequential compound games. Using these arrays, a partial analysis\nis given for tripod nim. Conjectures relating to the periods of the rows of\nthese arrays are put forward.",
          "pdf_url": "http://arxiv.org/pdf/2401.07943v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2401.07943v1",
          "source": "arXiv"
        },
        {
          "title": "Iterative Tree Analysis for Medical Critics",
          "authors": [
            "Zenan Huang",
            "Mingwei Li",
            "Zheng Zhou",
            "Youxin Jiang"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Large Language Models (LLMs) have been widely adopted across various domains,\nyet their application in the medical field poses unique challenges,\nparticularly concerning the generation of hallucinations. Hallucinations in\nopen-ended long medical text manifest as misleading critical claims, which are\ndifficult to verify due to two reasons. First, critical claims are often deeply\nentangled within the text and cannot be extracted based solely on surface-level\npresentation. Second, verifying these claims is challenging because\nsurface-level token-based retrieval often lacks precise or specific evidence,\nleaving the claims unverifiable without deeper mechanism-based analysis. In\nthis paper, we introduce a novel method termed Iterative Tree Analysis (ITA)\nfor medical critics. ITA is designed to extract implicit claims from long\nmedical texts and verify each claim through an iterative and adaptive tree-like\nreasoning process. This process involves a combination of top-down task\ndecomposition and bottom-up evidence consolidation, enabling precise\nverification of complex medical claims through detailed mechanism-level\nreasoning. Our extensive experiments demonstrate that ITA significantly\noutperforms previous methods in detecting factual inaccuracies in complex\nmedical text verification tasks by 10%. Additionally, we will release a\ncomprehensive test set to the public, aiming to foster further advancements in\nresearch within this domain.",
          "pdf_url": "http://arxiv.org/pdf/2501.10642v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2501.10642v1",
          "source": "arXiv"
        },
        {
          "title": "RELICT: A Replica Detection Framework for Medical Image Generation",
          "authors": [
            "Orhun Utku Aydin",
            "Alexander Koch",
            "Adam Hilbert",
            "Jana Rieger",
            "Felix Lohrke",
            "Fujimaro Ishida",
            "Satoru Tanioka",
            "Dietmar Frey"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Despite the potential of synthetic medical data for augmenting and improving\nthe generalizability of deep learning models, memorization in generative models\ncan lead to unintended leakage of sensitive patient information and limit model\nutility. Thus, the use of memorizing generative models in the medical domain\ncan jeopardize patient privacy. We propose a framework for identifying\nreplicas, i.e. nearly identical copies of the training data, in synthetic\nmedical image datasets. Our REpLIca deteCTion (RELICT) framework for medical\nimage generative models evaluates image similarity using three complementary\napproaches: (1) voxel-level analysis, (2) feature-level analysis by a\npretrained medical foundation model, and (3) segmentation-level analysis. Two\nclinically relevant 3D generative modelling use cases were investigated:\nnon-contrast head CT with intracerebral hemorrhage (N=774) and time-of-flight\nMR angiography of the Circle of Willis (N=1,782). Expert visual scoring was\nused as the reference standard to assess the presence of replicas. We report\nthe balanced accuracy at the optimal threshold to assess replica classification\nperformance. The reference visual rating identified 45 of 50 and 5 of 50\ngenerated images as replicas for the NCCT and TOF-MRA use cases, respectively.\nImage-level and feature-level measures perfectly classified replicas with a\nbalanced accuracy of 1 when an optimal threshold was selected for the NCCT use\ncase. A perfect classification of replicas for the TOF-MRA case was not\npossible at any threshold, with the segmentation-level analysis achieving a\nbalanced accuracy of 0.79. Replica detection is a crucial but neglected\nvalidation step for the development of generative models in medical imaging.\nThe proposed RELICT framework provides a standardized, easy-to-use tool for\nreplica detection and aims to facilitate responsible and ethical medical image\nsynthesis.",
          "pdf_url": "http://arxiv.org/pdf/2502.17360v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2502.17360v1",
          "source": "arXiv"
        },
        {
          "title": "Transformer-based unsupervised patient representation learning based on\n  medical claims for risk stratification and analysis",
          "authors": [
            "Xianlong Zeng",
            "Simon Lin",
            "Chang Liu"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "The claims data, containing medical codes, services information, and incurred\nexpenditure, can be a good resource for estimating an individual's health\ncondition and medical risk level. In this study, we developed Transformer-based\nMultimodal AutoEncoder (TMAE), an unsupervised learning framework that can\nlearn efficient patient representation by encoding meaningful information from\nthe claims data. TMAE is motivated by the practical needs in healthcare to\nstratify patients into different risk levels for improving care delivery and\nmanagement. Compared to previous approaches, TMAE is able to 1) model\ninpatient, outpatient, and medication claims collectively, 2) handle irregular\ntime intervals between medical events, 3) alleviate the sparsity issue of the\nrare medical codes, and 4) incorporate medical expenditure information. We\ntrained TMAE using a real-world pediatric claims dataset containing more than\n600,000 patients and compared its performance with various approaches in two\nclustering tasks. Experimental results demonstrate that TMAE has superior\nperformance compared to all baselines. Multiple downstream applications are\nalso conducted to illustrate the effectiveness of our framework. The promising\nresults confirm that the TMAE framework is scalable to large claims data and is\nable to generate efficient patient embeddings for risk stratification and\nanalysis.",
          "pdf_url": "http://arxiv.org/pdf/2106.12658v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2106.12658v1",
          "source": "arXiv"
        },
        {
          "title": "A survey on attention mechanisms for medical applications: are we moving\n  towards better algorithms?",
          "authors": [
            "Tiago Gonçalves",
            "Isabel Rio-Torto",
            "Luís F. Teixeira",
            "Jaime S. Cardoso"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "The increasing popularity of attention mechanisms in deep learning algorithms\nfor computer vision and natural language processing made these models\nattractive to other research domains. In healthcare, there is a strong need for\ntools that may improve the routines of the clinicians and the patients.\nNaturally, the use of attention-based algorithms for medical applications\noccurred smoothly. However, being healthcare a domain that depends on\nhigh-stake decisions, the scientific community must ponder if these\nhigh-performing algorithms fit the needs of medical applications. With this\nmotto, this paper extensively reviews the use of attention mechanisms in\nmachine learning (including Transformers) for several medical applications.\nThis work distinguishes itself from its predecessors by proposing a critical\nanalysis of the claims and potentialities of attention mechanisms presented in\nthe literature through an experimental case study on medical image\nclassification with three different use cases. These experiments focus on the\nintegrating process of attention mechanisms into established deep learning\narchitectures, the analysis of their predictive power, and a visual assessment\nof their saliency maps generated by post-hoc explanation methods. This paper\nconcludes with a critical analysis of the claims and potentialities presented\nin the literature about attention mechanisms and proposes future research lines\nin medical applications that may benefit from these frameworks.",
          "pdf_url": "http://arxiv.org/pdf/2204.12406v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2204.12406v1",
          "source": "arXiv"
        },
        {
          "title": "Topology and curvature of metric spaces",
          "authors": [
            "Parvaneh Joharinad",
            "Jürgen Jost"
          ],
          "year": 2019,
          "citations": 0,
          "abstract": "We develop a new concept of non-positive curvature for metric spaces, based\non intersection patterns of closed balls. In contrast to the synthetic\napproaches of Alexandrov and Buesemann, our concept also applies to metric\nspaces that might be discrete. The natural comparison spaces that emerge from\nour discussion are no longer Euclidean spaces, but rather tripod spaces. These\ntripod spaces include the hyperconvex spaces which have trivial Cech homology.\nThis suggests a link of our geometrical method to the topological method of\npersistent homology in topological data analysis. We also investigate the\ngeometry of general tripod spaces.",
          "pdf_url": "http://arxiv.org/pdf/1904.00222v3.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1904.00222v3",
          "source": "arXiv"
        },
        {
          "title": "A Novel Home-Built Metrology to Analyze Oral Fluid Droplets and Quantify\n  the Efficacy of Masks",
          "authors": [
            "Ava Tan Bhowmik"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "Wearing masks is crucial to preventing the spread of potentially\npathogen-containing droplets, especially amidst the COVID-19 pandemic. However,\nnot all face coverings are equally effective and most experiments evaluating\nmask efficacy are very expensive and complex to operate. In this work, a novel,\nhome-built, low-cost, and accurate metrology to visualize orally-generated\nfluid droplets has been developed. The project includes setup optimization,\ndata collection, data analysis, and applications. The final materials chosen\nwere quinine-containing tonic water, 397-402 nm wavelength UV tube lights, an\niPhone and tripod, string, and a spray bottle. The experiment took place in a\ndark closet with a dark background. During data collection, the test subject\nfirst wets their mouth with an ingestible fluorescent liquid (tonic water) and\nspeaks, sneezes, or coughs under UV darklight. The fluorescence from the tonic\nwater droplets generated can be visualized, recorded by an iPhone 8+ camera in\nslo-mo (240 fps), and analyzed. The software VLC is used for frame separation\nand Fiji/ImageJ is used for image processing and analysis. The dependencies of\noral fluid droplet generation and propagation on different phonics, the\nloudness of speech, and the type of expiratory event were studied in detail and\nestablished using the metrology developed. The efficacy of different types of\nmasks was evaluated and correlated with fabric microstructures. All masks\nblocked droplets to varying extent. Masks with smaller-sized pores and thicker\nmaterial were found to block the most droplets. This low-cost technique can be\neasily constructed at home using materials that total to a cost of less than\n$50. Despite the minimal cost, the method is very accurate and the data is\nquantifiable.",
          "pdf_url": "http://arxiv.org/pdf/2201.03993v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2201.03993v1",
          "source": "arXiv"
        },
        {
          "title": "From Claims to Evidence: A Unified Framework and Critical Analysis of\n  CNN vs. Transformer vs. Mamba in Medical Image Segmentation",
          "authors": [
            "Pooya Mohammadi Kazaj",
            "Giovanni Baj",
            "Yazdan Salimi",
            "Anselm W. Stark",
            "Waldo Valenzuela",
            "George CM. Siontis",
            "Habib Zaidi",
            "Mauricio Reyes",
            "Christoph Graeni",
            "Isaac Shiri"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "While numerous architectures for medical image segmentation have been\nproposed, achieving competitive performance with state-of-the-art models\nnetworks such as nnUNet, still leave room for further innovation. In this work,\nwe introduce nnUZoo, an open source benchmarking framework built upon nnUNet,\nwhich incorporates various deep learning architectures, including CNNs,\nTransformers, and Mamba-based models. Using this framework, we provide a fair\ncomparison to demystify performance claims across different medical image\nsegmentation tasks. Additionally, in an effort to enrich the benchmarking, we\nexplored five new architectures based on Mamba and Transformers, collectively\nnamed X2Net, and integrated them into nnUZoo for further evaluation. The\nproposed models combine the features of conventional U2Net, nnUNet, CNN,\nTransformer, and Mamba layers and architectures, called X2Net (UNETR2Net\n(UNETR), SwT2Net (SwinTransformer), SS2D2Net (SwinUMamba), Alt1DM2Net\n(LightUMamba), and MambaND2Net (MambaND)). We extensively evaluate the\nperformance of different models on six diverse medical image segmentation\ndatasets, including microscopy, ultrasound, CT, MRI, and PET, covering various\nbody parts, organs, and labels. We compare their performance, in terms of dice\nscore and computational efficiency, against their baseline models, U2Net, and\nnnUNet. CNN models like nnUNet and U2Net demonstrated both speed and accuracy,\nmaking them effective choices for medical image segmentation tasks.\nTransformer-based models, while promising for certain imaging modalities,\nexhibited high computational costs. Proposed Mamba-based X2Net architecture\n(SS2D2Net) achieved competitive accuracy with no significantly difference from\nnnUNet and U2Net, while using fewer parameters. However, they required\nsignificantly longer training time, highlighting a trade-off between model\nefficiency and computational cost.",
          "pdf_url": "http://arxiv.org/pdf/2503.01306v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2503.01306v1",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Imbalanced classification medical imaging",
      "final_query": "Imbalanced classification medical imaging",
      "attempts": 1,
      "paper_count": 20,
      "open_access_count": 14,
      "papers": [
        {
          "title": "Imbalanced Classification in Medical Imaging via Regrouping",
          "authors": [
            "Le Peng",
            "Y. Travadi",
            "Rui Zhang",
            "Yingwei Cui",
            "Ju Sun"
          ],
          "year": 2022,
          "citations": 5,
          "abstract": "We propose performing imbalanced classification by regrouping majority classes into small classes so that we turn the problem into balanced multiclass classification. This new idea is dramatically different from popular loss reweighting and class resampling methods. Our preliminary result on imbalanced medical image classification shows that this natural idea can substantially boost the classification performance as measured by average precision (approximately area-under-the-precision-recall-curve, or AUPRC), which is more appropriate for evaluating imbalanced classification than other metrics such as balanced accuracy.",
          "pdf_url": "https://arxiv.org/pdf/2210.12234",
          "doi": "10.48550/arXiv.2210.12234",
          "url": "https://www.semanticscholar.org/paper/f277b1aab84277e93b08db4e213aa828e3365941",
          "source": "Semantic Scholar"
        },
        {
          "title": "Transduction Enhanced Inductive Inference for Imbalanced Classification in Medical Images",
          "authors": [
            "Yizhe Zhang",
            "Tao Zhou",
            "Ye Wu",
            "Shuo Wang",
            "D. Chen"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "In medical images, disease and rare cases often have a much smaller population than normal cases. Due to their smaller sample sizes, these minority classes can easily be overwhelmed by the majority classes during model training. Disease and rare cases are of main interest in medical image analysis and diagnosis. Techniques such as class re-weighting, minority over-sampling, and majority under-sampling have been used to tackle the class imbalance problem. Although these techniques showed successes, their hyper-parameters (e.g., class weights) are not easy to determine and adjust in practice, and their effects can be marginal in sub-optimal settings. In another track of research, algorithms such as under-bagging KNN perform balanced transductive inference in a given feature space. This type of algorithms shows strong theoretical guarantee and impressive empirical results for imbalanced classification but is restricted by the quality of the given features. In this paper, we propose to combine a class-balanced transductive inference method with an inductive inference model (e.g., ResNet) in such a way that: (1) the inductive learning will yield features more suitable for transductive inference for classification on imbalanced data, and (2) classification performance can be significantly improved by the transductive inference performed on top of the inductively learned features. Our proposed Transduction Enhanced Inductive Inference (TEII) is novel, theoretically sound, and effective in training time and test time. Experiments on three public classification datasets show that our TEII delivers impressive results on data exhibiting moderate to severe class-imbalance behaviors.",
          "pdf_url": "",
          "doi": "10.1109/ISBI56570.2024.10635370",
          "url": "https://www.semanticscholar.org/paper/8165887e93ab1e468f8a96133a5ee64ab4fe4a4e",
          "source": "Semantic Scholar"
        },
        {
          "title": "Cluster-Guided Semi-Supervised Domain Adaptation for Imbalanced Medical Image Classification",
          "authors": [
            "S. Harada",
            "Ryoma Bise",
            "Kengo Araki",
            "A. Yoshizawa",
            "K. Terada",
            "Mariyo Rokutan-Kurata",
            "N. Nakajima",
            "Hiroyuki Abe",
            "T. Ushiku",
            "Seiichi Uchida"
          ],
          "year": 2023,
          "citations": 3,
          "abstract": "Semi-supervised domain adaptation is a technique to build a classifier for a target domain by modifying a classifier in another (source) domain using many unlabeled samples and a small number of labeled samples from the target domain. In this paper, we develop a semi-supervised domain adaptation method, which has robustness to class-imbalanced situations, which are common in medical image classification tasks. For robustness, we propose a weakly-supervised clustering pipeline to obtain high-purity clusters and utilize the clusters in representation learning for domain adaptation. The proposed method showed state-of-the-art performance in the experiment using severely class-imbalanced pathological image patches.",
          "pdf_url": "http://arxiv.org/pdf/2303.01283",
          "doi": "10.1109/ISBI53787.2023.10230451",
          "url": "https://www.semanticscholar.org/paper/77718040e94a7bb27d53091f1538917ebd0cf01e",
          "source": "Semantic Scholar"
        },
        {
          "title": "Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification",
          "authors": [
            "Marawan Elbatel",
            "Hualiang Wang",
            "Robert Mart'i",
            "H. Fu",
            "X. Li"
          ],
          "year": 2023,
          "citations": 9,
          "abstract": "In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \\url{https://github.com/xmed-lab/Fed-MAS}.",
          "pdf_url": "https://arxiv.org/pdf/2307.14959",
          "doi": "10.48550/arXiv.2307.14959",
          "url": "https://www.semanticscholar.org/paper/c0ff8596b3ab637257a8917406b91f59c1905318",
          "source": "Semantic Scholar"
        },
        {
          "title": "STCNN: Combining SMOTE-TOMEK With CNN for Imbalanced Classification of Alzheimer's Disease",
          "authors": [
            ".. Anjali",
            "Dipti Singh",
            "O. Pandey",
            "Hong-ning Dai"
          ],
          "year": 2024,
          "citations": 9,
          "abstract": "The most frequent cause of dementia worldwide is Alzheimer's disease (AD). Progressing from mild to severe, it gradually deteriorates, making independent tasks more challenging. Due to the aging population and the timing of diagnoses, its prevalence has exceeded expectations. Existing models for categorizing cases include magnetic resonance imaging (MRI), cognitive testing, and medical history. However, these methods lack precision and sensitivity and are not always effective. A framework for identifying particular features of AD from MRI images is developed using the convolutional neural network (CNN). To prevent the issue of class imbalance, the synthetic minority oversampling technique is used, which exists in the MRI image dataset from Kaggle. An STCNN model is proposed to predict the different dementia stages from MRI and achieves 99.36% and 99% accuracy and F1-score, respectively. We compared the proposed model with the benchmark models and discovered that the STCNN model outperformed the state-of-the-art models in terms of accuracy, efficiency, and performance.",
          "pdf_url": "",
          "doi": "10.1109/LSENS.2024.3357196",
          "url": "https://www.semanticscholar.org/paper/96cd07dfb51bace9aefecec69ea89688cea6908c",
          "source": "Semantic Scholar"
        },
        {
          "title": "Comparative analysis of supervised and self-supervised learning with small and imbalanced medical imaging datasets",
          "authors": [
            "Andrea Espis",
            "Chiara Marzi",
            "S. Diciotti"
          ],
          "year": 2025,
          "citations": 1,
          "abstract": "Self-supervised learning (SSL) in computer vision has shown its potential to reduce reliance on labeled data. However, most studies focused on balanced, large, broad-domain datasets like ImageNet, whereas, in real-world medical applications, dataset size is typically limited. This study compares the performance of SSL versus supervised learning (SL) on small, imbalanced medical imaging datasets. We experimented with four binary classification tasks: age prediction and diagnosis of Alzheimer’s disease from brain magnetic resonance imaging scans, pneumonia from chest radiograms, and retinal diseases associated with choroidal neovascularization from optical coherence tomography with a mean size of training sets of 843 images, 771 images, 1,214 images, and 33,484 images, respectively. We tested various combinations of label availability and class frequency distribution, repeating the training with different random seeds to assess result uncertainty. In most experiments involving small training sets, SL outperformed the selected SSL paradigms, even when a limited portion of labeled data was available. Our findings highlight the importance of carefully selecting learning paradigms based on specific application requirements, which are influenced by factors such as training set size, label availability, and class frequency distribution.",
          "pdf_url": "",
          "doi": "10.1038/s41598-025-99000-0",
          "url": "https://www.semanticscholar.org/paper/63b28619ea093fca5bc54183318138d880d73446",
          "source": "Semantic Scholar"
        },
        {
          "title": "Deep learning-based image classification for AI-assisted integration of pathology and radiology in medical imaging",
          "authors": [
            "Lanting He",
            "Lan Luan",
            "Dan Hu"
          ],
          "year": 2025,
          "citations": 2,
          "abstract": "Introduction The integration of pathology and radiology through artificial intelligence (AI) represents a groundbreaking advancement in medical imaging, providing a powerful tool for accurate diagnostics and the optimization of clinical workflows. Traditional image classification methods encounter substantial challenges due to the inherent complexity and heterogeneity of medical imaging datasets, which include multi-modal data sources, imbalanced class distributions, and the critical need for interpretability in clinical decision-making. Methods Addressing these limitations, this study introduces an innovative deep learning-based framework tailored for AI-assisted medical imaging tasks. It incorporates two novel components: the Adaptive Multi-Resolution Imaging Network (AMRI-Net) and the Explainable Domain-Adaptive Learning (EDAL) strategy. AMRI-Net enhances diagnostic accuracy by leveraging multi-resolution feature extraction, attention-guided fusion mechanisms, and task-specific decoders, allowing the model to accurately identify both detailed and overarching patterns across various imaging techniques, such as X-rays, CT, and MRI scans. EDAL significantly improves domain generalizability through advanced domain alignment techniques while integrating uncertainty-aware learning to prioritize high-confidence predictions. It employs attention-based interpretability tools to highlight critical image regions, improving transparency and clinical trust in AI-driven diagnoses. Results Experimental results on multi-modal medical imaging datasets underscore the framework's superior performance, with classification accuracies reaching up to 94.95% and F1-Scores up to 94.85%, thereby enhancing transparency and clinical trust in AI-driven diagnoses. Discussion This research bridges the gap between pathology and radiology, offering a comprehensive AI-driven solution that aligns with the evolving demands of modern healthcare by ensuring precision, reliability, and interpretability in medical imaging.",
          "pdf_url": "",
          "doi": "10.3389/fmed.2025.1574514",
          "url": "https://www.semanticscholar.org/paper/8597147dd3a184560ef7121869a2ea8bb9097c90",
          "source": "Semantic Scholar"
        },
        {
          "title": "Deep learning-based image classification for integrating pathology and radiology in AI-assisted medical imaging",
          "authors": [
            "Chenming Lu",
            "Jiayin Zhang",
            "Ren Liu"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "The integration of pathology and radiology in medical imaging has emerged as a critical need for advancing diagnostic accuracy and improving clinical workflows. Current AI-driven approaches for medical image analysis, despite significant progress, face several challenges, including handling multi-modal imaging, imbalanced datasets, and the lack of robust interpretability and uncertainty quantification. These limitations often hinder the deployment of AI systems in real-world clinical settings, where reliability and adaptability are essential. To address these issues, this study introduces a novel framework, the Domain-Informed Adaptive Network (DIANet), combined with an Adaptive Clinical Workflow Integration (ACWI) strategy. DIANet leverages multi-scale feature extraction, domain-specific priors, and Bayesian uncertainty modeling to enhance interpretability and robustness. The proposed model is tailored for multi-modal medical imaging tasks, integrating adaptive learning mechanisms to mitigate domain shifts and imbalanced datasets. Complementing the model, the ACWI strategy ensures seamless deployment through explainable AI (XAI) techniques, uncertainty-aware decision support, and modular workflow integration compatible with clinical systems like PACS. Experimental results demonstrate significant improvements in diagnostic accuracy, segmentation precision, and reconstruction fidelity across diverse imaging modalities, validating the potential of this framework to bridge the gap between AI innovation and clinical utility.",
          "pdf_url": "",
          "doi": "10.1038/s41598-025-07883-w",
          "url": "https://www.semanticscholar.org/paper/a6138a2a03f38bd4ad4366303d7a73773772923e",
          "source": "Semantic Scholar"
        },
        {
          "title": "Training Over a Distribution of Hyperparameters for Enhanced Performance and Adaptability on Imbalanced Classification",
          "authors": [
            "Kelsey Lieberman",
            "Swarna Kamlam Ravindran",
            "Shuai Yuan",
            "Carlo Tomasi"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Although binary classification is a well-studied problem, training reliable classifiers under severe class imbalance remains a challenge. Recent techniques mitigate the ill effects of imbalance on training by modifying the loss functions or optimization methods. We observe that different hyperparameter values on these loss functions perform better at different recall values. We propose to exploit this fact by training one model over a distribution of hyperparameter values--instead of a single value--via Loss Conditional Training (LCT). Experiments show that training over a distribution of hyperparameters not only approximates the performance of several models but actually improves the overall performance of models on both CIFAR and real medical imaging applications, such as melanoma and diabetic retinopathy detection. Furthermore, training models with LCT is more efficient because some hyperparameter tuning can be conducted after training to meet individual needs without needing to retrain from scratch.",
          "pdf_url": "",
          "doi": "10.48550/arXiv.2410.03588",
          "url": "https://www.semanticscholar.org/paper/67ccbb83e83fac13b50e21dee6a7ee2f11395577",
          "source": "Semantic Scholar"
        },
        {
          "title": "LMFLOSS: A Hybrid Loss For Imbalanced Medical Image Classification",
          "authors": [
            "Abu Adnan Sadi",
            "L. Chowdhury",
            "Nur-Wa-Bushra Jahan",
            "Mohammad Newaz Sharif Rafi",
            "Radeya Chowdhury",
            "Faisal Ahamed Khan",
            "Nabeel Mohammed"
          ],
          "year": 2022,
          "citations": 14,
          "abstract": "With advances in digital technology, the classification of medical images has become a crucial step for image-based clinical decision support systems. Automatic medical image classification represents a pivotal domain where the use of AI holds the potential to create a significant social impact. However, several challenges act as obstacles to the development of practical and effective solutions. One of these challenges is the prevalent class imbalance problem in most medical imaging datasets. As a result, existing AI techniques, particularly deep-learning-based methodologies, often underperform in such scenarios. In this study, we propose a novel framework called Large Margin aware Focal (LMF) loss to mitigate the class imbalance problem in medical imaging. The LMF loss represents a linear combination of two loss functions optimized by two hyperparameters. This framework harnesses the distinct characteristics of both loss functions by enforcing wider margins for minority classes while simultaneously emphasizing challenging samples found in the datasets. We perform rigorous experiments on three neural network architectures and with four medical imaging datasets. We provide empirical evidence that our proposed framework consistently outperforms other baseline methods, showing an improvement of 2%-9% in macro-f1 scores. Through class-wise analysis of f1 scores, we also demonstrate how the proposed framework can significantly improve performance for minority classes. The results of our experiments show that our proposed framework can perform consistently well across different architectures and datasets. Overall, our study demonstrates a simple and effective approach to addressing the class imbalance problem in medical imaging datasets. We hope our work will inspire new research toward a more generalized approach to medical image classification.",
          "pdf_url": "http://arxiv.org/pdf/2212.12741",
          "doi": "10.48550/arXiv.2212.12741",
          "url": "https://www.semanticscholar.org/paper/7e02db80f9e06df0e797e644ba565d582cb6f8e0",
          "source": "Semantic Scholar"
        },
        {
          "title": "Imbalanced Classification in Medical Imaging via Regrouping",
          "authors": [
            "Le Peng",
            "Yash Travadi",
            "Rui Zhang",
            "Ying Cui",
            "Ju Sun"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "We propose performing imbalanced classification by regrouping majority\nclasses into small classes so that we turn the problem into balanced multiclass\nclassification. This new idea is dramatically different from popular loss\nreweighting and class resampling methods. Our preliminary result on imbalanced\nmedical image classification shows that this natural idea can substantially\nboost the classification performance as measured by average precision\n(approximately area-under-the-precision-recall-curve, or AUPRC), which is more\nappropriate for evaluating imbalanced classification than other metrics such as\nbalanced accuracy.",
          "pdf_url": "http://arxiv.org/pdf/2210.12234v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2210.12234v2",
          "source": "arXiv"
        },
        {
          "title": "Medical Knowledge-Guided Deep Learning for Imbalanced Medical Image\n  Classification",
          "authors": [
            "Long Gao",
            "Chang Liu",
            "Dooman Arefan",
            "Ashok Panigrahy",
            "Margarita L. Zuley",
            "Shandong Wu"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Deep learning models have gained remarkable performance on a variety of image\nclassification tasks. However, many models suffer from limited performance in\nclinical or medical settings when data are imbalanced. To address this\nchallenge, we propose a medical-knowledge-guided one-class classification\napproach that leverages domain-specific knowledge of classification tasks to\nboost the model's performance. The rationale behind our approach is that some\nexisting prior medical knowledge can be incorporated into data-driven deep\nlearning to facilitate model learning. We design a deep learning-based\none-class classification pipeline for imbalanced image classification, and\ndemonstrate in three use cases how we take advantage of medical knowledge of\neach specific classification task by generating additional middle classes to\nachieve higher classification performances. We evaluate our approach on three\ndifferent clinical image classification tasks (a total of 8459 images) and show\nsuperior model performance when compared to six state-of-the-art methods. All\ncodes of this work will be publicly available upon acceptance of the paper.",
          "pdf_url": "http://arxiv.org/pdf/2111.10620v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2111.10620v2",
          "source": "arXiv"
        },
        {
          "title": "ProCo: Prototype-aware Contrastive Learning for Long-tailed Medical\n  Image Classification",
          "authors": [
            "Zhixiong Yang",
            "Junwen Pan",
            "Yanzhan Yang",
            "Xiaozhou Shi",
            "Hong-Yu Zhou",
            "Zhicheng Zhang",
            "Cheng Bian"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "Medical image classification has been widely adopted in medical image\nanalysis. However, due to the difficulty of collecting and labeling data in the\nmedical area, medical image datasets are usually highly-imbalanced. To address\nthis problem, previous works utilized class samples as prior for re-weighting\nor re-sampling but the feature representation is usually still not\ndiscriminative enough. In this paper, we adopt the contrastive learning to\ntackle the long-tailed medical imbalance problem. Specifically, we first\npropose the category prototype and adversarial proto-instance to generate\nrepresentative contrastive pairs. Then, the prototype recalibration strategy is\nproposed to address the highly imbalanced data distribution. Finally, a unified\nproto-loss is designed to train our framework. The overall framework, namely as\nPrototype-aware Contrastive learning (ProCo), is unified as a single-stage\npipeline in an end-to-end manner to alleviate the imbalanced problem in medical\nimage classification, which is also a distinct progress than existing works as\nthey follow the traditional two-stage pipeline. Extensive experiments on two\nhighly-imbalanced medical image classification datasets demonstrate that our\nmethod outperforms the existing state-of-the-art methods by a large margin.",
          "pdf_url": "http://arxiv.org/pdf/2209.00183v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2209.00183v1",
          "source": "arXiv"
        },
        {
          "title": "Self-Supervised Learning Featuring Small-Scale Image Dataset for\n  Treatable Retinal Diseases Classification",
          "authors": [
            "Luffina C. Huang",
            "Darren J. Chiu",
            "Manish Mehta"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Automated medical diagnosis through image-based neural networks has increased\nin popularity and matured over years. Nevertheless, it is confined by the\nscarcity of medical images and the expensive labor annotation costs.\nSelf-Supervised Learning (SSL) is an good alternative to Transfer Learning (TL)\nand is suitable for imbalanced image datasets. In this study, we assess four\npretrained SSL models and two TL models in treatable retinal diseases\nclassification using small-scale Optical Coherence Tomography (OCT) images\nranging from 125 to 4000 with balanced or imbalanced distribution for training.\nThe proposed SSL model achieves the state-of-art accuracy of 98.84% using only\n4,000 training images. Our results suggest the SSL models provide superior\nperformance under both the balanced and imbalanced training scenarios. The SSL\nmodel with MoCo-v2 scheme has consistent good performance under the imbalanced\nscenario and, especially, surpasses the other models when the training set is\nless than 500 images.",
          "pdf_url": "http://arxiv.org/pdf/2404.10166v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2404.10166v1",
          "source": "arXiv"
        },
        {
          "title": "Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised\n  Framework",
          "authors": [
            "Mosong Ma",
            "Tania Stathaki",
            "Michalis Lazarou"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Deep learning in medical imaging is often limited by scarce and imbalanced\nannotated data. We present SSGNet, a unified framework that combines class\nspecific generative modeling with iterative semisupervised pseudo labeling to\nenhance both classification and segmentation. Rather than functioning as a\nstandalone model, SSGNet augments existing baselines by expanding training data\nwith StyleGAN3 generated images and refining labels through iterative pseudo\nlabeling. Experiments across multiple medical imaging benchmarks demonstrate\nconsistent gains in classification and segmentation performance, while Frechet\nInception Distance analysis confirms the high quality of generated samples.\nThese results highlight SSGNet as a practical strategy to mitigate annotation\nbottlenecks and improve robustness in medical image analysis.",
          "pdf_url": "http://arxiv.org/pdf/2510.06123v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2510.06123v1",
          "source": "arXiv"
        },
        {
          "title": "Addressing Small and Imbalanced Medical Image Datasets Using Generative\n  Models: A Comparative Study of DDPM and PGGANs with Random and Greedy K\n  Sampling",
          "authors": [
            "Iman Khazrak",
            "Shakhnoza Takhirova",
            "Mostafa M. Rezaee",
            "Mehrdad Yadollahi",
            "Robert C. Green II",
            "Shuteng Niu"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "The development of accurate medical image classification models is often\nconstrained by privacy concerns and data scarcity for certain conditions,\nleading to small and imbalanced datasets. To address these limitations, this\nstudy explores the use of generative models, such as Denoising Diffusion\nProbabilistic Models (DDPM) and Progressive Growing Generative Adversarial\nNetworks (PGGANs), for dataset augmentation. The research introduces a\nframework to assess the impact of synthetic images generated by DDPM and PGGANs\non the performance of four models: a custom CNN, Untrained VGG16, Pretrained\nVGG16, and Pretrained ResNet50. Experiments were conducted using Random\nSampling and Greedy K Sampling to create small, imbalanced datasets. The\nsynthetic images were evaluated using Frechet Inception Distance (FID) and\ncompared to original datasets through classification metrics. The results show\nthat DDPM consistently generated more realistic images with lower FID scores\nand significantly outperformed PGGANs in improving classification metrics\nacross all models and datasets. Incorporating DDPM-generated images into the\noriginal datasets increased accuracy by up to 6%, enhancing model robustness\nand stability, particularly in imbalanced scenarios. Random Sampling\ndemonstrated superior stability, while Greedy K Sampling offered diversity at\nthe cost of higher FID scores. This study highlights the efficacy of DDPM in\naugmenting small, imbalanced medical image datasets, improving model\nperformance by balancing the dataset and expanding its size.",
          "pdf_url": "http://arxiv.org/pdf/2412.12532v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2412.12532v1",
          "source": "arXiv"
        },
        {
          "title": "Iterative Online Image Synthesis via Diffusion Model for Imbalanced\n  Classification",
          "authors": [
            "Shuhan Li",
            "Yi Lin",
            "Hao Chen",
            "Kwang-Ting Cheng"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Accurate and robust classification of diseases is important for proper\ndiagnosis and treatment. However, medical datasets often face challenges\nrelated to limited sample sizes and inherent imbalanced distributions, due to\ndifficulties in data collection and variations in disease prevalence across\ndifferent types. In this paper, we introduce an Iterative Online Image\nSynthesis (IOIS) framework to address the class imbalance problem in medical\nimage classification. Our framework incorporates two key modules, namely Online\nImage Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively\ntarget the imbalance classification issue at both the instance level and the\nclass level. The OIS module alleviates the data insufficiency problem by\ngenerating representative samples tailored for online training of the\nclassifier. On the other hand, the AAS module dynamically balances the\nsynthesized samples among various classes, targeting those with low training\naccuracy. To evaluate the effectiveness of our proposed method in addressing\nimbalanced classification, we conduct experiments on the HAM10000 and APTOS\ndatasets. The results obtained demonstrate the superiority of our approach over\nstate-of-the-art methods as well as the effectiveness of each component. The\nsource code will be released upon acceptance.",
          "pdf_url": "http://arxiv.org/pdf/2403.08407v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2403.08407v1",
          "source": "arXiv"
        },
        {
          "title": "Semi-supervised learning for medical image classification using\n  imbalanced training data",
          "authors": [
            "Tri Huynh",
            "Aiden Nibali",
            "Zhen He"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Medical image classification is often challenging for two reasons: a lack of\nlabelled examples due to expensive and time-consuming annotation protocols, and\nimbalanced class labels due to the relative scarcity of disease-positive\nindividuals in the wider population. Semi-supervised learning (SSL) methods\nexist for dealing with a lack of labels, but they generally do not address the\nproblem of class imbalance. In this study we propose Adaptive Blended\nConsistency Loss (ABCL), a drop-in replacement for consistency loss in\nperturbation-based SSL methods. ABCL counteracts data skew by adaptively mixing\nthe target class distribution of the consistency loss in accordance with class\nfrequency. Our experiments with ABCL reveal improvements to unweighted average\nrecall on two different imbalanced medical image classification datasets when\ncompared with existing consistency losses that are not designed to counteract\nclass imbalance.",
          "pdf_url": "http://arxiv.org/pdf/2108.08956v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2108.08956v1",
          "source": "arXiv"
        },
        {
          "title": "CLINICAL: Targeted Active Learning for Imbalanced Medical Image\n  Classification",
          "authors": [
            "Suraj Kothawade",
            "Atharv Savarkar",
            "Venkat Iyer",
            "Lakshman Tamil",
            "Ganesh Ramakrishnan",
            "Rishabh Iyer"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "Training deep learning models on medical datasets that perform well for all\nclasses is a challenging task. It is often the case that a suboptimal\nperformance is obtained on some classes due to the natural class imbalance\nissue that comes with medical data. An effective way to tackle this problem is\nby using targeted active learning, where we iteratively add data points to the\ntraining data that belong to the rare classes. However, existing active\nlearning methods are ineffective in targeting rare classes in medical datasets.\nIn this work, we propose Clinical (targeted aCtive Learning for ImbalaNced\nmedICal imAge cLassification) a framework that uses submodular mutual\ninformation functions as acquisition functions to mine critical data points\nfrom rare classes. We apply our framework to a wide-array of medical imaging\ndatasets on a variety of real-world class imbalance scenarios - namely, binary\nimbalance and long-tail imbalance. We show that Clinical outperforms the\nstate-of-the-art active learning methods by acquiring a diverse set of data\npoints that belong to the rare classes.",
          "pdf_url": "http://arxiv.org/pdf/2210.01520v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2210.01520v1",
          "source": "arXiv"
        },
        {
          "title": "Learning Discriminative Representation via Metric Learning for\n  Imbalanced Medical Image Classification",
          "authors": [
            "Chenghua Zeng",
            "Huijuan Lu",
            "Kanghao Chen",
            "Ruixuan Wang",
            "Wei-Shi Zheng"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "Data imbalance between common and rare diseases during model training often\ncauses intelligent diagnosis systems to have biased predictions towards common\ndiseases. The state-of-the-art approaches apply a two-stage learning framework\nto alleviate the class-imbalance issue, where the first stage focuses on\ntraining of a general feature extractor and the second stage focuses on\nfine-tuning the classifier head for class rebalancing. However, existing\ntwo-stage approaches do not consider the fine-grained property between\ndifferent diseases, often causing the first stage less effective for medical\nimage classification than for natural image classification tasks. In this\nstudy, we propose embedding metric learning into the first stage of the\ntwo-stage framework specially to help the feature extractor learn to extract\nmore discriminative feature representations. Extensive experiments mainly on\nthree medical image datasets show that the proposed approach consistently\noutperforms existing onestage and two-stage approaches, suggesting that metric\nlearning can be used as an effective plug-in component in the two-stage\nframework for fine-grained class-imbalanced image classification tasks.",
          "pdf_url": "http://arxiv.org/pdf/2207.06975v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2207.06975v1",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Attention mechanisms image classification",
      "final_query": "Attention mechanisms image classification",
      "attempts": 1,
      "paper_count": 10,
      "open_access_count": 10,
      "papers": [
        {
          "title": "A Novel Global Spatial Attention Mechanism in Convolutional Neural\n  Network for Medical Image Classification",
          "authors": [
            "Linchuan Xu",
            "Jun Huang",
            "Atsushi Nitanda",
            "Ryo Asaoka",
            "Kenji Yamanishi"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Spatial attention has been introduced to convolutional neural networks (CNNs)\nfor improving both their performance and interpretability in visual tasks\nincluding image classification. The essence of the spatial attention is to\nlearn a weight map which represents the relative importance of activations\nwithin the same layer or channel. All existing attention mechanisms are local\nattentions in the sense that weight maps are image-specific. However, in the\nmedical field, there are cases that all the images should share the same weight\nmap because the set of images record the same kind of symptom related to the\nsame object and thereby share the same structural content. In this paper, we\nthus propose a novel global spatial attention mechanism in CNNs mainly for\nmedical image classification. The global weight map is instantiated by a\ndecision boundary between important pixels and unimportant pixels. And we\npropose to realize the decision boundary by a binary classifier in which the\nintensities of all images at a pixel are the features of the pixel. The binary\nclassification is integrated into an image classification CNN and is to be\noptimized together with the CNN. Experiments on two medical image datasets and\none facial expression dataset showed that with the proposed attention, not only\nthe performance of four powerful CNNs which are GoogleNet, VGG, ResNet, and\nDenseNet can be improved, but also meaningful attended regions can be obtained,\nwhich is beneficial for understanding the content of images of a domain.",
          "pdf_url": "http://arxiv.org/pdf/2007.15897v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2007.15897v1",
          "source": "arXiv"
        },
        {
          "title": "Attention Mechanisms in Computer Vision: A Survey",
          "authors": [
            "Meng-Hao Guo",
            "Tian-Xing Xu",
            "Jiang-Jiang Liu",
            "Zheng-Ning Liu",
            "Peng-Tao Jiang",
            "Tai-Jiang Mu",
            "Song-Hai Zhang",
            "Ralph R. Martin",
            "Ming-Ming Cheng",
            "Shi-Min Hu"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Humans can naturally and effectively find salient regions in complex scenes.\nMotivated by this observation, attention mechanisms were introduced into\ncomputer vision with the aim of imitating this aspect of the human visual\nsystem. Such an attention mechanism can be regarded as a dynamic weight\nadjustment process based on features of the input image. Attention mechanisms\nhave achieved great success in many visual tasks, including image\nclassification, object detection, semantic segmentation, video understanding,\nimage generation, 3D vision, multi-modal tasks and self-supervised learning. In\nthis survey, we provide a comprehensive review of various attention mechanisms\nin computer vision and categorize them according to approach, such as channel\nattention, spatial attention, temporal attention and branch attention; a\nrelated repository https://github.com/MenghaoGuo/Awesome-Vision-Attentions is\ndedicated to collecting related work. We also suggest future directions for\nattention mechanism research.",
          "pdf_url": "http://arxiv.org/pdf/2111.07624v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2111.07624v1",
          "source": "arXiv"
        },
        {
          "title": "Parameter-Free Channel Attention for Image Classification and\n  Super-Resolution",
          "authors": [
            "Yuxuan Shi",
            "Lingxiao Yang",
            "Wangpeng An",
            "Xiantong Zhen",
            "Liuqing Wang"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "The channel attention mechanism is a useful technique widely employed in deep\nconvolutional neural networks to boost the performance for image processing\ntasks, eg, image classification and image super-resolution. It is usually\ndesigned as a parameterized sub-network and embedded into the convolutional\nlayers of the network to learn more powerful feature representations. However,\ncurrent channel attention induces more parameters and therefore leads to higher\ncomputational costs. To deal with this issue, in this work, we propose a\nParameter-Free Channel Attention (PFCA) module to boost the performance of\npopular image classification and image super-resolution networks, but\ncompletely sweep out the parameter growth of channel attention. Experiments on\nCIFAR-100, ImageNet, and DIV2K validate that our PFCA module improves the\nperformance of ResNet on image classification and improves the performance of\nMSRResNet on image super-resolution tasks, respectively, while bringing little\ngrowth of parameters and FLOPs.",
          "pdf_url": "http://arxiv.org/pdf/2303.11055v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2303.11055v1",
          "source": "arXiv"
        },
        {
          "title": "Attention Branch Network: Learning of Attention Mechanism for Visual\n  Explanation",
          "authors": [
            "Hiroshi Fukui",
            "Tsubasa Hirakawa",
            "Takayoshi Yamashita",
            "Hironobu Fujiyoshi"
          ],
          "year": 2018,
          "citations": 0,
          "abstract": "Visual explanation enables human to understand the decision making of Deep\nConvolutional Neural Network (CNN), but it is insufficient to contribute the\nperformance improvement. In this paper, we focus on the attention map for\nvisual explanation, which represents high response value as the important\nregion in image recognition. This region significantly improves the performance\nof CNN by introducing an attention mechanism that focuses on a specific region\nin an image. In this work, we propose Attention Branch Network (ABN), which\nextends the top-down visual explanation model by introducing a branch structure\nwith an attention mechanism. ABN can be applicable to several image recognition\ntasks by introducing a branch for attention mechanism and is trainable for the\nvisual explanation and image recognition in end-to-end manner. We evaluate ABN\non several image recognition tasks such as image classification, fine-grained\nrecognition, and multiple facial attributes recognition. Experimental results\nshow that ABN can outperform the accuracy of baseline models on these image\nrecognition tasks while generating an attention map for visual explanation. Our\ncode is available at\nhttps://github.com/machine-perception-robotics-group/attention_branch_network.",
          "pdf_url": "http://arxiv.org/pdf/1812.10025v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1812.10025v2",
          "source": "arXiv"
        },
        {
          "title": "Spatial--spectral FFPNet: Attention-Based Pyramid Network for\n  Segmentation and Classification of Remote Sensing Images",
          "authors": [
            "Qingsong Xu",
            "Xin Yuan",
            "Chaojun Ouyang",
            "Yue Zeng"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "We consider the problem of segmentation and classification of high-resolution\nand hyperspectral remote sensing images. Unlike conventional natural (RGB)\nimages, the inherent large scale and complex structures of remote sensing\nimages pose major challenges such as spatial object distribution diversity and\nspectral information extraction when existing models are directly applied for\nimage classification. In this study, we develop an attention-based pyramid\nnetwork for segmentation and classification of remote sensing datasets.\nAttention mechanisms are used to develop the following modules: i) a novel and\nrobust attention-based multi-scale fusion method effectively fuses useful\nspatial or spectral information at different and same scales; ii) a region\npyramid attention mechanism using region-based attention addresses the target\ngeometric size diversity in large-scale remote sensing images; and iii\ncross-scale attention} in our adaptive atrous spatial pyramid pooling network\nadapts to varied contents in a feature-embedded space. Different forms of\nfeature fusion pyramid frameworks are established by combining these\nattention-based modules. First, a novel segmentation framework, called the\nheavy-weight spatial feature fusion pyramid network (FFPNet), is proposed to\naddress the spatial problem of high-resolution remote sensing images. Second,\nan end-to-end spatial--spectral FFPNet is presented for classifying\nhyperspectral images. Experiments conducted on ISPRS Vaihingen and ISPRS\nPotsdam high-resolution datasets demonstrate the competitive segmentation\naccuracy achieved by the proposed heavy-weight spatial FFPNet. Furthermore,\nexperiments on the Indian Pines and the University of Pavia hyperspectral\ndatasets indicate that the proposed spatial--spectral FFPNet outperforms the\ncurrent state-of-the-art methods in hyperspectral image classification.",
          "pdf_url": "http://arxiv.org/pdf/2008.08775v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2008.08775v1",
          "source": "arXiv"
        },
        {
          "title": "Attention-based Image Upsampling",
          "authors": [
            "Souvik Kundu",
            "Hesham Mostafa",
            "Sharath Nittur Sridhar",
            "Sairam Sundaresan"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Convolutional layers are an integral part of many deep neural network\nsolutions in computer vision. Recent work shows that replacing the standard\nconvolution operation with mechanisms based on self-attention leads to improved\nperformance on image classification and object detection tasks. In this work,\nwe show how attention mechanisms can be used to replace another canonical\noperation: strided transposed convolution. We term our novel attention-based\noperation attention-based upsampling since it increases/upsamples the spatial\ndimensions of the feature maps. Through experiments on single image\nsuper-resolution and joint-image upsampling tasks, we show that attention-based\nupsampling consistently outperforms traditional upsampling methods based on\nstrided transposed convolution or based on adaptive filters while using fewer\nparameters. We show that the inherent flexibility of the attention mechanism,\nwhich allows it to use separate sources for calculating the attention\ncoefficients and the attention targets, makes attention-based upsampling a\nnatural choice when fusing information from multiple image modalities.",
          "pdf_url": "http://arxiv.org/pdf/2012.09904v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2012.09904v1",
          "source": "arXiv"
        },
        {
          "title": "Neural Attention Models for Sequence Classification: Analysis and\n  Application to Key Term Extraction and Dialogue Act Detection",
          "authors": [
            "Sheng-syun Shen",
            "Hung-yi Lee"
          ],
          "year": 2016,
          "citations": 0,
          "abstract": "Recurrent neural network architectures combining with attention mechanism, or\nneural attention model, have shown promising performance recently for the tasks\nincluding speech recognition, image caption generation, visual question\nanswering and machine translation. In this paper, neural attention model is\napplied on two sequence classification tasks, dialogue act detection and key\nterm extraction. In the sequence labeling tasks, the model input is a sequence,\nand the output is the label of the input sequence. The major difficulty of\nsequence labeling is that when the input sequence is long, it can include many\nnoisy or irrelevant part. If the information in the whole sequence is treated\nequally, the noisy or irrelevant part may degrade the classification\nperformance. The attention mechanism is helpful for sequence classification\ntask because it is capable of highlighting important part among the entire\nsequence for the classification task. The experimental results show that with\nthe attention mechanism, discernible improvements were achieved in the sequence\nlabeling task considered here. The roles of the attention mechanism in the\ntasks are further analyzed and visualized in this paper.",
          "pdf_url": "http://arxiv.org/pdf/1604.00077v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1604.00077v1",
          "source": "arXiv"
        },
        {
          "title": "Brain Tumor Classification using Vision Transformer with Selective\n  Cross-Attention Mechanism and Feature Calibration",
          "authors": [
            "Mohammad Ali Labbaf Khaniki",
            "Marzieh Mirzaeibonehkhater",
            "Mohammad Manthouri",
            "Elham Hasani"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Brain tumor classification is a challenging task in medical image analysis.\nIn this paper, we propose a novel approach to brain tumor classification using\na vision transformer with a novel cross-attention mechanism. Our approach\nleverages the strengths of transformers in modeling long-range dependencies and\nmulti-scale feature fusion. We introduce two new mechanisms to improve the\nperformance of the cross-attention fusion module: Feature Calibration Mechanism\n(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from\ndifferent branches to make them more compatible, while SCA selectively attends\nto the most informative features. Our experiments demonstrate that the proposed\napproach outperforms other state-of-the-art methods in brain tumor\nclassification, achieving improved accuracy and efficiency. The proposed FCM\nand SCA mechanisms can be easily integrated into other vision transformer\narchitectures, making them a promising direction for future research in medical\nimage analysis. Experimental results confirm that our approach surpasses\nexisting methods, achieving state-of-the-art performance in brain tumor\nclassification tasks.",
          "pdf_url": "http://arxiv.org/pdf/2406.17670v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2406.17670v2",
          "source": "arXiv"
        },
        {
          "title": "Impact of Attention on Adversarial Robustness of Image Classification\n  Models",
          "authors": [
            "Prachi Agrawal",
            "Narinder Singh Punn",
            "Sanjay Kumar Sonbhadra",
            "Sonali Agarwal"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Adversarial attacks against deep learning models have gained significant\nattention and recent works have proposed explanations for the existence of\nadversarial examples and techniques to defend the models against these attacks.\nAttention in computer vision has been used to incorporate focused learning of\nimportant features and has led to improved accuracy. Recently, models with\nattention mechanisms have been proposed to enhance adversarial robustness.\nFollowing this context, this work aims at a general understanding of the impact\nof attention on adversarial robustness. This work presents a comparative study\nof adversarial robustness of non-attention and attention based image\nclassification models trained on CIFAR-10, CIFAR-100 and Fashion MNIST datasets\nunder the popular white box and black box attacks. The experimental results\nshow that the robustness of attention based models may be dependent on the\ndatasets used i.e. the number of classes involved in the classification. In\ncontrast to the datasets with less number of classes, attention based models\nare observed to show better robustness towards classification.",
          "pdf_url": "http://arxiv.org/pdf/2109.00936v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2109.00936v1",
          "source": "arXiv"
        },
        {
          "title": "Beyond Self-attention: External Attention using Two Linear Layers for\n  Visual Tasks",
          "authors": [
            "Meng-Hao Guo",
            "Zheng-Ning Liu",
            "Tai-Jiang Mu",
            "Shi-Min Hu"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Attention mechanisms, especially self-attention, have played an increasingly\nimportant role in deep feature representation for visual tasks. Self-attention\nupdates the feature at each position by computing a weighted sum of features\nusing pair-wise affinities across all positions to capture the long-range\ndependency within a single sample. However, self-attention has quadratic\ncomplexity and ignores potential correlation between different samples. This\npaper proposes a novel attention mechanism which we call external attention,\nbased on two external, small, learnable, shared memories, which can be\nimplemented easily by simply using two cascaded linear layers and two\nnormalization layers; it conveniently replaces self-attention in existing\npopular architectures. External attention has linear complexity and implicitly\nconsiders the correlations between all data samples. We further incorporate the\nmulti-head mechanism into external attention to provide an all-MLP\narchitecture, external attention MLP (EAMLP), for image classification.\nExtensive experiments on image classification, object detection, semantic\nsegmentation, instance segmentation, image generation, and point cloud analysis\nreveal that our method provides results comparable or superior to the\nself-attention mechanism and some of its variants, with much lower\ncomputational and memory costs.",
          "pdf_url": "http://arxiv.org/pdf/2105.02358v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2105.02358v2",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Real-time medical image processing",
      "final_query": "Real-time medical image processing",
      "attempts": 1,
      "paper_count": 20,
      "open_access_count": 11,
      "papers": [
        {
          "title": "Real-Time Medical Image Processing",
          "authors": [
            "A. Rosenfeld",
            "K. Preston",
            "M. Onoe"
          ],
          "year": 1980,
          "citations": 18,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1007/978-1-4757-0121-0",
          "url": "https://www.semanticscholar.org/paper/d4a97d8f459322a0d7cbfe01fb93e01022f008a4",
          "source": "Semantic Scholar"
        },
        {
          "title": "Real-time medical image processing",
          "authors": [
            "E. Feleppa"
          ],
          "year": 1985,
          "citations": 3,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1109/PROC.1985.13212",
          "url": "https://www.semanticscholar.org/paper/03250965b81126ecc71bfb4057ba0409fb02f077",
          "source": "Semantic Scholar"
        },
        {
          "title": "Real-Time Medical Image Processing",
          "authors": [
            "B. Baxter"
          ],
          "year": 1981,
          "citations": 0,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1148/RADIOLOGY.140.2.340",
          "url": "https://www.semanticscholar.org/paper/1da437ef5db650ad220893879f36ae46f5b53e10",
          "source": "Semantic Scholar"
        },
        {
          "title": "5G and Cloud Computing-enabled Real-time Medical Image Processing and Point-of-Care Telemedicine",
          "authors": [],
          "year": 2024,
          "citations": 0,
          "abstract": "This study explored the integration of 5G and cloud computing for real-time medical image processing and point-of-care telemedicine. The mobile broadband network with high speed and ultra-low-latency, coupled with the scalable and powerful computational power of cloud computing, can be leveraged to create a new framework for the improved accuracy and efficiency of medical imaging, especially in point-of-care telemedicine. The system has been tested for various clinical scenarios including remote ultrasounds and tele-collaboration for the surgeries. The results showed that the proposed method reduced the time needed for image processing by 40% and improved the accuracy of the diagnosis by 35%. Patient care efficiency was also improved significantly. This approach is compatible with the fast-growing mobile health and will facilitate the swift expansion of telemedical care especially in rural areas.",
          "pdf_url": "",
          "doi": "10.54228/mjaret0624011",
          "url": "https://www.semanticscholar.org/paper/c5df91e44f91026765e6f62d14752ecd1c8de7fd",
          "source": "Semantic Scholar"
        },
        {
          "title": "Weak redundancy U-shaped network and heatmap-based object prompt method for real-time medical image processing",
          "authors": [
            "Chenzhuo Lu",
            "Zhuang Fu",
            "Ziwen Guo",
            "Jian Fei",
            "Rongli Xie"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Background In recent years, artificial intelligence (AI) technology has experienced significant growth, leading to the development of advanced tools that assist radiologists in image interpretation and diagnostic decision-making. In the field of medical image processing, object detection and segmentation are crucial research areas. Achieving automatic and rapid segmentation of organs and lesions can significantly enhance physicians’ work efficiency. However, most existing networks feature complex architectures and entail high computational complexity. Due to the limited processing power of the devices, achieving real-time segmentation often proves challenging. Therefore, the aim of this study was to design a lightweight segmentation algorithm that reduces resource consumption and enhances real-time performance. Methods Firstly, we propose a compact U-shaped network called the weak redundancy U-Net (WRU-Net) specifically designed for real-time segmentation tasks in medical imaging. By reducing the number of channels, feature redundancy across all scales is reduced, compelling the network to utilize resources efficiently at every level. Furthermore, we propose “auxiliary information flows” to facilitate the propagation of phased results, thereby enhancing the decoder. Secondly, this paper introduces a novel visual prompt mode that differs from the standard object detection mode. We refer to this as “object prompt”, which means visualizing the position of the target object in an image to guide the viewer. Unlike standard object detection tasks that provide bounding boxes, this paper achieves the aforementioned effect in the form of heatmaps. Correspondingly, we propose a network for heatmap prediction, which further simplifies task complexity and achieves semantic detection of the target object. Results We conducted experiments using the datasets of the thyroid nodule, chest, placental vessel, brain tumors, heart, liver, and spleen, which include medical images of X-ray, computed tomography (CT), and ultrasound. Furthermore, we conducted comparative experiments using several state-of-the-art networks. The segmentation performance of the networks was evaluated using metrics such as the Dice similarity coefficient (DSC), intersection over union (IoU), precision, and recall. The average DSC, IoU, precision, and recall of our model across each dataset were 88.53%, 82.58%, 86.34%, and 84.85%, respectively. Regarding efficiency, our WRU-Net achieved 130.67 KB model size and 488 frames per second (FPS), outperforming larger models. For heatmap prediction, our network exhibited a similarly efficient profile with a parameter size of only 24.75 KB and a speed of 9,862.13 FPS on graphics processing unit (GPU) and 107.93 FPS on central processing unit (CPU). Conclusions In scenarios where the calculation ability of the equipment is constrained or real-time performance requirements are stringent, the design of lightweight networks serves as a fundamental approach to achieving high-speed data stream processing functionality. For instance, this applies to embedded devices and real-time ultrasonic scanning applications. The method proposed in this paper significantly reduces the number of parameters, thereby decreasing computational resource consumption and enhancing speed. This improvement holds substantial significance for optimizing computational efficiency in practical applications.",
          "pdf_url": "",
          "doi": "10.21037/qims-2025-567",
          "url": "https://www.semanticscholar.org/paper/e2b3acf50730a22b59e628043690bd1a1baf15bb",
          "source": "Semantic Scholar"
        },
        {
          "title": "MedBiSeNet: Efficient Bilateral Segmentation Network for Real-time Medical Image Processing",
          "authors": [
            "Jiwon Kim",
            "Kyungkeon Chung",
            "Ghangmin Yun",
            "Jaekyung Lee",
            "Jueun Jung",
            "Bokyoung Seo",
            "Hyejin Lee",
            "K. Lee"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1109/ISCAS56072.2025.11043841",
          "url": "https://www.semanticscholar.org/paper/67a9e0a05893b183f46e1161a0f4f34cffd94d09",
          "source": "Semantic Scholar"
        },
        {
          "title": "Grid enabled magnetic resonance scanners for near real-time medical image processing",
          "authors": [
            "J. Crane",
            "Forrest W. Crawford",
            "S. Nelson"
          ],
          "year": 2006,
          "citations": 12,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1016/j.jpdc.2006.03.009",
          "url": "https://www.semanticscholar.org/paper/2653041e636a54a521a9814a4b6711d79b0ba98f",
          "source": "Semantic Scholar"
        },
        {
          "title": "FPGA Based Real Time Medical Image Processing",
          "authors": [
            "Akshay S. Janmane",
            "Apeksha S. Patil",
            "Madhuri V. Huilgol"
          ],
          "year": 2016,
          "citations": 0,
          "abstract": null,
          "pdf_url": "https://doi.org/10.9756/bijrce.8214",
          "doi": "10.9756/BIJRCE.8214",
          "url": "https://www.semanticscholar.org/paper/342508a2fb63fb9bb032d8aeb77a3c014a8e45c4",
          "source": "Semantic Scholar"
        },
        {
          "title": "Study on the Real Time Medical Image Processing",
          "authors": [
            "S. Yoo",
            "Gun-Ki Lee",
            "Nam-Chill Paik",
            "Won K. Kim"
          ],
          "year": 1987,
          "citations": 0,
          "abstract": null,
          "pdf_url": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/ab9cddfd3b1556ad6614c99d22ba5c49c8ee9fed",
          "source": "Semantic Scholar"
        },
        {
          "title": "Real‐Time Medical Image Processing edited by M. Onoe, K. Preston, and A. Rosenfeld",
          "authors": [
            "P. Joseph"
          ],
          "year": 1982,
          "citations": 0,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1118/1.595213",
          "url": "https://www.semanticscholar.org/paper/95db49928f62e9f4c301846b0940719ec0bfe3e8",
          "source": "Semantic Scholar"
        },
        {
          "title": "High-Resolution Photorealistic Image Translation in Real-Time: A\n  Laplacian Pyramid Translation Network",
          "authors": [
            "Jie Liang",
            "Hui Zeng",
            "Lei Zhang"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Existing image-to-image translation (I2IT) methods are either constrained to\nlow-resolution images or long inference time due to their heavy computational\nburden on the convolution of high-resolution feature maps. In this paper, we\nfocus on speeding-up the high-resolution photorealistic I2IT tasks based on\nclosed-form Laplacian pyramid decomposition and reconstruction. Specifically,\nwe reveal that the attribute transformations, such as illumination and color\nmanipulation, relate more to the low-frequency component, while the content\ndetails can be adaptively refined on high-frequency components. We consequently\npropose a Laplacian Pyramid Translation Network (LPTN) to simultaneously\nperform these two tasks, where we design a lightweight network for translating\nthe low-frequency component with reduced resolution and a progressive masking\nstrategy to efficiently refine the high-frequency ones. Our model avoids most\nof the heavy computation consumed by processing high-resolution feature maps\nand faithfully preserves the image details. Extensive experimental results on\nvarious tasks demonstrate that the proposed method can translate 4K images in\nreal-time using one normal GPU while achieving comparable transformation\nperformance against existing methods. Datasets and codes are available:\nhttps://github.com/csjliang/LPTN.",
          "pdf_url": "http://arxiv.org/pdf/2105.09188v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2105.09188v1",
          "source": "arXiv"
        },
        {
          "title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid\n  Network",
          "authors": [
            "Feng Zhang",
            "Haoyou Deng",
            "Zhiqiang Li",
            "Lida Li",
            "Bin Xu",
            "Qingbo Lu",
            "Zisheng Cao",
            "Minchen Wei",
            "Changxin Gao",
            "Nong Sang",
            "Xiang Bai"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Photo enhancement plays a crucial role in augmenting the visual aesthetics of\na photograph. In recent years, photo enhancement methods have either focused on\nenhancement performance, producing powerful models that cannot be deployed on\nedge devices, or prioritized computational efficiency, resulting in inadequate\nperformance for real-world applications. To this end, this paper introduces a\npyramid network called LLF-LUT++, which integrates global and local operators\nthrough closed-form Laplacian pyramid decomposition and reconstruction. This\napproach enables fast processing of high-resolution images while also achieving\nexcellent performance. Specifically, we utilize an image-adaptive 3D LUT that\ncapitalizes on the global tonal characteristics of downsampled images, while\nincorporating two distinct weight fusion strategies to achieve coarse global\nimage enhancement. To implement this strategy, we designed a spatial-frequency\ntransformer weight predictor that effectively extracts the desired distinct\nweights by leveraging frequency features. Additionally, we apply local\nLaplacian filters to adaptively refine edge details in high-frequency\ncomponents. After meticulously redesigning the network structure and\ntransformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on\nthe HDR+ dataset, but also further reduces runtime, with 4K resolution images\nprocessed in just 13 ms on a single GPU. Extensive experimental results on two\nbenchmark datasets further show that the proposed approach performs favorably\ncompared to state-of-the-art methods. The source code will be made publicly\navailable at https://github.com/fengzhang427/LLF-LUT.",
          "pdf_url": "http://arxiv.org/pdf/2510.11613v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2510.11613v1",
          "source": "arXiv"
        },
        {
          "title": "Medical Image Fusion: A survey of the state of the art",
          "authors": [
            "A. P. James",
            "B. V. Dasarathy"
          ],
          "year": 2013,
          "citations": 0,
          "abstract": "Medical image fusion is the process of registering and combining multiple\nimages from single or multiple imaging modalities to improve the imaging\nquality and reduce randomness and redundancy in order to increase the clinical\napplicability of medical images for diagnosis and assessment of medical\nproblems. Multi-modal medical image fusion algorithms and devices have shown\nnotable achievements in improving clinical accuracy of decisions based on\nmedical images. This review article provides a factual listing of methods and\nsummarizes the broad scientific challenges faced in the field of medical image\nfusion. We characterize the medical image fusion research based on (1) the\nwidely used image fusion methods, (2) imaging modalities, and (3) imaging of\norgans that are under study. This review concludes that even though there\nexists several open ended technological and scientific challenges, the fusion\nof medical images has proved to be useful for advancing the clinical\nreliability of using medical imaging for medical diagnostics and analysis, and\nis a scientific discipline that has the potential to significantly grow in the\ncoming years.",
          "pdf_url": "http://arxiv.org/pdf/1401.0166v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1401.0166v1",
          "source": "arXiv"
        },
        {
          "title": "Reduce Noise in Computed Tomography Image using Adaptive Gaussian Filter",
          "authors": [
            "Rini Mayasari",
            "Nono Heryana"
          ],
          "year": 2019,
          "citations": 0,
          "abstract": "One image processing application that is very helpful for humans is to\nimprove image quality, poor image quality makes the image more difficult to\ninterpret because the information conveyed by the image is reduced. In the\nprocess of the acquisition of medical images, the resulting image has decreased\nquality (degraded) due to external factors and medical equipment used. For this\nreason, it is necessary to have an image processing process to improve the\nquality of medical images, so that later it is expected to help facilitate\nmedical personnel in analyzing and translating medical images, which will lead\nto an improvement in the quality of diagnosis. In this study, an analysis will\nbe carried out to improve the quality of medical images with noise reduction\nwith the Gaussian Filter Method. Next, it is carried out, and tested against\nmedical images, in this case, the lung photo image. The test image is given\nnoise in the form of impulse salt & pepper and adaptive Gaussian then analyzed\nits performance qualitatively by comparing the output filter image, noise\nimage, and the original image by naked eye.",
          "pdf_url": "http://arxiv.org/pdf/1902.05985v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1902.05985v1",
          "source": "arXiv"
        },
        {
          "title": "Reference Based Color Transfer for Medical Volume Rendering",
          "authors": [
            "Sudarshan Devkota",
            "Summanta Pattanaik"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "The benefits of medical imaging are enormous. Medical images provide\nconsiderable amounts of anatomical information and this facilitates medical\npractitioners in performing effective disease diagnosis and deciding upon the\nbest course of medical treatment. A transition from traditional monochromatic\nmedical images like CT scans, X-Rays or MRI images to a colored 3D\nrepresentation of the anatomical structure further enhances the capabilities of\nmedical professionals in extracting valuable medical information. The proposed\nframework in our research starts with performing color transfer by finding deep\nsemantic correspondence between two medical images: a colored reference image,\nand a monochromatic CT scan or an MRI image. We extend this idea of\nreference-based colorization technique to perform colored volume rendering from\na stack of grayscale medical images. Furthermore, we also propose to use an\neffective reference image recommendation system to aid in the selection of good\nreference images. With our approach, we successfully perform colored medical\nvolume visualization and essentially eliminate the painstaking process of user\ninteraction with a transfer function to obtain color and opacity parameters for\nvolume rendering.",
          "pdf_url": "http://arxiv.org/pdf/2210.08083v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2210.08083v1",
          "source": "arXiv"
        },
        {
          "title": "Enhancing Medical Imaging with GANs Synthesizing Realistic Images from\n  Limited Data",
          "authors": [
            "Yinqiu Feng",
            "Bo Zhang",
            "Lingxi Xiao",
            "Yutian Yang",
            "Tana Gegen",
            "Zexi Chen"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "In this research, we introduce an innovative method for synthesizing medical\nimages using generative adversarial networks (GANs). Our proposed GANs method\ndemonstrates the capability to produce realistic synthetic images even when\ntrained on a limited quantity of real medical image data, showcasing\ncommendable generalization prowess. To achieve this, we devised a generator and\ndiscriminator network architecture founded on deep convolutional neural\nnetworks (CNNs), leveraging the adversarial training paradigm for model\noptimization. Through extensive experimentation across diverse medical image\ndatasets, our method exhibits robust performance, consistently generating\nsynthetic images that closely emulate the structural and textural attributes of\nauthentic medical images.",
          "pdf_url": "http://arxiv.org/pdf/2406.18547v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2406.18547v1",
          "source": "arXiv"
        },
        {
          "title": "Knowledge AI: New Medical AI Solution for Medical image Diagnosis",
          "authors": [
            "Yingni Wang",
            "Shuge Lei",
            "Jian Dai",
            "Kehong Yuan"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "The implementation of medical AI has always been a problem. The effect of\ntraditional perceptual AI algorithm in medical image processing needs to be\nimproved. Here we propose a method of knowledge AI, which is a combination of\nperceptual AI and clinical knowledge and experience. Based on this method, the\ngeometric information mining of medical images can represent the experience and\ninformation and evaluate the quality of medical images.",
          "pdf_url": "http://arxiv.org/pdf/2101.03063v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2101.03063v1",
          "source": "arXiv"
        },
        {
          "title": "MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided\n  Diffusion with Visual Invariant",
          "authors": [
            "Chenlu Zhan",
            "Yu Lin",
            "Gaoang Wang",
            "Hongwei Wang",
            "Jian Wu"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Medical generative models, acknowledged for their high-quality sample\ngeneration ability, have accelerated the fast growth of medical applications.\nHowever, recent works concentrate on separate medical generation models for\ndistinct medical tasks and are restricted to inadequate medical multi-modal\nknowledge, constraining medical comprehensive diagnosis. In this paper, we\npropose MedM2G, a Medical Multi-Modal Generative framework, with the key\ninnovation to align, extract, and generate medical multi-modal within a unified\nmodel. Extending beyond single or two medical modalities, we efficiently align\nmedical multi-modal through the central alignment approach in the unified\nspace. Significantly, our framework extracts valuable clinical knowledge by\npreserving the medical visual invariant of each imaging modal, thereby\nenhancing specific medical information for multi-modal generation. By\nconditioning the adaptive cross-guided parameters into the multi-flow diffusion\nframework, our model promotes flexible interactions among medical multi-modal\nfor generation. MedM2G is the first medical generative model that unifies\nmedical generation tasks of text-to-image, image-to-text, and unified\ngeneration of medical modalities (CT, MRI, X-ray). It performs 5 medical\ngeneration tasks across 10 datasets, consistently outperforming various\nstate-of-the-art works.",
          "pdf_url": "http://arxiv.org/pdf/2403.04290v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2403.04290v1",
          "source": "arXiv"
        },
        {
          "title": "MedMAE: A Self-Supervised Backbone for Medical Imaging Tasks",
          "authors": [
            "Anubhav Gupta",
            "Islam Osman",
            "Mohamed S. Shehata",
            "John W. Braun"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Medical imaging tasks are very challenging due to the lack of publicly\navailable labeled datasets. Hence, it is difficult to achieve high performance\nwith existing deep-learning models as they require a massive labeled dataset to\nbe trained effectively. An alternative solution is to use pre-trained models\nand fine-tune them using the medical imaging dataset. However, all existing\nmodels are pre-trained using natural images, which is a completely different\ndomain from that of medical imaging, which leads to poor performance due to\ndomain shift. To overcome these problems, we propose a large-scale unlabeled\ndataset of medical images and a backbone pre-trained using the proposed dataset\nwith a self-supervised learning technique called Masked autoencoder. This\nbackbone can be used as a pre-trained model for any medical imaging task, as it\nis trained to learn a visual representation of different types of medical\nimages. To evaluate the performance of the proposed backbone, we used four\ndifferent medical imaging tasks. The results are compared with existing\npre-trained models. These experiments show the superiority of our proposed\nbackbone in medical imaging tasks.",
          "pdf_url": "http://arxiv.org/pdf/2407.14784v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2407.14784v1",
          "source": "arXiv"
        },
        {
          "title": "Medical Image Generation using Generative Adversarial Networks",
          "authors": [
            "Nripendra Kumar Singh",
            "Khalid Raza"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Generative adversarial networks (GANs) are unsupervised Deep Learning\napproach in the computer vision community which has gained significant\nattention from the last few years in identifying the internal structure of\nmultimodal medical imaging data. The adversarial network simultaneously\ngenerates realistic medical images and corresponding annotations, which proven\nto be useful in many cases such as image augmentation, image registration,\nmedical image generation, image reconstruction, and image-to-image translation.\nThese properties bring the attention of the researcher in the field of medical\nimage analysis and we are witness of rapid adaption in many novel and\ntraditional applications. This chapter provides state-of-the-art progress in\nGANs-based clinical application in medical image generation, and cross-modality\nsynthesis. The various framework of GANs which gained popularity in the\ninterpretation of medical images, such as Deep Convolutional GAN (DCGAN),\nLaplacian GAN (LAPGAN), pix2pix, CycleGAN, and unsupervised image-to-image\ntranslation model (UNIT), continue to improve their performance by\nincorporating additional hybrid architecture, has been discussed. Further, some\nof the recent applications of these frameworks for image reconstruction, and\nsynthesis, and future research directions in the area have been covered.",
          "pdf_url": "http://arxiv.org/pdf/2005.10687v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2005.10687v1",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "FDA AI medical imaging regulation",
      "final_query": "FDA AI medical imaging regulation",
      "attempts": 1,
      "paper_count": 10,
      "open_access_count": 10,
      "papers": [
        {
          "title": "Regulating AI Adaptation: An Analysis of AI Medical Device Updates",
          "authors": [
            "Kevin Wu",
            "Eric Wu",
            "Kit Rodolfa",
            "Daniel E. Ho",
            "James Zou"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "While the pace of development of AI has rapidly progressed in recent years,\nthe implementation of safe and effective regulatory frameworks has lagged\nbehind. In particular, the adaptive nature of AI models presents unique\nchallenges to regulators as updating a model can improve its performance but\nalso introduce safety risks. In the US, the Food and Drug Administration (FDA)\nhas been a forerunner in regulating and approving hundreds of AI medical\ndevices. To better understand how AI is updated and its regulatory\nconsiderations, we systematically analyze the frequency and nature of updates\nin FDA-approved AI medical devices. We find that less than 2% of all devices\nreport having been updated by being re-trained on new data. Meanwhile, nearly a\nquarter of devices report updates in the form of new functionality and\nmarketing claims. As an illustrative case study, we analyze pneumothorax\ndetection models and find that while model performance can degrade by as much\nas 0.18 AUC when evaluated on new sites, re-training on site-specific data can\nmitigate this performance drop, recovering up to 0.23 AUC. However, we also\nobserved significant degradation on the original site after re-training using\ndata from new sites, providing insight from one example that challenges the\ncurrent one-model-fits-all approach to regulatory approvals. Our analysis\nprovides an in-depth look at the current state of FDA-approved AI device\nupdates and insights for future regulatory policies toward model updating and\nadaptive AI.",
          "pdf_url": "http://arxiv.org/pdf/2407.16900v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2407.16900v1",
          "source": "arXiv"
        },
        {
          "title": "Towards an AI Accountability Policy",
          "authors": [
            "Przemyslaw Grabowicz",
            "Adrian Byrne",
            "Cyrus Cousins",
            "Nicholas Perello",
            "Yair Zick"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "We propose establishing an office to oversee AI systems by introducing a\ntiered system of explainability and benchmarking requirements for commercial AI\nsystems. We examine how complex high-risk technologies have been successfully\nregulated at the national level. Specifically, we draw parallels to the\nexisting regulation for the U.S. medical device industry and the pharmaceutical\nindustry (regulated by the FDA), the proposed legislation for AI in the\nEuropean Union (the AI Act), and the existing U.S. anti-discrimination\nlegislation. To promote accountability and user trust, AI accountability\nmechanisms shall introduce standarized measures for each category of intended\nhigh-risk use of AI systems to enable structured comparisons among such AI\nsystems. We suggest using explainable AI techniques, such as input influence\nmeasures, as well as fairness statistics and other performance measures of\nhigh-risk AI systems. We propose to standardize internal benchmarking and\nautomated audits to transparently characterize high-risk AI systems. The\nresults of such audits and benchmarks shall be clearly and transparently\ncommunicated and explained to enable meaningful comparisons of competing AI\nsystems via a public AI registry. Such standardized audits, benchmarks, and\ncertificates shall be specific to intended high-risk use of respective AI\nsystems and could constitute conformity assessment for AI systems, e.g., in the\nEuropean Union's AI Act.",
          "pdf_url": "http://arxiv.org/pdf/2307.13658v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2307.13658v2",
          "source": "arXiv"
        },
        {
          "title": "Regulating radiology AI medical devices that evolve in their lifecycle",
          "authors": [
            "Camila González",
            "Moritz Fuchs",
            "Daniel Pinto dos Santos",
            "Philipp Matthies",
            "Manuel Trenz",
            "Maximilian Grüning",
            "Akshay Chaudhari",
            "David B. Larson",
            "Ahmed Othman",
            "Moon Kim",
            "Felix Nensa",
            "Anirban Mukhopadhyay"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Over time, the distribution of medical image data drifts due to factors such\nas shifts in patient demographics, acquisition devices, and disease\nmanifestations. While human radiologists can adjust their expertise to\naccommodate such variations, deep learning models cannot. In fact, such models\nare highly susceptible to even slight variations in image characteristics.\nConsequently, manufacturers must conduct regular updates to ensure that they\nremain safe and effective. Performing such updates in the United States and\nEuropean Union required, until recently, obtaining re-approval. Given the time\nand financial burdens associated with these processes, updates were infrequent,\nand obsolete systems remained in operation for too long. During 2024, several\nregulatory developments promised to streamline the safe rollout of model\nupdates: The European Artificial Intelligence Act came into effect last August,\nand the Food and Drug Administration (FDA) issued final marketing submission\nrecommendations for a Predetermined Change Control Plan (PCCP) in December. We\nprovide an overview of these developments and outline the key building blocks\nnecessary for successfully deploying dynamic systems. At the heart of these\nregulations - and as prerequisites for manufacturers to conduct model updates\nwithout re-approval - are clear descriptions of data collection and re-training\nprocesses, coupled with robust real-world quality monitoring mechanisms.",
          "pdf_url": "http://arxiv.org/pdf/2412.20498v3.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2412.20498v3",
          "source": "arXiv"
        },
        {
          "title": "Responsible Deep Learning for Software as a Medical Device",
          "authors": [
            "Pratik Shah",
            "Jenna Lester",
            "Jana G Deflino",
            "Vinay Pai"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Tools, models and statistical methods for signal processing and medical image\nanalysis and training deep learning models to create research prototypes for\neventual clinical applications are of special interest to the biomedical\nimaging community. But material and optical properties of biological tissues\nare complex and not easily captured by imaging devices. Added complexity can be\nintroduced by datasets with underrepresentation of medical images from races\nand ethnicities for deep learning, and limited knowledge about the regulatory\nframework needed for commercialization and safety of emerging Artificial\nIntelligence (AI) and Machine Learning (ML) technologies for medical image\nanalysis. This extended version of the workshop paper presented at the special\nsession of the 2022 IEEE 19th International Symposium on Biomedical Imaging,\ndescribes strategy and opportunities by University of California professors\nengaged in machine learning (section I) and clinical research (section II), the\nOffice of Science and Engineering Laboratories (OSEL) section III, and\nofficials at the US FDA in Center for Devices & Radiological Health (CDRH)\nsection IV. Performance evaluations of AI/ML models of skin (RGB), tissue\nbiopsy (digital pathology), and lungs and kidneys (Magnetic Resonance, X-ray,\nComputed Tomography) medical images for regulatory evaluations and real-world\ndeployment are discussed.",
          "pdf_url": "http://arxiv.org/pdf/2312.13333v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2312.13333v1",
          "source": "arXiv"
        },
        {
          "title": "An FDA for AI? Pitfalls and Plausibility of Approval Regulation for\n  Frontier Artificial Intelligence",
          "authors": [
            "Daniel Carpenter",
            "Carson Ezell"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Observers and practitioners of artificial intelligence (AI) have proposed an\nFDA-style licensing regime for the most advanced AI models, or 'frontier'\nmodels. In this paper, we explore the applicability of approval regulation --\nthat is, regulation of a product that combines experimental minima with\ngovernment licensure conditioned partially or fully upon that experimentation\n-- to the regulation of frontier AI. There are a number of reasons to believe\nthat approval regulation, simplistically applied, would be inapposite for\nfrontier AI risks. Domains of weak fit include the difficulty of defining the\nregulated product, the presence of Knightian uncertainty or deep ambiguity\nabout harms from AI, the potentially transmissible nature of risks, and\ndistributed activities among actors involved in the AI lifecycle. We conclude\nby highlighting the role of policy learning and experimentation in regulatory\ndevelopment, describing how learning from other forms of AI regulation and\nimprovements in evaluation and testing methods can help to overcome some of the\nchallenges we identify.",
          "pdf_url": "http://arxiv.org/pdf/2408.00821v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2408.00821v1",
          "source": "arXiv"
        },
        {
          "title": "Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep\n  Learning-Based Automated Inspections of Class III Medical Devices",
          "authors": [
            "Julio Zanon Diaz",
            "Tommy Brennan",
            "Peter Corcoran"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "As deep learning (DL) technologies advance, their application in automated\nvisual inspection for Class III medical devices offers significant potential to\nenhance quality assurance and reduce human error. However, the adoption of such\nAI-based systems introduces new regulatory complexities-particularly under the\nEU Artificial Intelligence (AI) Act, which imposes high-risk system obligations\nthat differ in scope and depth from established regulatory frameworks such as\nthe Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation\n(QSR). This paper presents a high-level technical assessment of the foreseeable\nchallenges that manufacturers are likely to encounter when qualifying DL-based\nautomated inspections -- specifically static models -- within the existing\nmedical device compliance landscape. It examines divergences in risk management\nprinciples, dataset governance, model validation, explainability requirements,\nand post-deployment monitoring obligations. The discussion also explores\npotential implementation strategies and highlights areas of uncertainty,\nincluding data retention burdens, global compliance implications, and the\npractical difficulties of achieving statistical significance in validation with\nlimited defect data. Disclaimer: This paper presents a technical perspective\nand does not constitute legal or regulatory advice.",
          "pdf_url": "http://arxiv.org/pdf/2508.20144v3.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2508.20144v3",
          "source": "arXiv"
        },
        {
          "title": "The Impact of US Medical Product Regulatory Complexity on Innovation:\n  Preliminary Evidence of Interdependence, Early Acceleration, and Subsequent\n  Inversion",
          "authors": [
            "Iraj Daizadeh"
          ],
          "year": 2022,
          "citations": 0,
          "abstract": "Is the complexity of medical product (medicines and medical devices)\nregulation impacting innovation in the US? If so, how? Here, this question is\ninvestigated as follows: Various novel proxy metrics of regulation (FDA-issued\nguidelines) and innovation (corresponding FDA-registrations) from 1976-2020 are\nused to determine interdependence, a concept relying on strong correlation and\nreciprocal causality (estimated via variable lag transfer entropy and wavelet\ncoherence). Based on this interdependence, a mapping of regulation onto\ninnovation is conducted and finds that regulation seems to accelerate then\nsupports innovation until on or around 2015; at which time, an inverted U-curve\nemerged. If empirically evidentiary, an important innovation-regulation nexus\nin the US has been reached; and, as such, stakeholders should (re)consider the\ncomplexity of the regulatory landscape to enhance US medical product\ninnovation. Study limitations, extensions, and further thoughts complete this\ninvestigation.",
          "pdf_url": "http://arxiv.org/pdf/2211.12998v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2211.12998v1",
          "source": "arXiv"
        },
        {
          "title": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in\n  Diabetic Retinopathy",
          "authors": [
            "Nadim Barakat",
            "William Lotter"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI\nsystems can expand access to fundus photography screening. Current FDA-cleared\nsystems primarily provide binary referral outputs, where this minimal output\nmay limit clinical trust and utility. Yet, determining the most effective\noutput format to enhance clinician-AI performance is an empirical challenge\nthat is difficult to assess at scale. We evaluated multimodal large language\nmodels (MLLMs) for DR detection and their ability to simulate clinical AI\nassistance across different output types. Two models were tested on IDRiD and\nMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source\nmedical model. Experiments included: (1) baseline evaluation, (2) simulated AI\nassistance with synthetic predictions, and (3) actual AI-to-AI collaboration\nwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at\nbaseline, achieving higher sensitivity and AUROC, while GPT-4o showed\nnear-perfect specificity but low sensitivity. Both models adjusted predictions\nbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrect\nones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o\nachieved strong results when guided by MedGemma's descriptive outputs, even\nwithout direct image access (AUROC up to 0.96). These findings suggest MLLMs\nmay improve DR screening pipelines and serve as scalable simulators for\nstudying clinical AI assistance across varying output configurations. Open,\nlightweight models such as MedGemma may be especially valuable in low-resource\nsettings, while descriptive outputs could enhance explainability and clinician\ntrust in clinical workflows.",
          "pdf_url": "http://arxiv.org/pdf/2509.13234v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2509.13234v1",
          "source": "arXiv"
        },
        {
          "title": "Ethics by Design: A Lifecycle Framework for Trustworthy AI in Medical\n  Imaging From Transparent Data Governance to Clinically Validated Deployment",
          "authors": [
            "Umer Sadiq Khan",
            "Saif Ur Rehman Khan"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "The integration of artificial intelligence (AI) in medical imaging raises\ncrucial ethical concerns at every stage of its development, from data\ncollection to deployment. Addressing these concerns is essential for ensuring\nthat AI systems are developed and implemented in a manner that respects patient\nrights and promotes fairness. This study aims to explore the ethical\nimplications of AI in medical imaging, focusing on five key stages: data\ncollection, data processing, model training, model evaluation, and deployment.\nThe goal is to evaluate how these stages adhere to fundamental ethical\nprinciples, including data privacy, fairness, transparency, accountability, and\nautonomy. An analytical approach was employed to examine the ethical challenges\nassociated with each stage of AI development. We reviewed existing literature,\nguidelines, and regulations concerning AI ethics in healthcare and identified\ncritical ethical issues at each stage. The study outlines specific inquiries\nand principles for each phase of AI development. The findings highlight key\nethical issues: ensuring patient consent and anonymization during data\ncollection, addressing biases in model training, ensuring transparency and\nfairness during model evaluation, and the importance of continuous ethical\nassessments during deployment. The analysis also emphasizes the impact of\naccessibility issues on different stakeholders, including private, public, and\nthird-party entities. The study concludes that ethical considerations must be\nsystematically integrated into each stage of AI development in medical imaging.\nBy adhering to these ethical principles, AI systems can be made more robust,\ntransparent, and aligned with patient care and data control. We propose\ntailored ethical inquiries and strategies to support the creation of ethically\nsound AI systems in medical imaging.",
          "pdf_url": "http://arxiv.org/pdf/2507.04249v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2507.04249v1",
          "source": "arXiv"
        },
        {
          "title": "Certified Safe: A Schematic for Approval Regulation of Frontier AI",
          "authors": [
            "Cole Salvador"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Recent and unremitting capability advances have been accompanied by calls for\ncomprehensive, rather than patchwork, regulation of frontier artificial\nintelligence (AI). Approval regulation is emerging as a promising candidate. An\napproval regulation scheme is one in which a firm cannot legally market, or in\nsome cases develop, a product without explicit approval from a regulator on the\nbasis of experiments performed upon the product that demonstrate its safety.\nThis approach is used successfully by the FDA and FAA. Further, its application\nto frontier AI has been publicly supported by many prominent stakeholders. This\nreport proposes an approval regulation schematic for only the largest AI\nprojects in which scrutiny begins before training and continues through to\npost-deployment monitoring. The centerpieces of the schematic are two major\napproval gates, the first requiring approval for large-scale training and the\nsecond for deployment. Five main challenges make implementation difficult:\nnoncompliance through unsanctioned deployment, specification of deployment\nreadiness requirements, reliable model experimentation, filtering out safe\nmodels before the process, and minimizing regulatory overhead. This report\nmakes a number of crucial recommendations to increase the feasibility of\napproval regulation, some of which must be followed urgently if such a regime\nis to succeed in the near future. Further recommendations, produced by this\nreport's analysis, may improve the effectiveness of any regulatory regime for\nfrontier AI.",
          "pdf_url": "http://arxiv.org/pdf/2408.06210v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2408.06210v1",
          "source": "arXiv"
        }
      ]
    },
    {
      "original_query": "Shoulder implant classification benchmark",
      "final_query": "Shoulder implant classification benchmark",
      "attempts": 1,
      "paper_count": 20,
      "open_access_count": 10,
      "papers": [
        {
          "title": "Transfer Learning-Based Class Imbalance-Aware Shoulder Implant Classification from X-Ray Images",
          "authors": [
            "Marut Jindal",
            "Birmohan Singh"
          ],
          "year": 2024,
          "citations": 4,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1007/s42235-023-00477-0",
          "url": "https://www.semanticscholar.org/paper/d7facd4b605b61f6972ab9e4b50664b20167ea06",
          "source": "Semantic Scholar"
        },
        {
          "title": "An Ensemble Voting Approach for Shoulder Implant Classification from X-Ray Images",
          "authors": [
            "Elif Baykal Kablan"
          ],
          "year": 2024,
          "citations": 1,
          "abstract": "Total Shoulder Arthroplasty (TSA) is an effective method that involves replacing damaged joint surfaces with appropriate prosthetic components to manage pain and improve joint mobility. Over time, the prosthetic replacement or improvement process requires both the surgeon and the patient to know the prosthesis manufacturer for successful outcomes. However, inadequate medical records often necessitate additional X-rays for the patient. In this paper, a deep learning-based method is proposed to analyze prosthesis details from X-ray images automatically. The method combines predictions made by pre-trained ViT, DeiT, and Swin model versions on a dataset consisting of 597 shoulder implant X-ray images to achieve more accurate classification. The performances of hard and soft voting ensembles were analyzed respectively. During the experiments, the soft voting approach yielded the highest performance, achieving 96.15 % accuracy, 92.12 % precision, and 88.56 % recall, marking the highest classification performance observed thus far. The results demonstrate that our voting ensemble method can be utilized as a reliable and effective tool for automatically analyzing prosthesis details.",
          "pdf_url": "",
          "doi": "10.1109/TSP63128.2024.10605940",
          "url": "https://www.semanticscholar.org/paper/d6a1b7151fd7ad070a4c6396b97fffb2d60f04bd",
          "source": "Semantic Scholar"
        },
        {
          "title": "Harnessing the Potential of Deep Learning for Total Shoulder Implant Classification: A Comparative Study",
          "authors": [
            "Aakriti Mishra",
            "A. Ramanathan",
            "V. Batta",
            "C. Malathy",
            "Soumya Snigdha Kundu",
            "M. Gayathri",
            "D. Vathana",
            "S. Kamineni"
          ],
          "year": 2023,
          "citations": 1,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1007/978-3-031-48593-0_9",
          "url": "https://www.semanticscholar.org/paper/b39eeb1ef6c0b61923596e93d94196c17a04d83f",
          "source": "Semantic Scholar"
        },
        {
          "title": "Classification of Shoulder Implant Manufacturer Using Pre‐Trained DenseNet201 Combined With Capsule Network",
          "authors": [
            "Xianzhong Jian",
            "Zhenling Zhou",
            "Wuwen Zhang"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "This study aims to accelerate revision surgery and treatment using X‐ray imaging and deep learning to identify shoulder implant manufacturers in advance.",
          "pdf_url": "",
          "doi": "10.1002/rcs.2672",
          "url": "https://www.semanticscholar.org/paper/c83781aeda2d1312a758004897de894e9ece54e0",
          "source": "Semantic Scholar"
        },
        {
          "title": "Machine Learning Model for Classification of Shoulder Implant Manufacturer Using X-Ray Images",
          "authors": [
            "Idris Djibo",
            "I. Z. Yakubu",
            "Muhammad Raiyan",
            "G. Manikandan",
            "Fatima Shittu",
            "Z. Musa"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Recently, synthetic prosthesis built from metals and plastic components are often used to mitigate pain and restore functions of injured human shoulders. The procedure involves the replacement of the affected shoulder ball and socket joint with the synthetically generated prosthesis. Long after the replacement of the affected part, the synthetic prosthesis maybe damaged or worn out, thus the reoperation process maybe repeated or revised. To guarantee a robust and seamless reoperation, detail of the model and manufacturer of the synthetic prosthesis is a paramount. There are circumstances where information regarding the prosthesis is not available. To determine the model and manufacturer, a thorough inspection and physical analogy of various manufacturers’ prosthesis are therefore required. The manual method consumes longer time and is liable to errors. With the evolution and continuous growth in the field of Machine Learning, prediction models can be used to learn the prosthesis and predict the model and manufacturer, thus reducing the time and error in the manual approach. In this paper, a model based on ensemble learning for identification of the manufacturer of synthetic prosthesis is proposed. Deep Convolution Neural Network (DCNN), High Resolution Network (HRNet), and Support Vector Machine (SVM) were combined to form the ensemble model. To ensure accurate prediction of the manufacturer, the components of the ensemble model are separately trained and then integrated using a novel weighted average ensemble technique. This strategy aids in identifying the prosthesis' manufacturer by assigning greater weight to the model that performs the best. The proposed model performance is compared with the performance of its individual components, where it outperforms the individual models in terms of accuracy, recall, precision and f measures.",
          "pdf_url": "",
          "doi": "10.1109/ICAST61769.2024.10856466",
          "url": "https://www.semanticscholar.org/paper/0548261ee0921a5cb04e43cc17ccc45ce765326e",
          "source": "Semantic Scholar"
        },
        {
          "title": "EFFICACY OF ARTIFICIAL INTELLIGENCE-BASED MODELS FOR SHOULDER ARTHROPLASTY IMPLANT DETECTION AND CLASSIFICATION USING UPPER LIMB RADIOGRAPHS: A SYSTEMATIC REVIEW AND META-ANALYSIS",
          "authors": [
            "A.M. Asgari",
            "F. Shaker",
            "M. Fallahy",
            "M. Soleimani",
            "S. Shafiei",
            "Y. Fallah"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Shoulder arthroplasty (SA) has been performed with different types of implants, each requiring different replacement systems. However, data on previously utilized implant types are not always available before revision surgery, which is paramount to determining the appropriate equipment and procedure. Therefore, this meta-analysis aimed to evaluate the accuracy of the AI models in classifying SA implant types.This systematic review was conducted in Pubmed, Embase, SCOPUS, and Web of Science from inception to December 2023, according to PRISMA guidelines. Peer-reviewed research evaluating the accuracy of AI-based tools on upper-limb X-rays for recognizing and categorizing SA implants was included. In addition to the overall meta-analysis, subgroup analysis was performed according to the type of AI model applied (CNN (Convolutional neural network), non-CNN, or Combination of both) and the similarity of utilized datasets between studies.13 articles were eligible for inclusion in this meta-analysis (including 138 different tests assessing models’ efficacy). Our meta-analysis demonstrated an overall sensitivity and specificity of 0.891 (95% CI:0.866-0.912) and 0.549 (95% CI:0.532,0.566) for classifying implants in SA, respectively. The results of our subgroup analyses were as follows: CNN-subgroup: a sensitivity of 0.898 (95% CI:0.873-0.919) and a specificity of 0.554 (95% CI:0.537,0.570), Non-CNN subgroup: a sensitivity of 0.809 (95% CI:0.665-0.900) and specificity of 0.522 (95% CI:0.440,0.603), combined subgroup: a sensitivity of 0.891 (95% CI:0.752-0.957) and a specificity of 0.547 (95% CI:0.463,0.629).Studies using the same dataset demonstrated an overall sensitivity and specificity of 0.881 (95% CI:0.856-0.903) and 0.542 (95% CI:0.53,0.554), respectively. Studies that used other datasets showed an overall sensitivity and specificity of 0.995 (95% CI:969,0.999) and 0.678 (95% CI:0.234, 0.936), respectively.AI-based classification of shoulder implant types can be considered a sensitive method. Our study showed the potential role of using CNN-based models and different datasets to enhance accuracy, which could be investigated in future studies.",
          "pdf_url": "",
          "doi": "10.1302/1358-992x.2024.18.060",
          "url": "https://www.semanticscholar.org/paper/49cbb15e4f895dee2c29e1f245ce37b650d141d4",
          "source": "Semantic Scholar"
        },
        {
          "title": "A HYBRID DEEP CNN-LSTM NETWORK (HDCLN) FOR SHOULDER IMPLANT CLASSIFICATION USING X-RAY IMAGES",
          "authors": [
            "Rajendra Prasad Banavathu",
            "M. Jamesstephen",
            "Prasad Reddy"
          ],
          "year": null,
          "citations": 0,
          "abstract": null,
          "pdf_url": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/54c272df5e667e6dead27b1487890ad241b288da",
          "source": "Semantic Scholar"
        },
        {
          "title": "OII-DS: A benchmark Oral Implant Image Dataset for object detection and image classification evaluation",
          "authors": [
            "Qianqing Nie",
            "Chen Li",
            "Jinzhu Yang",
            "Yudong Yao",
            "Hongzan Sun",
            "Tao Jiang",
            "Marcin Grzegorzek",
            "Ao Chen",
            "Hao Chen",
            "Weiming Hu",
            "Rui Li",
            "Jiawei Zhang",
            "Danning Wang"
          ],
          "year": 2023,
          "citations": 18,
          "abstract": null,
          "pdf_url": "",
          "doi": "10.1016/j.compbiomed.2023.107620",
          "url": "https://www.semanticscholar.org/paper/c9c69d8dac3370be41369343162cc361f999b68e",
          "source": "Semantic Scholar"
        },
        {
          "title": "Shoulder Implant X-Ray Manufacturer Classification: Exploring with Vision Transformer",
          "authors": [
            "Meng Zhou",
            "Shanglin Mo"
          ],
          "year": 2021,
          "citations": 13,
          "abstract": "Shoulder replacement surgery, also called total shoulder replacement, is a common and complex surgery in Orthopedics discipline. It involves replacing a dead shoulder joint with an artificial implant. In the market, there are many artificial implant manufacturers and each of them may produce different implants with different structures compares to other providers. The problem arises in the following situation: a patient has some problems with the shoulder implant accessory and the manufacturer of that implant maybe unknown to either the patient or the doctor, therefore, correctly identification of the manufacturer is the key prior to the treatment. In this paper, we will demonstrate different methods for classifying the manufacturer of a shoulder implant.",
          "pdf_url": "",
          "doi": null,
          "url": "https://www.semanticscholar.org/paper/bbb15fed76a76680d8d34f67995c9e01f5ce4231",
          "source": "Semantic Scholar"
        },
        {
          "title": "A robust framework for shoulder implant X-ray image classification",
          "authors": [
            "M. Vo",
            "Anh H. Vo",
            "Tuong Le"
          ],
          "year": 2021,
          "citations": 14,
          "abstract": "PurposeMedical images are increasingly popular; therefore, the analysis of these images based on deep learning helps diagnose diseases become more and more essential and necessary. Recently, the shoulder implant X-ray image classification (SIXIC) dataset that includes X-ray images of implanted shoulder prostheses produced by four manufacturers was released. The implant's model detection helps to select the correct equipment and procedures in the upcoming surgery.Design/methodology/approachThis study proposes a robust model named X-Net to improve the predictability for shoulder implants X-ray image classification in the SIXIC dataset. The X-Net model utilizes the Squeeze and Excitation (SE) block integrated into Residual Network (ResNet) module. The SE module aims to weigh each feature map extracted from ResNet, which aids in improving the performance. The feature extraction process of X-Net model is performed by both modules: ResNet and SE modules. The final feature is obtained by incorporating the extracted features from the above steps, which brings more important characteristics of X-ray images in the input dataset. Next, X-Net uses this fine-grained feature to classify the input images into four classes (Cofield, Depuy, Zimmer and Tornier) in the SIXIC dataset.FindingsExperiments are conducted to show the proposed approach's effectiveness compared with other state-of-the-art methods for SIXIC. The experimental results indicate that the approach outperforms the various experimental methods in terms of several performance metrics. In addition, the proposed approach provides the new state of the art results in all performance metrics, such as accuracy, precision, recall, F1-score and area under the curve (AUC), for the experimental dataset.Originality/valueThe proposed method with high predictive performance can be used to assist in the treatment of injured shoulder joints.",
          "pdf_url": "",
          "doi": "10.1108/dta-08-2021-0210",
          "url": "https://www.semanticscholar.org/paper/81995bd78a142f92ba0cc06bc9fa6fb721b76e54",
          "source": "Semantic Scholar"
        },
        {
          "title": "Shoulder Implant X-Ray Manufacturer Classification: Exploring with\n  Vision Transformer",
          "authors": [
            "Meng Zhou",
            "Shanglin Mo"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Shoulder replacement surgery, also called total shoulder replacement, is a\ncommon and complex surgery in Orthopedics discipline. It involves replacing a\ndead shoulder joint with an artificial implant. In the market, there are many\nartificial implant manufacturers and each of them may produce different\nimplants with different structures compares to other providers. The problem\narises in the following situation: a patient has some problems with the\nshoulder implant accessory and the manufacturer of that implant maybe unknown\nto either the patient or the doctor, therefore, correctly identification of the\nmanufacturer is the key prior to the treatment. In this paper, we will\ndemonstrate different methods for classifying the manufacturer of a shoulder\nimplant. We will use Vision Transformer approach to this task for the first\ntime ever",
          "pdf_url": "http://arxiv.org/pdf/2104.07667v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2104.07667v2",
          "source": "arXiv"
        },
        {
          "title": "Computational simulation of bone remodelling post reverse total shoulder\n  arthroplasty",
          "authors": [
            "H. Liedtke",
            "A. T. McBride",
            "S. Sivarasu",
            "S. Roche"
          ],
          "year": 2017,
          "citations": 0,
          "abstract": "Bone is a living material. It adapts, in an optimal sense, to loading by\nchanging its density and trabeculae architecture - a process termed\nremodelling. Implanted orthopaedic devices can significantly alter the loading\non the surrounding bone, which can have a detrimental impact on bone ingrowth\nthat is critical to ensure secure implant fixation. In this contribution, a\ncomputational model that accounts for bone remodelling is developed to\nelucidate the response of bone following a reverse shoulder procedure for\nrotator cuff deficient patients. The physical process of remodelling is\nmodelled using continuum scale, open system thermodynamics whereby the density\nof bone evolves isotropically in response to the loading it experiences. The\nfully-nonlinear continuum theory is solved approximately using the finite\nelement method. The code developed to model the reverse shoulder procedure is\nvalidated using a series of benchmark problems.",
          "pdf_url": "http://arxiv.org/pdf/1705.08324v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1705.08324v1",
          "source": "arXiv"
        },
        {
          "title": "Influence of Rotator Cuff Integrity on Loading and Kinematics Before and\n  After Reverse Shoulder Arthroplasty",
          "authors": [
            "Fabien Péan",
            "Philippe Favre",
            "Orcun Goksel"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Reverse Shoulder Arthroplasty (RSA) has become a very common procedure for\nshoulder joint replacement, even for scenarios where an anatomical\nreconstruction would traditionally be used. In this study, we investigate joint\nreaction forces and scapular kinematics for rotator cuff tears of different\ntendons with and without a reverse prosthesis. Available motion capture data\nduring anterior flexion was input to a finite-element musculoskeletal shoulder\nmodel, and muscle activations were computed using inverse dynamics. The model\nwas validated with respect to in-vivo glenohumeral joint reaction force (JRF)\nmeasurements, and also compared to existing clinical and biomechanical data.\nSimulations were carried out for the intact joint as well as for various\ntendons involved in a rotator cuff tear: superior (supraspinatus),\nsuperior-anterior (supraspinatus and subscapularis), and superior-posterior\n(supraspinatus, infraspinatus and teres minor). Each rotator cuff tear\ncondition was repeated after shifting the humerus and the glenohumeral joint\ncenter of rotation to simulate the effect of a reverse prosthesis. Changes in\ncompressive, shear, and total JRF were analysed, along with scapular upward\nrotation. The model compared favourably to in-vivo measurements, and existing\nclinical and biomechanical knowledge. Simulated JRF lie in the ranges of\nin-vivo JRF measurements and shows a linear increase past 90 degree flexion.\nImplanting a reverse prosthesis with a functional rotator cuff or with an\nisolated supraspinatus tear led to over 2 times higher compressive force\ncomponent than with massive rotator cuff tears (superior-anterior or\nsuperior-posterior). Such higher compression forces might increase the risk of\nwear and implant fracture.",
          "pdf_url": "http://arxiv.org/pdf/2012.09763v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2012.09763v1",
          "source": "arXiv"
        },
        {
          "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture\n  Detection in Clinical Radiographs",
          "authors": [
            "Hemanth Kumar M",
            "Karthika M",
            "Saianiruth M",
            "Vasanthakumar Venugopal",
            "Anandakumar D",
            "Revathi Ezhumalai",
            "Charulatha K",
            "Kishore Kumar J",
            "Dayana G",
            "Kalyan Sivasailam",
            "Bargava Subramanian"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.",
          "pdf_url": "http://arxiv.org/pdf/2507.13408v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2507.13408v1",
          "source": "arXiv"
        },
        {
          "title": "iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing",
          "authors": [
            "Mengxi Liu",
            "Hymalai Bello",
            "Bo Zhou",
            "Paul Lukowicz",
            "Jakob Karolus"
          ],
          "year": 2024,
          "citations": 0,
          "abstract": "Hand-over-face gestures can provide important implicit interactions during\nconversations, such as frustration or excitement. However, in situations where\ninterlocutors are not visible, such as phone calls or textual communication,\nthe potential meaning contained in the hand-over-face gestures is lost. In this\nwork, we present iFace, an unobtrusive, wearable impedance-sensing solution for\nrecognizing different hand-over-face gestures. In contrast to most existing\nworks, iFace does not require the placement of sensors on the user's face or\nhands. Instead, we proposed a novel sensing configuration, the shoulders, which\nremains invisible to both the user and outside observers. The system can\nmonitor the shoulder-to-shoulder impedance variation caused by gestures through\nelectrodes attached to each shoulder. We evaluated iFace in a user study with\neight participants, collecting six kinds of hand-over-face gestures with\ndifferent meanings. Using a convolutional neural network and a user-dependent\nclassification, iFace reaches 82.58 \\% macro F1 score. We discuss potential\napplication scenarios of iFace as an implicit interaction interface.",
          "pdf_url": "http://arxiv.org/pdf/2403.18433v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2403.18433v1",
          "source": "arXiv"
        },
        {
          "title": "Advanced spike sorting approaches in implantable VLSI wireless brain\n  computer interfaces: a survey",
          "authors": [
            "Soujatya Sarkar"
          ],
          "year": 2023,
          "citations": 0,
          "abstract": "Brain Computer/Machine Interfaces (BCI/BMIs) have substantial potential for\nenhancing the lives of disabled individuals by restoring functionalities of\nmissing body parts or allowing paralyzed individuals to regain speech and other\nmotor capabilities. Due to severe health hazards arising from skull incisions\nrequired for wired BCI/BMIs, scientists are focusing on developing VLSI\nwireless BCI implants using biomaterials. However, significant challenges, like\npower efficiency and implant size, persist in creating reliable and efficient\nwireless BCI implants. With advanced spike sorting techniques, VLSI wireless\nBCI implants can function within the power and size constraints while\nmaintaining neural spike classification accuracy. This study explores advanced\nspike sorting techniques to overcome these hurdles and enable VLSI wireless\nBCI/BMI implants to transmit data efficiently and achieve high accuracy.",
          "pdf_url": "http://arxiv.org/pdf/2309.00913v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2309.00913v2",
          "source": "arXiv"
        },
        {
          "title": "Shoulder Physiotherapy Exercise Recognition: Machine Learning the\n  Inertial Signals from a Smartwatch",
          "authors": [
            "David Burns",
            "Nathan Leung",
            "Michael Hardisty",
            "Cari Whyne",
            "Patrick Henry",
            "Stewart McLachlin"
          ],
          "year": 2018,
          "citations": 0,
          "abstract": "Objective: Participation in a physical therapy program is considered one of\nthe greatest predictors of successful conservative management of common\nshoulder disorders. However, adherence to these protocols is often poor and\ntypically worse for unsupervised home exercise programs. Currently, there are\nlimited tools available for objective measurement of adherence in the home\nsetting. The goal of this study was to develop and evaluate the potential for\nperforming home shoulder physiotherapy monitoring using a commercial\nsmartwatch.\n  Approach: Twenty healthy adult subjects with no prior shoulder disorders\nperformed seven exercises from an evidence-based rotator cuff physiotherapy\nprotocol, while 6-axis inertial sensor data was collected from the active\nextremity. Within an activity recognition chain (ARC) framework, four\nsupervised learning algorithms were trained and optimized to classify the\nexercises: k-nearest neighbor (k-NN), random forest (RF), support vector\nmachine classifier (SVC), and a convolutional recurrent neural network (CRNN).\nAlgorithm performance was evaluated using 5-fold cross-validation stratified\nfirst temporally and then by subject.\n  Main Results: Categorical classification accuracy was above 94% for all\nalgorithms on the temporally stratified cross validation, with the best\nperformance achieved by the CRNN algorithm (99.4%). The subject stratified\ncross validation, which evaluated classifier performance on unseen subjects,\nyielded lower accuracies scores again with CRNN performing best (88.9%).\n  Significance: This proof of concept study demonstrates the technical\nfeasibility of a smartwatch device and supervised machine learning approach to\nmore easily monitor and assess the at-home adherence of shoulder physiotherapy\nexercise protocols.",
          "pdf_url": "http://arxiv.org/pdf/1802.01489v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/1802.01489v2",
          "source": "arXiv"
        },
        {
          "title": "Classification of Shoulder X-Ray Images with Deep Learning Ensemble\n  Models",
          "authors": [
            "Fatih Uysal",
            "Fırat Hardalaç",
            "Ozan Peker",
            "Tolga Tolunay",
            "Nil Tokgöz"
          ],
          "year": 2021,
          "citations": 0,
          "abstract": "Fractures occur in the shoulder area, which has a wider range of motion than\nother joints in the body, for various reasons. To diagnose these fractures,\ndata gathered from Xradiation (X-ray), magnetic resonance imaging (MRI), or\ncomputed tomography (CT) are used. This study aims to help physicians by\nclassifying shoulder images taken from X-ray devices as fracture / non-fracture\nwith artificial intelligence. For this purpose, the performances of 26 deep\nlearning-based pretrained models in the detection of shoulder fractures were\nevaluated on the musculoskeletal radiographs (MURA) dataset, and two ensemble\nlearning models (EL1 and EL2) were developed. The pretrained models used are\nResNet, ResNeXt, DenseNet, VGG, Inception, MobileNet, and their spinal fully\nconnected (Spinal FC) versions. In the EL1 and EL2 models developed using\npretrained models with the best performance, test accuracy was 0.8455,0.8472,\nCohens kappa was 0.6907, 0.6942 and the area that was related with fracture\nclass under the receiver operating characteristic (ROC) curve (AUC) was\n0.8862,0.8695. As a result of 28 different classifications in total, the\nhighest test accuracy and Cohens kappa values were obtained in the EL2 model,\nand the highest AUC value was obtained in the EL1 model.",
          "pdf_url": "http://arxiv.org/pdf/2102.00515v3.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2102.00515v3",
          "source": "arXiv"
        },
        {
          "title": "Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of\n  Person Re-Identification",
          "authors": [
            "Boqiang Xu",
            "Lingxiao He",
            "Xingyu Liao",
            "Wu Liu",
            "Zhenan Sun",
            "Tao Mei"
          ],
          "year": 2020,
          "citations": 0,
          "abstract": "Person re-identification (Re-ID) aims at retrieving an input person image\nfrom a set of images captured by multiple cameras. Although recent Re-ID\nmethods have made great success, most of them extract features in terms of the\nattributes of clothing (e.g., color, texture). However, it is common for people\nto wear black clothes or be captured by surveillance systems in low light\nillumination, in which cases the attributes of the clothing are severely\nmissing. We call this problem the Black Re-ID problem. To solve this problem,\nrather than relying on the clothing information, we propose to exploit\nhead-shoulder features to assist person Re-ID. The head-shoulder adaptive\nattention network (HAA) is proposed to learn the head-shoulder feature and an\ninnovative ensemble method is designed to enhance the generalization of our\nmodel. Given the input person image, the ensemble method would focus on the\nhead-shoulder feature by assigning a larger weight if the individual insides\nthe image is in black clothing. Due to the lack of a suitable benchmark dataset\nfor studying the Black Re-ID problem, we also contribute the first Black-reID\ndataset, which contains 1274 identities in training set. Extensive evaluations\non the Black-reID, Market1501 and DukeMTMC-reID datasets show that our model\nachieves the best result compared with the state-of-the-art Re-ID methods on\nboth Black and conventional Re-ID problems. Furthermore, our method is also\nproved to be effective in dealing with person Re-ID in similar clothing. Our\ncode and dataset are avaliable on https://github.com/xbq1994/.",
          "pdf_url": "http://arxiv.org/pdf/2008.08528v1.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2008.08528v1",
          "source": "arXiv"
        },
        {
          "title": "A Wearable Strain-Sensor-Based Shoulder Patch for Fatigue Detection in\n  Bicep Curls",
          "authors": [
            "Ming Xuan Chua",
            "Shuhua Peng",
            "Thanh Nho Do",
            "Chun Hui Wang",
            "Liao Wu"
          ],
          "year": 2025,
          "citations": 0,
          "abstract": "A common challenge in home-based rehabilitation is muscle compensation\ninduced by pain or fatigue, where patients with weakened primary muscles\nrecruit secondary muscle groups to assist their movement, causing issues such\nas delayed rehabilitation progress or risk of further injury. In a home-based\nsetting, the subtle compensatory actions may not be perceived since\nphysiotherapists cannot directly observe patients. To address this problem,\nthis study develops a novel wearable strain sensor-based shoulder patch to\ndetect fatigue-induced muscle compensation during bicep curl exercises. Built\non an observation that the amplitude of a strain sensor's resistance is\ncorrelated to the motion of a joint that the sensor is attached to, we develop\nan algorithm that can robustly detect the state when significant changes appear\nin the shoulder joint motion, which indicates fatigue-induced muscle\ncompensation in bicep curls. The developed shoulder patch is tested on 13\nsubjects who perform bicep curl exercises with a 5 kg dumbbell until reaching\nfatigue. During the experiment, the performance of the shoulder patch is also\nbenchmarked with optical tracking sensors and surface electromyography (sEMG)\nsensors. Results reveal that the proposed wearable sensor and detection methods\neffectively monitor fatigue-induced muscle compensation during bicep curl\nexercises in both Real-Time and Post Hoc modes. This development marks a\nsignificant step toward enhancing the effectiveness of home-based\nrehabilitation by providing physiotherapists with a tool to monitor and adjust\ntreatment plans remotely.",
          "pdf_url": "http://arxiv.org/pdf/2501.14792v2.pdf",
          "doi": null,
          "url": "http://arxiv.org/abs/2501.14792v2",
          "source": "arXiv"
        }
      ]
    }
  ]
}